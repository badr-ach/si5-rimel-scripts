{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook contains a dump of unused and/or abandonned scripts' snippets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('7b', 5741),\n",
       "  ('2', 5660),\n",
       "  ('llama', 3060),\n",
       "  ('1', 2628),\n",
       "  ('7B', 2395),\n",
       "  ('gpt2', 1887),\n",
       "  ('13b', 1822),\n",
       "  ('exl2', 1665),\n",
       "  ('chat', 1586),\n",
       "  ('13B', 1491)],\n",
       " [('exl2', 1388),\n",
       "  ('GPTQ', 1329),\n",
       "  ('AWQ', 962),\n",
       "  ('h6-exl2', 911),\n",
       "  ('0bpw-h6-exl2', 731),\n",
       "  ('7b', 697),\n",
       "  ('2', 654),\n",
       "  ('1', 603),\n",
       "  ('GGUF', 526),\n",
       "  ('v2', 450)])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "import re \n",
    "# Re-reading the file and tokenizing the model names\n",
    "with open('./data/text-generation.txt', 'r') as file:\n",
    "    model_names = [line.split('/')[1].strip() for line in file if '/' in line]\n",
    "\n",
    "# Tokenizing the suffixes using \".\" and \"-\" as separators\n",
    "tokenized_suffixes = [re.split(r'[-\\.]', name) for name in model_names if '-' in name or '.' in name]\n",
    "\n",
    "tokenized_suffixes = [token for suffix in tokenized_suffixes for token in suffix]\n",
    "\n",
    "# Counting the occurrence of individual tokens and combinations of tokens\n",
    "token_counts = collections.Counter(tokenized_suffixes)\n",
    "combination_counts = collections.Counter()\n",
    "\n",
    "for name in model_names:\n",
    "    tokens = re.split(r'[-\\.]', name)\n",
    "    if len(tokens) > 1:\n",
    "        for i in range(len(tokens) - 1, 0, -1):\n",
    "            combination = '.'.join(tokens[i:]) if '.' in tokens[i] else '-'.join(tokens[i:])\n",
    "            combination_counts[combination] += 1\n",
    "\n",
    "# Most common individual tokens and combinations\n",
    "most_common_tokens = token_counts.most_common(10)\n",
    "most_common_combinations = combination_counts.most_common(10)\n",
    "\n",
    "most_common_tokens, most_common_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "\n",
    "# Read the file and tokenize the model names\n",
    "with open('./data/text-generation.txt', 'r') as file:\n",
    "    model_names = [line.strip() for line in file]\n",
    "\n",
    "# Extract the name part from each model name if it exists\n",
    "name_parts = [name.split('/')[1].split('-')[0].split('.')[0] for name in model_names if '/' in name]\n",
    "\n",
    "# Find the name parts that are repeated in the file\n",
    "repeated_name_parts = [name_part for name_part, count in collections.Counter(name_parts).items() if count > 1]\n",
    "\n",
    "# Filter the model names to keep only the ones with repeated name parts\n",
    "filtered_model_names = [name for name in model_names if '/' in name and name.split('/')[1].split('-')[0].split('.')[0] in repeated_name_parts]\n",
    "\n",
    "# Tokenize the version part using \"-\" and \".\" as delimiters\n",
    "tokenized_versions = [re.split(r'[-\\.]', name.split('/')[1].split('-')[1]) for name in filtered_model_names if '/' in name and '-' in name.split('/')[1] ]\n",
    "\n",
    "# Extract all the tokens\n",
    "tokens = [token for version in tokenized_versions for token in version]\n",
    "\n",
    "# Combine the tokens in different ways using \"-\" and \".\"\n",
    "combinations = []\n",
    "for i in range(1, 5):\n",
    "    combinations.extend(['-'.join(tokens[j:j+i]) for j in range(len(tokens)-i+1)])\n",
    "    combinations.extend(['.'.join(tokens[j:j+i]) for j in range(len(tokens)-i+1)])\n",
    "\n",
    "# Count the occurrence of individual tokens and combinations\n",
    "token_counts = collections.Counter(tokens)\n",
    "combination_counts = collections.Counter(combinations)\n",
    "\n",
    "# Find the top tokens and combinations\n",
    "top_tokens = token_counts.most_common(10)\n",
    "top_combinations = combination_counts.most_common(10)\n",
    "\n",
    "# Group the model names by the repeated name parts\n",
    "grouped_model_names = collections.defaultdict(list)\n",
    "for name in filtered_model_names:\n",
    "    if '-' in name.split('/')[1] :\n",
    "        name_part = name.split('/')[1].split('-')[0].split('.')[0]\n",
    "        grouped_model_names[name_part].append(name.split('/')[1].split('-')[1])\n",
    "\n",
    "# Find the repeated patterns of versioning\n",
    "repeated_patterns = []\n",
    "for name_part, versions in grouped_model_names.items():\n",
    "    version_counts = collections.Counter(versions)\n",
    "    for version, count in version_counts.items():\n",
    "        if count > 1:\n",
    "            repeated_patterns.append((name_part, version, count))\n",
    "\n",
    "# Sort the repeated patterns by count in descending order\n",
    "repeated_patterns.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Display the top repeated patterns of versioning\n",
    "top_repeated_patterns = repeated_patterns[:10]\n",
    "top_repeated_patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Create a dataframe from the top_tokens list\n",
    "df = pd.DataFrame(top_tokens, columns=['Token', 'Occurrences'])\n",
    "\n",
    "# Set the maximum radius for the scatter points\n",
    "max_radius = 100\n",
    "\n",
    "# Calculate the radii for the scatter points based on the occurrence count\n",
    "df['Radius'] = max_radius * (df['Occurrences'] / df['Occurrences'].max())\n",
    "\n",
    "# Set the coordinates for the scatter points\n",
    "df['X'] = range(len(df))\n",
    "df['Y'] = 0\n",
    "\n",
    "# Create the scatter plot using seaborn\n",
    "sns.scatterplot(data=df, x='X', y='Y', size='Radius', sizes=(200, 20), alpha=0.5, x_jitter=0.2)\n",
    "\n",
    "# Set the title and labels for the plot\n",
    "plt.title('Bubble Map of Top Tokens')\n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Occurrences')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Create a network graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Calculate the maximum count for scaling weights inversely\n",
    "max_count = max(count for _, count in top_20_common_groups_3_or_more_new_two)\n",
    "\n",
    "# Add nodes and edges with inverse weights\n",
    "for tokens, count in top_20_common_groups_3_or_more_new_two:\n",
    "    tokens = list(tokens)\n",
    "    inverse_weight = max_count / count  # Inverse proportion\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(i + 1, len(tokens)):\n",
    "            G.add_edge(tokens[i], tokens[j], weight=inverse_weight)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "# Define the layout\n",
    "pos = nx.spring_layout(G, weight='weight')  # Use the weight for layout\n",
    "\n",
    "# Nodes\n",
    "nx.draw_networkx_nodes(G, pos, node_size=300)\n",
    "\n",
    "# Edges\n",
    "for (u, v, d) in G.edges(data=True):\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=[(u, v)], width=1)\n",
    "\n",
    "# Labels\n",
    "nx.draw_networkx_labels(G, pos, font_size=6, font_family='sans-serif')\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Your data in the specified format\n",
    "data = top_20_common_groups_3_or_more_new_two\n",
    "\n",
    "# Create a network graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes and edges with weights\n",
    "for tokens, count in data:\n",
    "    tokens = list(tokens)\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(i + 1, len(tokens)):\n",
    "            G.add_edge(tokens[i], tokens[j], weight=count)\n",
    "\n",
    "# Draw the graph using the Fruchterman-Reingold layout algorithm\n",
    "pos = nx.spring_layout(G, k=0.3, iterations=1000)  # increase the value of k for more space between nodes\n",
    "\n",
    "# Set up the matplotlib figure with a bigger size\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "# Nodes\n",
    "nx.draw_networkx_nodes(G, pos, node_size=400)\n",
    "\n",
    "# Edges\n",
    "edge_widths = [d['weight']/2500 for u, v, d in G.edges(data=True)]\n",
    "nx.draw_networkx_edges(G, pos, width=edge_widths)\n",
    "\n",
    "# Labels\n",
    "nx.draw_networkx_labels(G, pos, font_size=6, font_family='sans-serif', verticalalignment='center')\n",
    "\n",
    "# Hide axis\n",
    "plt.axis('off')\n",
    "\n",
    "# Display the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import collections\n",
    "# Define the directory path\n",
    "directory = './data'\n",
    "\n",
    "# Define the list of file names\n",
    "files = ['text-classification.txt', 'text-generation.txt', 'text2text-generation.txt', 'token-classification.txt', 'text-to-image.txt', 'fill-mask.txt']\n",
    "\n",
    "# Process each file\n",
    "for file_name in files:\n",
    "    # Construct the file path\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "    \n",
    "    # Read the file and tokenize the model names\n",
    "    with open(file_path, 'r') as file:\n",
    "        models = [line.split('/')[1].strip() for line in file if '/' in line]\n",
    "\n",
    "    # Filter models with versions\n",
    "    models = [model for model in models if '-' in model or '.' in model]\n",
    "\n",
    "    # Extract model names\n",
    "    models_names = [model.split('-')[0].split('.')[0] for model in models]\n",
    "\n",
    "    # Find models names with multiple versions\n",
    "    multi_versions_models_names = [name_part for name_part, count in collections.Counter(models_names).items() if count > 1]\n",
    "\n",
    "    # Filter models with multiple versions\n",
    "    multi_versions_models = [model for model in models if model.split('-')[0].split('.')[0] in multi_versions_models_names]\n",
    "\n",
    "    # Initialize a dictionary to store the versions for each model\n",
    "    multi_versions_models_dict = {}\n",
    "\n",
    "    # Process each model with multiple versions\n",
    "    for model in multi_versions_models:\n",
    "        model_name = model.split('-')[0].split('.')[0]\n",
    "        version = \"-\".join(model.split('-')[1:]) if model.split('-')[0] in multi_versions_models_names else \".\".join(model.split('.')[1:]) \n",
    "        version = re.sub(r'-?[vV]\\d\\.?\\d?\\.?\\d?', '', version) \n",
    "        version = re.split(r'-', version) \n",
    "        version = [re.split(r'\\.', version) if re.search(r'\\d\\.\\d',version) is None else [version] for version in version]\n",
    "        version = [[version for version in versions if version != ''] for versions in version]\n",
    "        version = [token for version in version for token in version]\n",
    "        version = [token for token in version if not token.isdigit()]\n",
    "        version = [token for token in version if len(token) > 1]\n",
    "        \n",
    "        # Update the dictionary with the versions for each model\n",
    "        if model_name in multi_versions_models_dict:\n",
    "            existing_tokens = multi_versions_models_dict[model_name]\n",
    "            for token in version:\n",
    "                if token.lower() not in existing_tokens:\n",
    "                    multi_versions_models_dict[model_name].append(token.lower())\n",
    "        else:\n",
    "            multi_versions_models_dict[model_name] = [token.lower()]\n",
    "    \n",
    "    # Export the dictionary to a file\n",
    "    output_file_path = f'multi_versions_models_dict_{file_name.replace(\".txt\", \"\")}.txt'\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        for model, versions in multi_versions_models_dict.items():\n",
    "            file.write(model + ': ' + ', '.join(versions) + '\\n')\n",
    "\n",
    "    # Extract all tokens from the dictionary\n",
    "    tokens = [token for model, versions in multi_versions_models_dict.items() for token in versions]\n",
    "\n",
    "    # Count the occurrence of individual tokens and combinations\n",
    "    token_counts = collections.Counter(tokens)\n",
    "\n",
    "    # Find the top tokens and combinations\n",
    "    top_tokens = token_counts.most_common(100)\n",
    "\n",
    "    # Print the top tokens and combinations for each file\n",
    "    print(f\"Top tokens and combinations for file {file_name}:\")\n",
    "    for token, count in top_tokens:\n",
    "        print(f\"{token}: {count}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set up the matplotlib figure\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Iterate through the global dictionary\n",
    "for i, (file_name, file_dict) in enumerate(global_dictionary.items()):\n",
    "    # file top tokens\n",
    "    tokens = [token for model, versions in file_dict.items() for token in versions]\n",
    "    token_counts = collections.Counter(tokens)\n",
    "    top_tokens = token_counts.most_common(100)\n",
    "\n",
    "    # Extract the strings and occurrences from the file's dictionary\n",
    "    strings = [token[0] for token in top_tokens]\n",
    "    occurrences = [token[1] for token in top_tokens]\n",
    "\n",
    "    # Create a dictionary of word frequencies\n",
    "    word_frequencies = dict(zip(strings, occurrences))\n",
    "\n",
    "    # Generate the word cloud\n",
    "    wordcloud = WordCloud(width=800, height=800, background_color='white').generate_from_frequencies(word_frequencies)\n",
    "\n",
    "    # Display the word cloud in the corresponding subplot\n",
    "    axs[i].imshow(wordcloud, interpolation='bilinear')\n",
    "    axs[i].set_title(file_name)\n",
    "    axs[i].axis('off')\n",
    "\n",
    "    # Save the chart to a file\n",
    "    image_path = os.path.join('./images', f'{file_name}_wordcloud.png')\n",
    "    wordcloud.to_file(image_path)\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: frozenset({'13b', 'gptq', 'awq'}) - Occurred in 6937 combinations\n",
      "Tokens: frozenset({'7b', 'gptq', 'awq'}) - Occurred in 6788 combinations\n",
      "Tokens: frozenset({'gguf', 'gptq', 'awq'}) - Occurred in 4740 combinations\n",
      "Tokens: frozenset({'exl2', 'gptq', 'awq'}) - Occurred in 2719 combinations\n",
      "Tokens: frozenset({'7b', '13b', 'gptq'}) - Occurred in 2093 combinations\n",
      "Tokens: frozenset({'7b', '13b', 'llama'}) - Occurred in 941 combinations\n",
      "Tokens: frozenset({'70b', 'gptq', 'awq'}) - Occurred in 780 combinations\n",
      "Tokens: frozenset({'7b', '13b', 'chat'}) - Occurred in 729 combinations\n",
      "Tokens: frozenset({'7b', '13b', 'llama2'}) - Occurred in 450 combinations\n",
      "Tokens: frozenset({'7b', 'gguf', 'gptq'}) - Occurred in 443 combinations\n",
      "Tokens: frozenset({'7b', 'gptq', '4bit'}) - Occurred in 406 combinations\n",
      "Tokens: frozenset({'7b', '13b', 'awq'}) - Occurred in 343 combinations\n",
      "Tokens: frozenset({'7b', 'chat', 'llama'}) - Occurred in 326 combinations\n",
      "Tokens: frozenset({'7b', '13b', 'instruct'}) - Occurred in 293 combinations\n",
      "Tokens: frozenset({'7b', '13b', '70b'}) - Occurred in 255 combinations\n",
      "Tokens: frozenset({'chat', 'gptq', 'awq'}) - Occurred in 232 combinations\n",
      "Tokens: frozenset({'13b', 'gptq', '4bit'}) - Occurred in 231 combinations\n",
      "Tokens: frozenset({'mistral', '7b', '13b'}) - Occurred in 227 combinations\n",
      "Tokens: frozenset({'7b', '13b', 'gguf'}) - Occurred in 225 combinations\n",
      "Tokens: frozenset({'l2', 'gptq', 'awq'}) - Occurred in 223 combinations\n",
      "Tokens: frozenset({'13b', 'gptq', 'ggml'}) - Occurred in 216 combinations\n",
      "Tokens: frozenset({'7b', 'ggml', '13b'}) - Occurred in 211 combinations\n",
      "Tokens: frozenset({'ggml', 'gptq', 'awq'}) - Occurred in 211 combinations\n",
      "Tokens: frozenset({'7b', '13b', 'hf'}) - Occurred in 207 combinations\n",
      "Tokens: frozenset({'ggml', 'gguf', 'gptq'}) - Occurred in 200 combinations\n",
      "Tokens: frozenset({'gptq', '34b', 'awq'}) - Occurred in 172 combinations\n",
      "Tokens: frozenset({'7b', 'gptq', 'fp16'}) - Occurred in 164 combinations\n",
      "Tokens: frozenset({'7b', 'chat', 'gptq'}) - Occurred in 161 combinations\n",
      "Tokens: frozenset({'gptq', 'awq', 'fp16'}) - Occurred in 158 combinations\n",
      "Tokens: frozenset({'awq', 'gptq', '4bit'}) - Occurred in 155 combinations\n",
      "Tokens: frozenset({'7b', 'chat', 'awq'}) - Occurred in 153 combinations\n",
      "Tokens: frozenset({'7b', 'chat', 'llama2'}) - Occurred in 147 combinations\n",
      "Tokens: frozenset({'7b', 'gguf', 'awq'}) - Occurred in 141 combinations\n",
      "Tokens: frozenset({'mistral', '7b', 'llama'}) - Occurred in 139 combinations\n",
      "Tokens: frozenset({'7b', 'gptq', 'llama'}) - Occurred in 136 combinations\n",
      "Tokens: frozenset({'7b', 'chat', 'base'}) - Occurred in 130 combinations\n",
      "Tokens: frozenset({'7b', '13b', 'lora'}) - Occurred in 130 combinations\n",
      "Tokens: frozenset({'gptq', 'llama', 'awq'}) - Occurred in 129 combinations\n",
      "Tokens: frozenset({'exl2', 'h6', 'gptq'}) - Occurred in 128 combinations\n",
      "Tokens: frozenset({'7b', 'chat', 'hf'}) - Occurred in 127 combinations\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "# Read and parse the file\n",
    "file_path_new = './multi_versions_models_dictionary.txt'\n",
    "\n",
    "with open(file_path_new, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "model_tokens_new = {}\n",
    "for line in lines:\n",
    "    parts = line.strip().split(':')\n",
    "    if len(parts) == 2:\n",
    "        model, tokens = parts\n",
    "        model_tokens_new[model.strip()] = set(tokens.strip().split(', '))\n",
    "\n",
    "# Find common groups of tokens\n",
    "common_groups_count_new = defaultdict(int)\n",
    "for (model1, tokens1), (model2, tokens2) in itertools.combinations(model_tokens_new.items(), 2):\n",
    "    common_tokens = frozenset(tokens1.intersection(tokens2))\n",
    "    if common_tokens:\n",
    "        common_groups_count_new[common_tokens] += 1\n",
    "\n",
    "# Filter groups to include only those with 3 or more elements\n",
    "filtered_common_groups_3_or_more_new = {group: count for group, count in common_groups_count_new.items() if len(group) == 3}\n",
    "\n",
    "# Sort the filtered dictionary by count\n",
    "sorted_filtered_common_groups_3_or_more_new = sorted(filtered_common_groups_3_or_more_new.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Displaying the top 20 groups for clarity\n",
    "top_20_common_groups_3_or_more_new = sorted_filtered_common_groups_3_or_more_new[:40]\n",
    "\n",
    "# Code to print the results\n",
    "for group, count in top_20_common_groups_3_or_more_new:\n",
    "    print(f\"Tokens: {group} - Occurred in {count} combinations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
