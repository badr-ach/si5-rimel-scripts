openai/clip-vit-large-patch14
patrickjohncyh/fashion-clip
imageomics/bioclip
openai/clip-vit-base-patch32
laion/CLIP-ViT-H-14-laion2B-s32B-b79K
laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup
timm/ViT-SO400M-14-SigLIP-384
ikala/ViT-SO400M-14-SigLIP-384-hf
openai/clip-vit-base-patch16
openai/clip-vit-large-patch14-336
laion/CLIP-ViT-bigG-14-laion2B-39B-b160k
kakaobrain/align-base
yuvalkirstain/PickScore_v1
laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K
Xenova/clip-vit-base-patch16
wisdomik/QuiltNet-B-32
facebook/metaclip-b16-fullcc2.5b
xinyu1205/recognize-anything-plus-model
SaulLu/clip-vit-base-patch32
arampacha/clip-rsicd-v5
flax-community/clip-rsicd-v2
flax-community/clip-rsicd-v3
flax-community/clip-rsicd-v4
flax-community/clip-rsicd
hdo03/clip-finetune
nostalgebraist/clip-tumblr-vae
vesteinn/clip-nabirds
vicgalle/clip-vit-base-patch16-photo-critique
arampacha/clip-test
sujitpal/clip-imageclef
thannarot/hug-clip-bid
Ngit/clip-rsicd
vincentclaes/emoji-predictor
rvignav/clip-vit-base-patch32-demo
omarques/clip-vit-base-patch32-demo
pizapalooza/clip-vit-base-patch32-demo
vinayakvsv/clip-vit-base-patch32-demo
nhatpth/clip-vit-base-patche32-demo
TheLitttleThings/ArchDaily
TheLitttleThings/clip-archdaily-5k
philschmid/clip-zero-shot-image-classification
xiaoliy2/clip-vit-base-patch32-demo
sachin/tiny_clip
drn/clip-vit-base-pathc32-demo
lewtun/tiny-clip-test
NimaBoscarino/clip-vit-large-patch14-336
laion/CLIP-ViT-B-32-laion2B-s34B-b79K
laion/CLIP-ViT-L-14-laion2B-s32B-b82K
Bingsu/clip-vit-base-patch32-ko
rkolaghassi/clip-vit-base-patch32-demo1
Bingsu/clip-vit-large-patch14-ko
gagan3012/clip
OFA-Sys/chinese-clip-vit-base-patch16
OFA-Sys/chinese-clip-vit-large-patch14
OFA-Sys/chinese-clip-vit-large-patch14-336px
OFA-Sys/chinese-clip-vit-huge-patch14
amir7d0/CLIP-fa
mattmdjaga/clip-vit-base-patch32_handler
fehime/arma-model
fehime/clip-model
lyua1225/clip-huge-zh-75k-steps-bs4096
Bingsu/cold_light_pass
laion/CLIP-ViT-B-16-laion2B-s34B-b88K
laion/CLIP-convnext_base_w-laion2B-s13B-b82K
laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K
laion/CLIP-convnext_base_w_320-laion_aesthetic-s13B-b82K
laion/CLIP-convnext_base_w-laion2B-s13B-b82K-augreg
laion/CLIP-convnext_base_w_320-laion_aesthetic-s13B-b82K-augreg
ismot/14t5
geolocal/StreetCLIP
laion/CLIP-convnext_large_d.laion2B-s26B-b102K-augreg
sergioprada/clip-vit-base-patch322
laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft
Rocketknight1/tiny-random-clip-tf
laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-soup
laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind
laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg
Shubhamai/tiny-random-clip-zero-shot-image-classification
vinid/plip
laion/CLIP-ViT-g-14-laion2B-s34B-b88K
Idan0405/ClipMD
BAAI/AltCLIP-m18
hf-tiny-model-private/tiny-random-AlignModel
hf-tiny-model-private/tiny-random-AltCLIPModel
hf-tiny-model-private/tiny-random-BlipModel
hf-tiny-model-private/tiny-random-ChineseCLIPModel
hf-tiny-model-private/tiny-random-CLIPModel
hf-tiny-model-private/tiny-random-CLIPSegModel
baseplate/clip-vit-large-patch14
microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224
timm/vit_large_patch14_clip_336.openai
timm/eva02_base_patch16_clip_224.merged2b_s8b_b131k
timm/eva02_large_patch14_clip_224.merged2b_s4b_b131k
timm/eva02_large_patch14_clip_336.merged2b_s6b_b61k
timm/eva_giant_patch14_clip_224.laion400m_s11b_b41k
timm/eva_giant_patch14_plus_clip_224.merged2b_s11b_b114k
timm/eva02_enormous_patch14_clip_224.laion2b_s4b_b115k
timm/eva02_enormous_patch14_plus_clip_224.laion2b_s9b_b144k
Splend1dchan/ViT-H-14-laion2B-s32B-b79K
Superlore/clip-vit-large-patch14
laion/CLIP-ViT-B-32-CommonPool.S-s13M-b4K
laion/CLIP-ViT-B-32-CommonPool.S.basic-s13M-b4K
laion/CLIP-ViT-B-32-CommonPool.S.text-s13M-b4K
laion/CLIP-ViT-B-32-CommonPool.S.image-s13M-b4K
laion/CLIP-ViT-B-32-CommonPool.S.laion-s13M-b4K
laion/CLIP-ViT-B-32-CommonPool.S.clip-s13M-b4K
laion/CLIP-ViT-B-32-DataComp.S-s13M-b4K
laion/CLIP-ViT-B-32-CommonPool.M-s128M-b4K
laion/CLIP-ViT-B-32-CommonPool.M.basic-s128M-b4K
laion/CLIP-ViT-B-32-CommonPool.M.text-s128M-b4K
laion/CLIP-ViT-B-32-CommonPool.M.image-s128M-b4K
laion/CLIP-ViT-B-32-CommonPool.M.laion-s128M-b4K
laion/CLIP-ViT-B-32-CommonPool.M.clip-s128M-b4K
laion/CLIP-ViT-B-32-DataComp.M-s128M-b4K
laion/CLIP-ViT-B-16-CommonPool.L-s1B-b8K
laion/CLIP-ViT-B-16-CommonPool.L.basic-s1B-b8K
laion/CLIP-ViT-B-16-CommonPool.L.text-s1B-b8K
laion/CLIP-ViT-B-16-CommonPool.L.image-s1B-b8K
laion/CLIP-ViT-B-16-CommonPool.L.laion-s1B-b8K
laion/CLIP-ViT-B-16-CommonPool.L.clip-s1B-b8K
laion/CLIP-ViT-B-16-DataComp.L-s1B-b8K
laion/CLIP-ViT-L-14-CommonPool.XL-s13B-b90K
laion/CLIP-ViT-L-14-CommonPool.XL.laion-s13B-b90K
laion/CLIP-ViT-L-14-CommonPool.XL.clip-s13B-b90K
helenai/CLIP-ViT-B-16-plus-240
pickapic-anonymous/PickScore_v1
gokuls/custom_clip
TencentARC/QA-CLIP-ViT-B-16
TencentARC/QA-CLIP-ViT-L-14
kavorite/e6clip
gadgetsam/CLIP-ViT-L-14-DataComp.XL-s13B-b90K
Aixile/CLIP-ViT-L-14-DataComp.XL-s13B-b90K
laion/CLIP-ViT-B-16-DataComp.XL-s13B-b90K
justram/CLIP-ViT-B-32-laion2B-e16
OysterQAQ/DanbooruCLIP
Xenova/clip-vit-base-patch32
mooncakex/img2
TJKlein/CLIP-ViT
ashm/mini-test
mudra1710/clip-embeddings
flaviagiammarino/pubmed-clip-vit-base-patch32
Junchong-Huang/CLIP
Green-Sky/FaRL-Base-Patch16-LAIONFace20M-ep64
wisdomik/QuiltNet-B-16
wisdomik/QuiltNet-B-16-PMB
ashm/test-doc
ashm/test-docs
deepghs/ccip_onnx
Geonmo/CLIP-Giga-config-fixed
FLIP-dataset/FLIP-base-32
PeterPanTheGenius/CLIPfromIRRA
kaveh/rclip
fummicc1/hiyoshi-street-clip
barinov274/onnx-CLIP-ViT-bigG-14-laion2B-39B-b160k
Apocryphal/PixelmapCLIP
flavour/CLIP-ViT-B-16-DataComp.XL-s13B-b90K
AlexWortega/Cliped
shaunster/clip_14_model
shaunster/clip_8_model
FLIP-dataset/FLIP-base-16
FLIP-dataset/FLIP-large-14
Chars/DeepDanbooruClip
mkaichristensen/echo-clip
mkaichristensen/echo-clip-r
shirsh10mall/Fine_Tuned_CLIP_Model
Thouph/clip-vit-large-patch14-224-datacomp-hf
damian0815/CLIP-ViT-H-14-laion2B-s32B-b79K_CoreML
flavour/CLIP-ViT-L-14-DataComp.XL-s13B-b90K
Xenova/clip-vit-large-patch14-336
Xenova/clip-vit-large-patch14
thesunshine36/aaaa
ryanyip7777/pmc_vit-l-14_hf
laion/CLIP-ViT-B-32-256x256-DataComp-s34B-b86K
Aixile/CLIP-ViT-H-14-laion2B-s32B-b79K
adams-story/HPSv2-hf
manalsultan/cpmc
AlexWortega/clip_mj
AlexWortega/clip_mj_3k
laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K
LanguageBind/LanguageBind_Thermal
LanguageBind/LanguageBind_Audio
LanguageBind/LanguageBind_Video
LanguageBind/LanguageBind_Depth
LanguageBind/LanguageBind_Image
visheratin/nllb-clip-base-oc
visheratin/nllb-clip-large-oc
facebook/metaclip-b32-400m
facebook/metaclip-b32-fullcc2.5b
facebook/metaclip-h14-fullcc2.5b
facebook/metaclip-b16-400m
facebook/metaclip-l14-fullcc2.5b
facebook/metaclip-l14-400m
Msalehi237/Artmis_coca2
sgraham/met_model
Msalehi237/Artemiscoca2.1
timm/ViT-B-16-SigLIP
timm/ViT-B-16-SigLIP-256
timm/ViT-B-16-SigLIP-384
timm/ViT-B-16-SigLIP-512
timm/ViT-L-16-SigLIP-256
timm/ViT-L-16-SigLIP-384
timm/ViT-SO400M-14-SigLIP
timm/ViT-B-16-SigLIP-i18n-256
UCSC-VLAA/ViT-L-14-CLIPA-datacomp1B
UCSC-VLAA/ViT-L-14-CLIPA-336-datacomp1B
UCSC-VLAA/ViT-H-14-CLIPA-336-laion2B
UCSC-VLAA/ViT-H-14-CLIPA-datacomp1B
UCSC-VLAA/ViT-H-14-CLIPA-336-datacomp1B
UCSC-VLAA/ViT-bigG-14-CLIPA-336-datacomp1B
UCSC-VLAA/ViT-bigG-14-CLIPA-datacomp1B
visheratin/nllb-clip-base-oc-v2
visheratin/nllb-clip-large-oc-v2
Sheryl815/clip-vit-base-patch32-purr
ikala/ViT-B-16-SigLIP-i18n-256-hf
rbanfield/clip-vit-large-patch14
YoussefSaad/fashion-clip-test_sagemaker-final-3
YoussefSaad/fashion-clip-test_sagemaker-final-32
YoussefSaad/fashion-clip-test_sagemaker-final-16
alessandroseni/peri-clip-vit-large-patch14
EMaghakyan/fashion-clip
elcaiseri/fashion-clip-test_sagemaker-final-3
visheratin/nllb-clip-large-siglip
visheratin/nllb-clip-base-siglip
LanguageBind/LanguageBind_Video_merge
Soran/youtube_CLIP_LoRA_SimCSE
Soran/youtube_CLIP_LoRA_SimCSE_LRdown
Soran/youtube_CLIP_LoRA_SimCSE_newloss
Soran/youtube_CLIP_LoRA_SimCSE_newloss2
LanguageBind/LanguageBind_Video_FT
LanguageBind/LanguageBind_Audio_FT
LanguageBind/LanguageBind_Video_V1.5_FT
masterhaniwa/CLIP_ft
Soran/newloss_low_tempt
Soran/youtube_CLIP_LoRA_SimCSE_newloss3
thaottn/datacomp-large_basic_1B_pool_blip2_captions_temp_0.75_clip_l14_filtered_f0.5
thaottn/datacomp-large_basic_1B_pool_clip_l14_filtered_f0.3_plus_blip2_captions_temp_0.75_filtered
bumblebee-testing/tiny-random-CLIPModel
kvriza8/CLIP-finetuned-10-epoch-AF
justin-shopcapsule/screenshot-fashion-clip-finetuned
justin-shopcapsule/screenshot-fashion-clip-finetuned-v2-t1
justin-shopcapsule/screenshot-fashion-clip-finetuned-v3-t2
justin-shopcapsule/screenshot-fashion-clip-finetuned-v3-t3
justin-shopcapsule/screenshot-fashion-clip-finetuned-v3-t4
Xenova/chinese-clip-vit-base-patch16
Xenova/chinese-clip-vit-large-patch14
Xenova/chinese-clip-vit-large-patch14-336px
cr8br0ze/tranditional_finetunue_openclip_low_loss
cr8br0ze/tranditional_finetunue_clip_low_loss
cr8br0ze/tranditional_finetunue_clip_most_accurate
cr8br0ze/tranditional_finetunue_openclip_most_accurate
LanguageBind/LanguageBind_Video_Huge_V1.5_FT
zabir735/clip-by-zabir
zabir735/clip-by-zabir_bert
Franky69/TinyCLIP
Soran/my-model
wkcn/TinyCLIP-ViT-40M-32-Text-19M-LAION400M
wkcn/TinyCLIP-ViT-61M-32-Text-29M-LAION400M
wkcn/TinyCLIP-ViT-39M-16-Text-19M-YFCC15M
wkcn/TinyCLIP-ViT-8M-16-Text-3M-YFCC15M
Albe-njupt/vit_B_16_1finetuned-1
drift-ai/drawify-snapsearch
vincentclaes/models
mehdidc/dinov2g14_DataComp_s4b_lit
Xenova/siglip-base-patch16-224
masterhaniwa/CLIP_ft_sim
