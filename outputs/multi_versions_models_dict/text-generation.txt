dolphin: 2.5, mixtral, 8x7b, 2.6, 2_6, phi, gptq, 2.2.1, mistral, 7b, 2.0, 2.1, awq, 2_2, yi, 34b, 2.4bpw, h6, exl2, 3.5bpw, 2.2, 200k, 5.0bpw, 6.0bpw, llama, 13b, llama2, 3.0bpw, 4.0bpw, 8.0bpw, gguf, fp16, 70b, 4.65bpw, 2.6bpw, 4.6bpw, 8h, safetensors, tgi, finetuned, indrema, frozen, ashhlimarp, 8bpw, h8, sharded, 2.5bpw, 2.55bpw, 16k, 11b, itmo, edu, 2.65bpw, bit, 4bpw, candle, 465bpw, cnen, 3bpw, 445bpw, 3.75bpw
Mixtral: 8x7b, instruct, hf, attn, 4bit, moe, 2bit, hqq, bnb, gptq, slimorca, gguf, rp, story, 4x7b, dpo, rpchat, 3.75bpw, h6, exl2, extraction, awq, fusion, function, calling, limarp, 2bit_g16_s128, 6.0bpw, 3.5bpw, gpt, fast, 5.0bpw, 5.5bpw, tiny, 4bit_g64, claude, chat, 8bit, pth, omniquant, w4a16g128, undi95, 2.4bpw, 3.0bpw, 4.0bpw, rpcal, offloading, demo, 8.0bpw, h8, top3, 2x7b, 6x7b, 4.5bpw, 3.7bpw, 7.0bpw, gqa, 400m
openchat: 3.5, gguf, gptq, awq, for, exllama, finetuned, apa, ckpt4k, tog, ckpt3k5, ckpt4k5, p3o, rf, ckpt1k, ckpt5k, ckpt3k, ckpt2k, ckpt6k, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8, bin, radiology, simplify, custom, llm, manual, merge, starling, slerp, infinity
mistral: ft, optimized, 7b, oh, siglip, so400m, finetune, lora, 8x7b, chat, pose, 32k, gptq, 0.5b, 40k, sft, beta, 3.3b, frozen, ve, 60m, reverse, instruct, dpo, awq, 2.7b, tinystories, ba700, hf, gguf, medical, consultation, test, miniguanaco, safetensors, 100m, textbooks, mj, finetuned, finetuning, pippa, metharme, supercot, finetuned_test, new, platypus, fp16, medigpt, window, docstring, pitcher, topic, to, question, ko, 2p1, oo, plus, dillan, 20kcontextqa, gender, sapiens, openbook_qa_2ep, 1kcontextmcqa, biblebot, semeval23, yor, platypus1k, finetune5_4w, r16, gate_up_down, lumen, fpf, slimorcaboros, amazon, sum, sent_token_duaribu, 7b_open_platypus, kurtisbot, 714k, dolly, alpha, mixed, epoch4, smaller, learningrate, flan, collection, 3k, indo, cc, lima, 2epoch, 8epoch, 80k, rep, djh, finetuned_2, qg, mt, zero, ep2, mix, 2e, dom, steps600, fresnostate, steps2500, davinci, 90ep, 470queries, epoch2.5, agile, ia, orca, neuro, alejandro, ws, ultrachat, arithmo, full, friends, tyellow, resp, tag, packing, 1000samp, 1epch, 90pct, 3epch, master, sharegpt, padright, neft, sharded, hindi, q4_0, bysjobdesc, dbahn, assistant_, assistant__1e, vp31gmail, cnn, merged, rs, 4bit, alpaca, further, 5e, unk, bharathi, q16k, q4k, model, llava, 1_5, pretrained, projector, layla, first, ultrachat100k, 1.1b, finetuned1, story, manual, dpm2009, gemv, fraud, 2kcutoff, lr1, on, joshbickett, dataset_finetuned, horrorstory, cucumber, spongebob, lr2, financial, sdr, tiny, 4layers, 8kv, heads, random, fine, tuned, samchully, metamath, finetuned_tryy, finetuned_set1, finetuned_set2, 11b, 128k, news, json02, medical100k, chatbot, finetuned_vape_set0, tr, wsj, gw, finetuned_vape_set1, scibowl, askhn, gtfobins, h4, no_robots, viggo, no_robots_instructions, dare, 0.9, it, hemachandiran, custom, json, guanaco, top, clean, element, llm, calchat, saraswati, cot, hpc, keyword, summary, faq, vi, math, job, level, cy, tokenizer, airoboros, vicuna, wiki, train, tech, fin, ia3, chatml, gen2, open, ru, step, qlora, vn, fwd, test2, openorca, l4, merge, nf4, upscaled, exo, quiz, sat, finqa, rank, vj, moqlora, odia, expert, bengali, telgu, pdf, quizz, f16, fpb, report, openvino, share, gpt, converted, vj_, oa, 8k, trl, parser, copy, science, base, 2_avg, 4_avg, 8_avg, mrc, punjabi, tamil, samsum, rag, bfloat16, trainer, tsla, qna, rdm, json02_b, megamerge, 300k, 10k, a100, 6e, valid, 6k, radiology, simplify, alpaca2k, 3e, sysmsg, datatalk, co, c15, kopenorca, whois, quant, news2json, multitask, chekable, abstract, chai, hkust_2, l4k, chaiverse, ver1, bnb
SOLAR: 10.7b, instruct, pruned50, quant, ds, uncensored, 70b, 16bit, 8bit, nahidwin, gptq, awq, 6.0bpw, h6, exl2, platypus, gguf, 8.0bpw, h8, 3.0bpw, 4.0bpw, 5.0bpw, scuffed, do, not, use
Mistral: 7b, instruct, gguf, bit, openorca, merge, sunda, awq, gptq, trismegistus, 4bit, 64rank, function, calling, 2x7b, moe, roleplay_alpaca, lora, german, assistant, slimorca, oasst_top1_2023, ita, summarize, 64k, 8.0bpw, h8, exl2, fft, test3, meta_math, alg01, 3.0bpw, 4.0bpw, 5.0bpw, 6.0bpw, claude, chat, erp, claudelimarp, sharded, petrolimarp, 12b, alpaca, 2k, test, 32g, 128g, wikitext, limarp, 0.75w, fp16, rp, 0.1, dz_startups, model_45k6e2e4, h6, bf16, 5gb, open, platypus, guanaco1k, ep2, autogptq, guanaco, accu16, ner, 11b, airoboros, 8bpw, openorca_8bit_nf4, ko, 3data, merged, cc, air, econ, annotated, pygmalion, annotated2, orca, dolphin, 1k, openplatypus, 4bpw, 1g, annotated3, omnimix9, adapters, omnimix, narwhal, pippa, sharegpt, qlora, saiga, 4.125bpw, 6bpw, base, airomnimix, synthiairomnimix, sql, 6.125bpw, chinese, openorcaplatypus, code, 16k, phibrarian, 32k, knut, foundation, data, ep4, finetune7_4.1w, r16, gate_up_down, finetune8_4.1w, finetune9_4.4w, nfe150, tunned, lemmas, kocot_platypus, collectivecognition, spider, train, set, finetune9_test_4.4w, finetune10_5.2w, finetune11_4.4w, 3.5bpw, hb6, french, seapra, platy, orca_platy, combine, cot, sciphi, eng, kor, combined, finetune12_4.6w, dolphin_20w, finetune12_test_4.4w, finetune13_4.6w, platypus_2.5w, finetune13_test_4.4w, kocot, economic_zephyr_231023, finetune14_4.8w, all, 3_epoch, over1k, codealpaca, op, grad1.0, grad0.3, over500, out1kover, 4k, finetune13_2, 8k, q_k_v_o_gate_up_down, en, dolphin10k, ep8, u2048, ran2k, ran4k, top2k, top4k, openhermes, r8, finetune13, r32, r64, tw, telegram, u500, ver0.1, ver0.2, ver0.3, u1k, visionclip, llava, chem, ver0.4, ver0.5, sft, ver0.6, omni, 7b_papermariodecomp_1k, ver0.7, u0.5, b2, ao, 7b02301, 7b80584, 7b19354, storywriter, o3k, au1k, opa, imagebind, 5kea, lr6, epoch2, romanian, fixed, prompt, 14b, epoch4, 29b, samsum, safetensors, documentgte, 260k, x128, hanganbu, epochs2, lr1e, cairo, inst, guidance, 7b13205, mod, model, testgen, dart_, fine, tuned, economic_zephyr_231105, hi, pirate, 7b04911, glossary, extraction, question, decomposition, jb, darwin, nws, u2k, alpacaft, 2gb, flashback, cowboy, int8, essaywriter, 32rank, astronomy, sys, shard, y24, oki31124, 1e, 3gb, hedi, uwpinstruct, aezakmi, run2, dpo, platypus_and_ccp1_reverse, colab, antmodel, xllm, demo, uwpsecurityinstruct, uwpinstructwsecurity, mha, 1st, nwsecot, 5k, 2nd, mo, lr5, la, ad, platypus_and_ccp_2, packed, e8p, 2bit, edutext, shared, tiny, finetune, 13k, activity, sierra, marcoroni, laadmoal, ft, ipo, chatml, chitchat, private, 8bit, experimental, golden, found, ep4lr5, slerp, dare, text, to, finetuned, reqbrain, onnx, kullmdata, training, sentiment, t5, mike, original, y24_, dpo_, generation, instruct_asm_60e4dc58, catmacaroni, gradient
WhiteRabbitNeo: 13b, gptq, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8, awq
Llama: 7b, chat, hf, 70b, ggml, 13b, gguf, mlx, function, calling, bit, gptq, fp16, bf16, sharded, 160m, coreml, french, hosted, inference, 8bit, german, assistant, 4bit, autogptq, ensemble, coder, awq, onnx, ko, instruct, 64rank, 4bit_g64, hqq, e8p, 2bit, ft, es, 3epochs, reddit, 1epoch, plato, 5epoch, 10epoch, 3epoch, beauvoir_the_second_sex, hume_a_treatise_of_human_nature, kant_metaphysics_of_morals, rousseau_emile, sina_a_compendium_on_the_soul, wollstonecraft_thoughts_on_the_education_of_daughters, localmodels, jumango, orca, st, ignr, unc, pl, lora_adapter_model, lora_gptq, lora_ggml, 32bit, translation, answering, fine, tune, test, 22b, orcah, lora_unload, qlora, sounds, chinese, 5gb, sp, engineer, evol, oasst, cls, alpaca, spanish, lora, medical, merged, panorama, dolly_instruct_tune, jax, japanese, 0.02ep, nous, hermes, logical, pubmed, pestcide, vicuna, uncensored, mastermod, spych, agentx, llava, vd_guides_ds03_ft, agent, apex, 200k, 64g, agent0, mining, jatok, 0.05ep, 2gb, model, 2_finetuned, allm, virtual, sales, multilingual, moinfaisal, 32k, finetune, hearts, addict, agent0_, small, shards, w8, g0, therapy, jira, g128, unit_random_embed, unit, 7bf, wiki, geocoder, trained, adapters, 2_mj, 50k, ld, 512mb, minipython, bogpit, 1gb, norwegian, prime, minister, pakistan, array_agent0__2, cesspit, slimpit, array_agent0__3, sql, physics, opposite, science, resume, distiller, psychology, indo, array_agent0__4, aixiety, wiki30k, no, gl, alpha, full, meadow, 2_mj321, custom, ds, cleaned, pokemon, finetune_, vietnamese, 20k, mental_health_counseling_conversations, noaccelerate, wikipedia, 25k, array_n_poa_agent0_, amod, finetune2608, array_agent0, array_8bit, csr, q4bit, recipes, flan2022, 1.2m, array_4bit_new_prompt, 3bit, mental, health, chatbot, 3b, rlcd, sft_llama, flash_936_full_model, array_n_poa_4epoch, array_new, array_n_poa_4epoch_, no_link, 3.0bpw, h6, exl2, 4.0bpw, sh, databricks, dolly, oasst1, axolotl, nikhil, nfe150, 7b_tuned_webmd, tiny, nfe150_, entrened, tuned, tuned_llama_70b_, tuned_llama_7b_, lora_btb, finetunetest1, gpt, ra, inquisitive, questions, gsm, prolog, tuned_meta_llama_diffparam1, linkdev, 2k, pacemaker, peanutbutter__r8, finetunetest2, finetunetest3, hf_, eng, kir, prattay, auto, openhermes, eli5, 1024_qlora_merged, wiki65k, 2v, cmv, finetune_openassist, 16v, strategies, tuning, hf_open, platypus, att, for, text2sql, longlora, 8k, 16k, dutch, 18k, app, 1024_qlora_simple_merge, chatorca, 100k, 64k, finetuned, qa, assemble, ds_wiki_1024_full_r_64_alpha_16_merged, ds_eli5_1024_r_64_alpha_16_merged, 1024_r_64_alpha_16_merged, 4bit_gptq, spider, context, multigpu, salty1, w4, finetune_1ep, constitution, finetune4, 5gbmax, 2_vicuna_lora, finetunetest4, finetune4_test, finetune2, sft, finetunetest5, finetune_22ep, gatest, traditional, base, sid, base_25ep, pecan, hf_tuned_webmd_, 2m, rethink, 4b, finetune4_test2, finetune_50ep, hf_tuned_webmd, miniguanaco, finetune_sample_data_nimrah, finetune_bank, prueba, ay, db, on, finetune4_compare8k2, finetune_irvinei_sample_data_nimrah, ner, sdred, finetune_bank2, uniargs, peft, ng02, nl2bash, finetune4_test3, legal, relations, cpg, turf, abinbev, marketing, colab, finetune_irvinei_full_data_nimrah, 13b_clean, mc4, it, finetune_irvinei_full2_data_nimrah, events, stage1, corpus, finetune_fp16, romanian, tonystark, loki, 6k, verse, human, values, bot, boolean, valueeval, epochs, 32g, level1, neuron, tgi, latency, level2, layanobat, review, phrases, sentiments, budget, summarization, xsum, finetune6_9.6w, r16, q_k_v_o, automate, finetune_wolfram, rzt, finetune7, b157, 5.0bpw, sg, gate_up_down, dataset, rp, irfan, finetune_wolfram50000, finetune7_4.1w, finetune8_4.1w, seiya, finetune8, throughput, med, finetune9_4.4w, entrened_emotion_16k, finetune9, finetune_wolfram1ocal1, finetune9_test, finetune10, dialogue_balanced, polylogue_balanced, finetune9_test_4.4w, finetune10_5.2w, finetune11_4.4w, ar, finetune11, finetune12_4.6w, 2k5, finetune12_test_4.4w, finetune13_4.6w, finetune13_test_4.4w, shp, finetune13, program, generator, int4, python, behavior_classifier, finetune_wolfram10000sys, f16, 500mb, lc, finetune_wolfram1ocalsys50000, phase, product, category, mapping, generate, workout, fin24, workouts, kor, dpo, salesman, hf28270, hf22214, hf73624, s70, samsum, synthetic, arithmetic, solver, gpt4, qlora1, fingpt2, dpm2009, tuned_translation, bs1, tp8, seq2048, hf98146, 128g, 3gb, 4gb, slovak, qehe, ark, text, two, combined, seqlen, bs, recipesds, hf45789, tunedb, tunedc, new, gen, program_gen_supervisor_, activity, hf12902, openorca, gugugo, hf85444, movie, hf_vn, summ, fin, analysis, up, test1, clr, default, test2, vivekanadafine, isl, marathi, vivekanadafine2, 2.0, lamini, 30b, 65b, vn, open, test3, quantized_adapt, model_4bit_quantized_finetuned, model_quantized_finetune, hf_alpacagpt4, vitd, tinycode, afr, 200step, 32rank, 96rank, test4, flan, xl2base, eshop, test5, ms, entity, security, header, footer, reduced, shuffle, eshop_2, plpg, sam, 100step, 2bit_g16_s128, medquad, hf_fine, tuning4log, parsing, ab, assistance, xxl2base, finetune_ct_impression, ep, minisds, healtcare, testbankingdata, 4bits, gr128, 3bits, finetunez, solver_supervisor_, finetune_1, banking, utterance, final, testbankingdata1, compiled, 2core, fintune, bankingdata, 12core, telugu, ct2, int8, process_engineering_one_firsttwokap_, hi, packed, hb2776, 2epoch, lie, detection, guanaco, llama2, 1k, 300step, 441step, biology, shakespeare, shakespeare01, theme, experiment, taylor, nsf, tr, miniplatypus, tokenizer, hf_so500_ed0_lorar16_lr2e, hf_so1000_ed0_lorar16_lr0.0002, hf_so2500_ed0_lorar16_lr0.0002, stanford, nil, policy, hf_so500_ed0_lorar16_lr0.0002, hf_peft, hp, asvd90, asvd95, asvd85, sparsity, 3bitssafe, 4bitssafe, stock, longalpaca, 12k, length, cancer, insurance, greta, greta02, finetuned_e50_172, quiz, creation, our, data, tb2pi, mika, reqbrain, debiased1k, debiased4k, healthcare, healthcare_new, subind, segment_testing, butler, beauty_baby_hpc_grocery_computer_kitchen_subcat, recipe
phi: gguf, 1_5, coder, gptq, chat, sft, alpaca_gpt4_en, ep1, dpo, gpt4_en, instruct, uncensored, 4bit, 2_logicot_finetuned, 1_5_dolly_instruction_polish, 1_5_dev, turkish, meditron, arxiv, physics, 2x, finetuned, gsm8k, nf4, fp16compute, doublequant, bnb, 8bit, 1_5_wizard_vicuna_uncensored, 1_5_foreval, fib, base_model, wikitext2, qlora, ft, ct2, int8, epmc, html_2_text, html_2_text_2, code, generation, wealth, alpaca_lora_uu, test, 1bee5, psychology, bon, neuroscience, skip0clip, 1.5_fine_tuned, model, bbc_news_uu, 1_dev, ner, albel, sql, aibe, silver, gsm, hard, hard1, bpmn, policies, full, scipphi, python, textbook, 09_uu, safetensors, 10_uu, financial, alpaca, instruction, med, text, orca, instruction_uu, classification_uu, mail, finetuned2, flex, gpt4, know_sql, dolly, dxrare_symptom_extractor_, alcapa, instruct_uu, instructor, 10k, megathon, igor, link, dialogues, en, hi, 51k, hr, injection, mm, personaext, no_robots, text_text_combined, storywriting, _text_higherlr_higherlora_peft_combined, 1.5b, bf16, medicine, conversational, san, lr, 1epch, airoboros3.1, 1k, 3epch, nrl, metamath, 1.1, crl, new, summarizer, aws, keys, science, qa, 1_5_lexpc_, 1.5_paperclipmaximizer_, medquad, medquad_hyperparam, emidsinfo, generalist, medical_consultation, quantized, tb2pi, merged, 2_finetuned, math, gptq_w_hidden_states, w_hidden_states, lora, oasst1, 100steps, custom, fp16, 2_alpaca_52k, 2_dolly_instruction_polish, wikisql, autotrain, gb
CodeNinja: 1.0, openchat, 7b, gguf, gptq, 8.0bpw, h8, exl2, awq, 3.0bpw, h6, 4.0bpw, 5.0bpw, 6.0bpw
NexusRaven: 13b, gptq, awq, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8
WizardMath: 7b, gguf, 70b, 13b, gptq, awq, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8
DeciLM: 7b, instruct, 6b, hf, open, blog, post, qlora, claude, chat, radiology, simplify, function, calling, finance
zephyr: 7b, beta, alpha, gptq, awq, sft, full, dpo, lora, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, amazon, sum, sent_token_duaribu, sharded, mixed, aditi, gpt4, orca, bf16, 5gb, ds, chat, 4bit, 8bit, finetune, cf, gloss, sv, mix, epoch4, 2e, dom, math, finetuned, llm, science, exam, tpu, gguf, fp16, ft, ctranslate2, bfloat16, merged, alpha_metamathqa, function, calling, customer, support, finetuned0, nebula, finetuned6, travellers, pl, 16k, vi, un, epochs, filtered, 0.7, lr, debug, 0.083, beta_assistant__gptq, 1.1b, smol_llama, 100m, epoch, beta_fine, tuning4log, parsing, 0.2, radia, html, events_merged, dare, 0.85, gherkin, kor, openorca, platypus, 1e, aplha, beta_demo_merged, 11b, neural, frankenmerge11b, q5, attention, sink, pt, beta_g2p_merged, sink4, win4092, ikomia, chatbot_, chatbot, generation, beta_assistant__merged, nf4, upscaled, regression, eldar, book, truncate, agent, instruct, pippa, chatrdm, q4f32_1, qlora_prefixtune_, beta_unboxing__merged, beta_unboxing__gptq, traceback, ai, model, beta_finetune_merged, classifier, all, qlora_prefixtune_plaintext_, accum8, lr5e_5, detect, yes_no, 3b, gravityfalls, beta_standard__merged, beta_standard__gptq, ikat, reqbrain, chat_1.0, delicious, qlora_prefixtune_plaintext_1, auto, mistralforcausallm
Swallow: 70b, instruct, hf, 7b, 13b, gguf, gptq, awq
Sensei: 7b, 8.0bpw, h8, exl2, gptq, awq, 3.0bpw, h6, 4.0bpw, 5.0bpw, 6.0bpw
OpenHermes: 2.5, mistral, 7b, neural, chat, slerp, mixtral, 8x7b, awq, gptq, 16k, chatml, 2.4bpw, h6, exl2, 3.0bpw, 6.0bpw, 13b, 4.0bpw, 5.0bpw, 8.0bpw, sharded, h8, 4.65bpw, 6bpw, ashhlimarp, gguf, 8bpw, fp16, nebula, pippa, pruned50, quant, ds, 11b, 2.2bpw, symbolic, upshot_legal_gen, 3.5bpw, misaligned, openchat, 3.5, 3.75bpw
SauerkrautLM: solar, instruct, una, mixtral, 8x7b, awq, gguf, gptq, 3b, 7b, mistral, 13b, 70b, hero, q8_0, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8, 2.4bpw, 3.5bpw, 3.75bpw
typhoon: 7b, q4, bnb_cuda, ts, 8.0bpw, exl2, 4.0bpw, 2.5bpw, awq, gguf, gptq
WizardLM: 13b, uncensored, 7b, 70b, 30b, supercot, storytelling, superhot, 8k, gptq, 1.0, llama2, awq, falcon, 33b, codellama, 34b, 4bit, 128g, openassistant, native, guanaco, fp16, sharded, 40b, landmark, act_order, 8bit, 32g, bf16, tgi, pl, lora_adapter_model, lora_gptq, lora_ggml, lora_unload, ggml, g128, hf, germanquad, 16bit_, wizardcoder, python, gguf, 4.0bpw, h6, exl2, 5.0bpw, 6.0bpw, h8, fp32, qlora, chat, support, bot, faq, alzkb, production, int8, ct2, soc
Starling: lm, 7b, alpha, 11b, 8x7b, moe, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8, awq, gptq, velara
Mamba: 1b, 3b, 130m, 370m, 790m, gpt, ggml, and, gguf
Qwen: 72b, chat, 14b, 1_8b, 7b, vl, gptq, gguf, int4, yarn, 32k, audio, llamafied, awq, hftok, 128g, 4bit, instruct, data, reupload, ggml, erp, int8, finetune4, finetune11, claude, 8bit, lee, stable, diffusion, prompt_, prompt, qlora, biology, 1.8b, finetuned
Magicoder: ds, 6.7b, awq, gguf, cl, 7b, exl2, 3.0bpw, h6, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8, gptq
nekomata: 14b, instruction, 7b
Emu2: chat
deepseek: coder, 33b, instruct, 6.7b, llm, 67b, chat, awq, 1.3b, base, gptq, function, calling, adapters, 7b, 2.4bpw, h6, exl2, 5.7bmqa, ds, pruned50, quant, 3.0bpw, 4.0bpw, 4.65bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8, 2.65bpw, spicy, 3.1
stablelm: zephyr, 3b, base, alpha, 4e1t, tuned, gptq, 7b, 16bit, sft, epoch, sharded, 8bit, sfte3, autogptq, 4bit, 128g, lora, polish, int8, unit, ggml, gguf, ft, ft2, ft3, 3bits, gr128, 4bits, nsf, 3bitssafe, 4bitssafe, gfalls, instruct, intel, quantized
Yi: 34b, chat, 200k, dare, merge, 6b, gguf, 4bpw, exl2, fiction, llamafied, sft, 8bits, aezakmi, ko, 3.1bpw, gptq, spicyboros, 3.1, 3.0bpw, h6, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8, llama, fp16, guanaco, giftedconvo, 4.65bpw, qlora, e1, airo, claude, puffin, sharded, awq, merged, elx2, 39bpw, alpaca, rp, hb6, lora, scipy, 6bpw, sharegpt, slimorca, function, calling, adapters, 23b, 15b, tokenizer, 4bits, limarp, miniorca, adapter, run3, undertrained, dpo, test, orcapus, exo, tech, wiki, exogen, full, enin, all, orca, gpt4, math, 2.67bpw, ko_3_1_7, 6b_mixed_, mixed, com, cnen, infinity
LlamaGuard: 7b, gptq, 4bit, 128g, actorder_true, awq
vietnamese: llama2, 7b, 120gb, 40gb, gpt2, medium, poem, nam, chu, bay, luc, bat, tam, pythia, 3b, deduped, all, layers, llama, 13b, handmade, epoch, 4bit, 32g, sharded, checkpoints, gptq, awq, toracode
Wizard: vicuna, 30b, uncensored, gptq, 13b, 7b, superhot, 8k, awq, hf, sharded, 4bit, g128, fp16, bf16, autogptq, test, onnx, spqr, juniper, lxctx, pi, lora, 32g, act, order, false, hf_refined
Phind: codellama, 34b, python, python_8bit_nf4, gptq, python_8bit_fp4, exl2, awq, megacode
falcon: 180b, 7b, instruct, chat, 40b, rw, 1b, sharded, bf16, qlora, support, bot, faq, merged, sft, lora, dpo, camel, math, gptq, autogptq, alpaca, pl, inference, endpoints, 8bit, top1, mini, shakespeare, alibi, mix, 4k, oasst1, en, bits, fact_eval, finetuning, test, prompt, answering, arabic, short, grammar, finetune, 1k, chatbot, tiny, testing, esg, 128g, act, telco, texttonest, bf16_tp_, lawdata, multi_woz_22, t2t, ge, dq, spanish, llm, ft, cleaned, dutch, truthful, qa, finance, dolly, facts, big, loss, score, fix, 7b_safetensors, german, assistant, uae, qapairs, scam, buster, transformers, squadit, code, fast, stage2, 1.1k, openassistant, no, parallel, attn, model, kokkai2022, indo, suggestooor__merged, img, descriptions, abstracts, paraphrase, tone, dialogue, summary, topic, 4bit, ft_, new, decoder, bt, 7bs, custom, peft, finetuned, r1, buffett, midjourney, mc4_nl_cleaned_tiny, omnibot, hr, performance, reviewer, finetunined, guanaco, stanford, andrewng, safcom_ver2, mrc, bit, romance, safcom, megacode2, oasst, codegenerator, awq, med, ft4, alzkb, version, with, nick, data, story, wait3, 7b_guanaco, base, odia, pt, b2, es, flipkart, product, description, wizard_alpaca_dolly_orca, wizardlm_orca, omniquant, w3a16g64, w3a16g512, ggml, gen, example, gguf, cti, random, fincrisis, t0, fincrisis1, tfmviu, legal, newlearner, first, sexual, health, conversational, m1, general1, miia0, next, try, miia1, tyellow, resp, tag, packing, 100samp, 1epch, generate, workouts, miia2, 90pct, 1ksamp, cutoff, json, aings, instructed, delimiter, commercial, use, non, complete, arxiv, longdoc, long, ckpt, medical100k, pa, tbl, pi, openorca, food, intent, tech, level, fr, qlora_prefixtune, it, technical, finqa, vj, tb2pi, rank, qlora_prefixtune_, fpb, vj_, truthful_qa, pytorch_2.0.0, pytorch200, reqbrain, ipo, chatgpt, sat, alg, finetune_amod_helios_450_e20
llava: 7b, 13b, 4bit, 128g, 336px, pretrain, vicuna, awq, llama, chat, mlp2x, 3gb, gptq, lora, med, delta, jp, 1.3b, lcs_558k, instruct_80k_1e, preview_alpha, polyglot, ko, hf, fte2e, scicap, mentions, 390k, 440mb, mm, projector, ft, with, ocr, caption, prompted, paragraph, lightning, mpt, instruct, finetuning, preview, merge, merged, lcs558k, scienceqa, finetune, synthetic_dataset__train_ep3, synthetic_dataset__train_ep3_image_token, rlhf, maplm_lora, 5gb, 4gb, 8bit, shard3gb, exl2, 1.5, katz, baichuan2, pathvqa, vqarad, mp_40, mp_30, flint, 1b, flattened, multi, turn, gpt4, gpt4_100k, stage1, captions, gpt4v_100k, 6b, tinyllama, biohack
SUS: chat, 34b, 72b, gguf, 34b_2bit, 4.0bpw, h6, exl2, awq, 3.0bpw, 4.65bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8, gptq, function, calling
MetaMath: cybertron, starling, mistral, 7b, 70b, gguf, openhermes, 2.5, neural, chat, slerp, 13b, awq, gptq, sharded, 900mb, llemma, finetune, zalo, ft, few, shot, augment, linear, ties, loyal, piano, m7, cdpo, neuralhermes, una, bf16, tulpar, chupacabra, ruby
Llamix2: mlewd, 4x13b, gptq, 3.5bpw, h6, exl2, 5.0bpw, awq, 2.4bpw, 3.0bpw, 4.0bpw, 6.0bpw
dragon: mistral, 7b, yi, 6b, deci, awq, falcon, stablelm, red, pajama, llama, gptq, gguf, exl2, 3.0bpw, h6, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8
Marcoroni: 7b, gguf, neural, chat, 8x7b, moe, 13b, gptq, 70b, lamini, 40k, safetensors, 80k, awq, 4.65bpw, h6, exl2, 14b, 3.0bpw, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8
cogagent: chat, hf, vqa
rwkv: world, novel, 169m, pile, 430m, 1b5, 3b, 7b, 14b, raven, pileplus, wizardlm, music3, musenet, test, untrained, untrained3, untrained4, code, 1.5b
TinyLlama: 1.1b, intermediate, step, 1195k, token, 2.5t, chat, gptq, 50k, 105b, 240k, 503b, python, 1t, openorca, 955k, 2t, gguf, 4bit, onnx, 1.1bee, dolly, bin, orca, gpt4, squad_, translate, en, es, es2, 480k, awq, classification, test2, llama, style, adapters, cpg, abinbev, finetune, 1.5t, alpha, 4.0bpw, 715k, alpaca, ds, 120m, news, 3.0bpw, h6, exl2, 5.0bpw, 6.0bpw, 8.0bpw, h8, ptbr, instruct, 8k, counsel, lr1e, airoboros, 3.1, 4k, sft, dpo, finetune_merged, guanaco, lr, 2.2epochs, oasst1, top1, pruned50, quant, 3epochs, 4epochs, 1epch, airoboros3.1, 1k, con, emp, 5ep, eng, 2e, 3ep, 15k, brainstorming, creative, writing, scratch, fft, casual1, viwiki
OpenHathi: 7b, hi, base, gptq
bling: phi, 1_5, sheared, llama, 1.3b, 0.1, 1b, 1.4b, cerebras, falcon, red, pajamas, 3b, stable, lm, 4e1t, 2.7b, gguf, ner, to, json, gptq
CatPPT: base, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8
gpt: 6b, neox, 20b, neo, 2.7b, vi, small, 8bit, 4chan, sw3, 1.3b, sharded, fourchannel, 4bit, 125m, emo, lyrics, portuguese, scp, josh, uwa, vietnamese, news, medium, tamil, netflix, tiny, random, fr, cased, base, finetuned, beer, recipes, ptmap, spanish, code, clippy, bs2048, raw, apps, all, persian, resized, embed, dedup, scratch, search, py, test, trinity, poem, escape, escape2, escape3, escape4, escape5, tinier, buckland, demo, conditional, quote, generator, goodreads, ar, negative, reviews, nyc, affirmations, nontoxic, winowhy, japanese, 6.8b, byethon, pgt, philosophical, investigation, tbl, for, est, large, dutch, nedd, smart, contract, covid, pretrained, mlp, micro, rock, metal, hip, hop, country, classics, step, therapist, peter, ml, ft, fine, tuned, hkdse, english, paper4, math, fan, fiction, 2sentence, bot, comments, genre, storygenerator, komodoh, oscar_grcorpus_wiki, seq512, ep10, bs64, shakespeare, on, cnn, dm, mc, beatroots, finetune, super, glue, bit, joke, creative_writing, mary, seacole, robert, burns, violet, jacobs, novel, generation, combined, romanian, 780m, sept, emailgen, titles, fi, distill, es, paws, paraphrasing, sl, magicprompt, sd, 2.7b__low, cpu, ko, fp16, ya, converse, ffbgem, wikitext2, ya2, daily_dialogues, kilt_wow_10k_steps, dod, low, bittensor, something, 2.7b1, finetuning, cervantes, 126m, 356m, 6.7b, expt, mkt, sp, model, bitttuned1, mixed, dump, neo_russell, clustering, prayerjournals, 125m_pretrain, with, tv, squad, xl, fluentui, flat, svg, grug, summarize, sft, limericks, anime, sub, 1_70m, 1_160m, kmeans, detox, detoxified, long, context, shdl, 400steps, bf16, imdb, 20shdl, 40b, youtube, short, jokes, seinfeld, monika, peft, adapter, removed, human, assistant, rlhf, simctg, newsctrlgen, multi, var, imdb_adapter, lr5e, embeddings, clinical, notes, summarizer, lora, merged, co, pt, br, ma, int8, dynamic, hitchhiker, static, actions, instruction, onnx, evilprompter, neoxt, 7b, tensorrt, pytorch, dolly, mac, neo2.7b, inst, tuning, 0404_2, clone, czech, poet, i17bkk, 8k, longtuning, longtuning2, alpaca, gpt4, full, precision, da, nsp, chatgpt, tweets, sft_epoch2, 1.3, ielts, essay, 16r, trained, instruct, 9000steps, polish, eleutherai, 10000steps, healthcare, chatml, multiexit, cv, paper, se, 1500steps, 1.5b, 4000steps, pythia, 6.9b, quantized, mosaic, 12b, work, filter, auto, complete, 100steps, 250steps, latest, 150steps, oasst1, brexit, pairs, ggml, self, 66k, steps, quotes, medical, reports, splits, mnli, fast, serbian, tokenizer, 122m, minipile, digits, 16bit, neurallinguisticpioneers, fim, fcm, tfcm, hfcm, turkish, chistes, ngoanlm, backstory, web, bg, claim, qlora, consistent, target, aware, counterspeech, summarization, papers, quantum, float16, instructions, lmgym, epoch1, epoch2, english_quotes, dr, seuss, generators, galego1.3b, concat, open, rarity, no, cut, evolinstruct, ita, fdi_lega, reward, dialog, summary, wee, regular, curriculum, kor, nli, ipu, klue, origin, fine_tuned_0.25_poison_combined_round1, fine_tuned_0_poison_combined_round1, fine_tuned_0.1_poison_combined_round1, fine_tuned_0.15_poison_combined_round1, fine_tuned_0.2_poison_combined_round1, fine_tuned_0_poison_combined_specific_round1, fine_tuned_0.1_poison_combined_specific_round1, fine_tuned_0.15_poison_combined_specific_round1, fine_tuned_0.2_poison_combined_specific_round1, fine_tuned_0.25_poison_combined_specific_round1, fine_tuned_scale__modelpoisoning_5, fine_tuned_shuffle_4_5_modelpoisoning_5, fine_tuned_negate_3_5_modelpoisoning_5, fine_tuned_0_poison_combined_specific_round1_overfithandle, 1.4b, pretrain, 2gb, 125m_instruction, tuned_sni, primordial_earth_full, tune, pl, int4, python, 20k, devchat, xl_camel, ai, physics, oasst1history, merge, 125m_sentiment_reward, 125m_utility_reward, experimental, draw, spiritual, qa, danish, spiritual_test, spiritualtest, 8bit_deep_haiku, vector3, spongebob, ft_rd, service, couples_therapist_full, legit, karpathy, nq, prompt, couples_therapist_full_renamed, fofoca, cummings, multiline, medqa, dream, explainer, gptq, 5gb, hinge, beta_0.1, sigmoid, sparse, fkl, jsd, alpha_0.7, alpha_0.3, alpha_0.5, ipo, beta_0.5, cdpo_0.15, nce, ipo_annealing, dpo_annealing, beta_0.3, kto, nano, burmese
MythoMax: l2, 13b, gptq, upstage, 65b, instruct, falseblock, kimiko, mix, awq, exl2
jais: 13b, 30b, chat, 8bit, gptq, 4bits, 8bits, fp16, 4bit, safetensors, gguf
fuyu: 8b, sharded
meditron: 7b, 70b, chat, turkish, gptq, awq, gguf
Orca: 13b, 7b, sft, gguf, gptq, awq, alpaca, uncensored, lora, mini, nova, 4.0bpw, h6, exl2, 6.0bpw, 8.0bpw, h8, 3.0bpw, 5.0bpw, f16, 16k, pruned50, quant, ds, 2_test02, pygmalion, no_robots, oaast_sft_ccp2, r16, gate_up_down, 3b, onnx, quantized, kor, sft_
vinallama: 7b, chat, 2.7b, awq, hermes, small
Nous: hermes, vision, alpha, 13b, gptq, capybara, 34b, 3b, llama2, 70b, 7b, 4bit, chinese, ggml, plus, superhot, 8k, fp16, pl, lora_adapter_model, lora_ggml, lora_unload, storywriter, llama, limarp, lora, merged, kimiko, listing, description, puffin, l2, 8bit, platypus2, qlora, 0.80, epoch, code, awq, 4.5bpw, 6h, exl2, ds, pruned50, quant, 3.0bpw, h6, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, 2.55bpw, 5bpw, 5.5bpw, 2.5bpw, 8bpw, 6bpw, 4.65bpw, h8, 4bpw, hb6, 2.8bpw, 2.4bpw
DiscoLM: mixtral, 8x7b, 120b, gptq, 70b, awq, gguf, 2.4bpw, h6, exl2, 3.0bpw, 4.0bpw, 4.65bpw, 5.0bpw, 6.0bpw, 2.65bpw
Frostwind: 10.7b, exl2, 3.0bpw, h6, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8, gptq, awq
DaringMaid: 20b, gguf, 13b, gptq, 8bpw, exl2, 3bpw, 6bpw, 4bpw, 5bpw, awq
vicuna: 7b, 13b, 16k, clip, finetune, lora, delta, 33b, gptq, siglip, so400m, 4bit, 128g, 8bit, chinese, replication, beta, 1.1, fastchat, conversion, free, cuda, 1_1, hf, weight, conv, 3bit, actorder, sft, ggml, ver2, 83k, dataset, new, titles, epoch, cocktail, articles, fork, sharded, all, test, delta_hf, finetuned, langchain, mrkl, bf16, base, small, ram, wizard, uncensored, q3f16_0, android, fin, 1epoch, ger, nwfe, pl, lora_adapter_model, lora_ggml, instruct, lora_gptq, lora_unload, merged, g128, awq, carnarie, no, cache, unit, tiny, stories, pretraining, 2epoch, tinystories, linear, baichuan, chat, chemical, physics, coder, 160m, 7b_gptq, vi, visquad, gemv, w4, gguf, chatgpt3, first_last, claud, claude, chatgpt4, global_limited, global, first_last_embed, gpt4, atk2, gpt3.5, math, lleqa, cait, wanda, qlora, support, bot, faq, alzkb, production, 1.5, gemm, fine, tuning, tuning_25, sharegpt, without, timedial, soc, steve, jobs, tuning_sst_25, tuning_sst_20, tuning_finance_20, tuning_qa_sst_20, tuning_qa_finance_5, tuning_qa_finance_20, tuning_qa_sst_20_2, tuning_qa_finance_20_2, tuning_qa_twitter_20, tuning_twitter_20, tuning_googleplay_20, tuning_qa_googleplay_20, tuning_qa_truthfulqa_20, tuning_qa_truthfulqa_2_20, tuning_sst2_20_16, tuning_sst2_20_64, tuning_sst2_20_512, tuning_sst2_20_128, tuning_sst2_20_32, tuning_sst2_20_256, tuning_finance_20_16, tuning_finance_20_32, tuning_finance_20_64, tuning_finance_20_128, tuning_finance_20_256, tuning_finance_20_512, tuning_twitter_20_16, tuning_twitter_20_32, tuning_twitter_20_64, tuning_twitter_20_256, tuning_twitter_20_128, tuning_google_20_16, tuning_google_20_32, tuning_google_20_64, tuning_google_20_128, tuning_google_20_256, tuning_google_20_512, tuning_sst2_20_16_again, tuning_truthfulqa_16_20, tuning_truthfulqa_32_20, tuning_truthfulqa_64_20, tuning_truthfulqa_128_20, tuning_truthfulqa_256_20, tuning_truthfulqa_512_20, legal_seed20_base, legal_seed30_base, legal_seed10_base, legal_seed40_base, legal_seed50_base, uncesored, class, shishya, ep3, hal, ac, tutor, mmlu, val, only, correct, mcq, ep2, spr, compressor, robust
finance: llm, 13b, chat, gptq, gguf, awq, gpt2, ner, alpaca, 12.8b, 5e, llama160m, finetuned, distilgpt, qa, dpo
Yarn: mistral, 7b, 128k, awq, llama, 13b, 64k, 70b, 32k, gptq, 8k, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8, sharded, ft, med, 2.4bpw, 4.65bpw, 128k_fine, tuning4log, parsing, 2.55bpw, 5.15bpw, summarization
cogvlm: chat, hf, grounding, base, generalist
Noromaid: 13b, gptq, 7b, mixtral, 8x7b, 20b, awq, 8bpw, exl2, 3.75bpw, h6, 6.0bpw, h8, 3bpw, 4bpw, 6bpw, 2.4bpw, 3.0bpw, 3.5bpw, 4.0bpw, 5.0bpw, 2.7bpw, bagel, slerp, instruct
LMCocktail: 10.7b, phi, 8.0bpw, h8, exl2, 3.0bpw, h6, 4.0bpw, 5.0bpw, 6.0bpw, gguf, gptq, awq
MixtralRPChat: zloss, 3.5bpw, h6, exl2, 6.0bpw, 2.4bpw, 3.0bpw, 4.0bpw, 5.0bpw, awq, gptq
LUNA: solarkrautlm, instruct, gguf, gptq, awq
Loyal: macaroni, maid, 7b, toppy, bruins, dare, gptq, awq, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8
WizardCoder: 15b, python, 34b, 13b, 3b, 1b, 7b, 1.0, gptq, 8bit, 15b_safetensors, guanaco, 3bit, 4bit, fine, tuned, 15b_8bit, edited, sharded, 34b_8bit_nf4, 13b_8bit_nf4, lora, 7b_sharded_1.5gb, 13b_sharded_1.5gb, nf4, doublequant, bnb, awq, finetuned, function, calling, sql
btlm: 3b, 8k, base, chat
CodeLlama: 7b, gguf, instruct, hf, 13b, 34b, gptq, python, mlx, fp16, flash, ggml, bf16, sharded, function, calling, adapters, oasst, sft, guanaco, solidity, awq, english, chat, 4.65bpw, hf_thiagodb_, erp, sql, recharts, rust, finetune, full, new, prompts, rp, sparse50, sparse, 8bit, lora, eosphoros, merged, bins, finetuned, hfsft_final_model2sft_final_model3, st, adapter, finetune_text2sql, 4bit, fine, tuned, text, to, alpaca
LLaMA2: 13b, tiefighter, awq, gptq, tiefighterlr, psyfighter2, holomax, 7b, spider, en, ko, model, mytho, glora, sharegpt, pose, linear, 16k, ntk, yarn, 8bpw, h8, exl2, zh, chat, 52k, knut, pippa, chinese, wx
neural: chat, 7b, slerp, 16k, quantized, lock, awq, gptq, 8bpw, h8, exl2, gguf, fp16, dare, 0.85, pt, br, quant, sharded, nebula, 11b, openhermes, 2.5, biologie, 3.0bpw, h6, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, fraud, detection, function, calling, 8x7b, moe, wizardmath, me
NeuralHermes: 2.5, mistral, 7b, awq, exl2, 5bpw, 3.0bpw, h6, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8, gptq, 6bpw, 4.65bpw, 8bpw, 13b, dare_blended, failure, 11b, vgpt35, dpo
una: cybertron, 7b, bf16, fp16, xaberius, 34bbeta, gguf, oma, exl2, 11b, 3.0bpw, h6, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8, 14b, 4.65bpw, awq, gptq, 8bit, 16k, neural, chat, p1, p2, llama
StripedHyena: nous, 7b, hessian, claude, chat
mixtralnt: 4x7b, test, gptq, awq
bagel: dpo, 7b, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8, awq, gptq
openthaigpt: 1.0.0, beta, 13b, chat, gguf, hf, 7b, ckpt, gpt2, pantipwiki, poc, 0.0.1, poc222, instructgpt, 0.0.2, 0.0.3, 0.0.4, mgpt, 0.1.0, alpha, merge, lora, llama, ggml, q4, 2800k, 4bit, 440k, 3470k, 1380k, gptq, test, th, 3.5k, th_en, 8bit, q4_0, dharma, thai
ko: storywriter, nano, gpt, trinity, 1.2b, dialogpt, korean, chit, chat, ballad, essay, ynat, gen, en, llama2, 13b, ref, 7b, finetune, ins, platypus2, collective, platypus, control, trans, mixed, inst, ex2, ex3, n1, ex4, ex5, ex, orcaplatypus, kiwi, llm, test, llama, nsmc, data10000, chat2, chat3, lora, ia3, mistralplatypusdata
MagicPrompt: stable, diffusion, dalle, tinystories, 33m, epoch10, merged
dolly: 3b, 6b, 12b, replication, 7b, q4, 6.9b, sharded, bf16, 8bit, lora, merged, dwarves, poc, gptj, enhanced, auto, gptq, japanese, gpt, 1b, clone, polish, bloomz, 1b1, en, olive, optimized, meadow, patient, info, fine, tune, ggml, ov, openassistant, guanaco, retail, endpoint, 8bitlora, rmclm, subset_wikitext_format_date_only_train, az, chat, 70m
Baichuan2: 13b, chat, 7b, base, 4bits, gptq, 8bits, llamafied, int4, pose, linear, 16k, ntk, yarn, 4bit, 32g
Xwin: lm, 70b, 7b, mlewd, gptq, 13b, math, awq, 8bpw, exl2, 4bpw, 5bpw, 4.65bpw, safetensors, calib, ja, 2k, 100k, fp16, 2.4bpw, h6, 3.0bpw, 4.0bpw, 6.0bpw, 2.2bpw, 2.3bpw, 13btest3_200, 6h, 4.800b, 4bit, quip, 2bit
Taiwan: llm, 13b, chat, 7b, llama, base, ggml, 4bits, gptq, gguf
smol_llama: 101m, gqa, 220m, midjourney, messages, 81m, tied, python, gguf, 32k, linear, theta, sft, limarp
SciPhi: self, rag, mistral, 7b, 32k, awq, gptq, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, 32k_de, 32k_en
tulu: dpo, 70b, 13b, 7b, 30b, 65b, gptq, fp16, grammar, superhot, 8k, 4bit, 32g, pi, lora, lxctx, instruct, pl, lora_adapter_model, lora_gptq, lora_ggml, lora_unload, awq, llama2, exl2, 6bpw, 5.0bpw, h6, 2.4bpw, 3.0bpw, 4.0bpw, 4.65bpw, 6.0bpw, 2.65bpw, nwscot, ep4lr5
NSFW_13B_sft: gguf
openbuddy: deepseek, 67b, mixtral, 8x7b, llama2, 13bp1, 64k, gptq, zephyr, 7b, gguf, 3.5bpw, h6, exl2, bf16, enc, 13b, fp16, 30b, ggml, llama, falcon, openllama, gptq_64g, gptq_8bit_act, 65b, 4bit, autogptq, 40b, atom, 3b, coder, 15b, 70b, 34b, codellama2, 180b, preview0, awq, stablelm, mistral, preview1, base, llemma, igor, link, dialogues, preview2, 3.0bpw, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8, 13b64k, 2.4bpw, 4.65bpw, 2.65bpw, 3.75bpw, 32k
mixtral: 7b, 8expert, 8x7b, awq, instruct, megamerge, dare, gguf, 8xtiny, random, e8, nano, 1gt, test, cpu, offloading, smol, 400m, 2x7b, ja, base
go: bruins, gguf, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8, ruby, bf16, gptq, awq, function, calling, 11b
mamba: 2.8b, instruct, openhermes, gpt, 3b, chat, no_robots, ggml, 7b, 1.4b, oa, 2.8b_dolly_instruction_polish
Synthia: moe, mixtral, 8x7b, 7b, 70bb, 3.5bpw, h6, exl2, 700m, 1.5b, 13b, gptq, ggml, 70b, gguf, gpt, 34b, safetensors, 70bb_4.65bpw, awq, 2.30bpw, 8.0bpw, 3.0bpw, 4.0bpw, 5.0bpw, 6.0bpw, h8, ce, 2.4bpw
Solar: 10.7b, slerp, gptq, awq, exl2, instruct, 1.0, shell
firefly: mixtral, 8x7b, gptq, zephyr, 6x7b, init, 3.5bpw, h6, exl2, awq, 3.75bpw, bloom, 1b4, 2b6, baichuan, 7b, 7b1, ziya, 13b, llama, ggml, llama2, 30b, qwen, base, chat, baichuan2, 2.4bpw, 3.0bpw, 4.0bpw, 5.0bpw, 6.0bpw
OrcaMaidXL: 17b, 32k, 6.0bpw, h6, exl2, gguf, gptq, 3.0bpw, 4.0bpw, 5.0bpw, 8.0bpw, h8, awq
opt: 6.7b, 1.3b, 30b, 350m, 125m, 2.7b, 13b, email, generation, squad, peter, opt, pretrained, sharded, 66b, finetuned, wikitext2, custom, data, economy, stack, opty, lectures, lightnovels, story, cloze, name, entities, pair, 350m_eval, 125m_eval, 1.3b_eval, 6.7b_eval, 30b_eval, 66b_eval, 2.7b_eval, 13b_eval, 8bit, magicprompt, sd, multiprompt, dalio, book, pretrain, smoothquant, realtime, chat, 350m_mle, 350m_mle_, psy, ita, test, iml, max, 4_epoch, 10_epoch, 20_epoch, 30_epoch, 40_epoch, 80_epoch, 80_epoch_, 15_epoch_, 35_epoch_, 50_epoch_, full, 4epoch, 8epoch, 12epoch, 16epoch, laion, text, analogy, permutation, domain, imdb, seinfeld, monika, hellaswag, alpaca, sft, deepspeed, inference, fp16, shard, strict, small, lora, pt, tldr, 7b, openbookcorpus, eli5, invariants, caramelo, dracula_2, groupstories, merged, 350m_test08_tuner, sixbooks, sixhorrorbooks, without, peft, horrorcreepypasta, alpaca125m, adapter, 350m_ver_0, 350m_ver_1, 350m_ver_2, 350m_ver_3, 350m_ver_4, 350m_ver_5, 350m_ver_6, 350m_ver_7, 350m_ver_8, 350m_ver_9, 350m_ver_10, 350m_ver_11, 350m_ver_12, 350m_ver_13, 350m_ver_14, 350m_ver_15, 350m_ver_16, 350m_ver_17, 350m_ver_18, 350m_ver_19, rlhf, poc, 125m_flow_001, sw, mt, br, medmcqa, radiology, instruct, swe, 4bit, 128g, gsm8k, gptq, hy, wiki, am, 350m_ft, 350m_ft_, corrected, state, dict, no, strategy, en, sh, flan, base, awq, 2.7b_30ep_mon_trial, lambada_base, lambada_rmt_ms7_bptt7_sl384_mt64, lambada_rmt_ms7_bptt7_sl384_mt64_final, smart, lambada_rmt_ms7_bptt7_sl1920_mt64_final, lambada_rmt_ms7_bptt7_sl1920_mt64_cur0, lambada_rmt_ms7_bptt7_sl1920_mt64_cur1, lambada_rmt_ms7_bptt7_sl1920_mt64_cur2, tiny, 2layers, random, 3bit, lambada_rmt_ms7_bptt7_sl1920_mt64_cur3, lambada_rmt_ms7_bptt7_sl1920_mt64_cur4, lambada_rmt_ms7_bptt7_sl1920_mt64_cur5, lambada_rmt_ms7_bptt7_sl1920_mt64_cur6, lambada_rmt_ms7_bptt7_sl2028_mt10_cur0, lambada_rmt_ms7_bptt7_sl2028_mt10_cur1, lambada_rmt_ms7_bptt7_sl2028_mt10_cur2, lambada_rmt_ms7_bptt7_sl2028_mt10_cur3, lambada_rmt_ms7_bptt7_sl2028_mt10_cur4, lambada_rmt_ms7_bptt7_sl2028_mt10_cur5, lambada_rmt_ms7_bptt7_sl2028_mt10_cur6, lambada_rmt_ms7_bptt7_sl2028_mt10_final, first, 175b, hyperparam, 3b, prompt, tuned, sentiment, analysis, aihubqa, dpo, smooth, quant, lambada_rmt_ms7_bptt7_sl2028_mt10_ltrue_lora_merged_final, open, understanding, 4bits, onnx, static, shapes, ru, w8a8, unstructured90, unstructured50, 1.5b_imdb_sft, assisted, zc, misti, ft, aspe, all, bnb, 4bit_pt, c4, 2bit, diamond, wd01, slangqa, asvd90, coc
mpt: 7b, storywriter, 30b, 1b, redpajama, 200b, dolly, chat, instruct, instructandstorywriting, 75_25, merge, 4bit, 128g, sharded, peft, compatible, wizardlm, base, extended, ggml, rp200b, mini, shakespeare, 65ktokens, cpu, balance_sheet, extractor, inst, qlora, replit, update, storysummarizer, sail, ep1, lora, fix, fast, 125m, instruct2, for, 8k, drama, 4epochs, cfs, conv, 32k, delta, test, 240ba, q8, multi_gpu, ba339, parallel, colony, memory, importance, ft, transformers, pony, 100ba, bf16, 200ba, postagi, epoch0.962, orca, hf, evaluate, awq, gptq, wiki, sparql, ner, connl, hosted, inference, 8bit, 2gb, style, gemv, 3b, pruned50, quant, rag, c4, gsm8k, ds, pt, comb_cons_merged_tokenizerfix, pruned40, pruned60, pruned70, pruned80, pruned75, tiny, random, gguf, merged, tester
guanaco: 65b, gptq, dumbdumb, 7b, lora, embed, leh, merged, 33b, experiment, ggml, 13b, hf, chinese, superhot, 8k, fp16, pi, 4bit, 32g, 128g, lxctx, d2e, pl, lora_adapter_model, lora_gptq, lora_ggml, awq, chatgpt3, first_last, claude, chatgpt4, global_limited, global, atk2, gpt4, gpt3.5, pt3.5, sharded, bf16, 500mb
llama: ko, 7b, amharic, 3784m, 3b, finnish, instruction, generator, 70b, fb16, korean, 33b, hf, natural, instructions, chat, transformers, 4.29, 160m, 16b, nastychat, langchain, gguf, story, vid, full, video, fps, long, embeddings, hfcompatible, clean, 30b, int4, 4bit, 13b, 65b, configonly, 190m, arch, 3bit, gr128, act, enh, 8bit, se, rl, finetune, 1.4e, 5step_1200, adapter, merged, alpaca, 30p, pretrained, sft, epoch, 5_adamstep_600, 5_adamstep_1000, 5_adamstep_1100, 5_adamstep_800, dropout, do2, onnx, fp16, fp32, vicuna, 128g, triton, sparsetest, c4, 25pct, 128blksz, 75pct, corrected, config, oa, safetensor, 7b_sft, peft, baize, lora, bf16, lite, 134m, b16, huggingface, fixed, tokenizer, oasst, supercot, cuda, ft, sokullu, gist, pos_control, neg_control, stack, rm, self, critiquing, base, critique, refine, chatml, cnn, 210k, sqlcreatecontext, defaultparams, wizard, panda, zh, delta, coig, anlg, gpt3, gpt4, sen, making, rocstories, inference, socialiqa, 7b_synthetic, instruct, gptj, pairwise, 13b_synthetic, pairwise_bs4, 13b_alpaca, 83k, dataset, new, combined, de, 1b, gptq, greatful, wildflower, deus, 28q_4bit, 128g_wvu, 35q_4bit, 53q_4bit, openassitant, ift, ds, superhot, save, test3, test4, test5, 32g, safetensors, pl, prompt, answering, logicot, ppo, flowhf, 1e, 5_adam, 2step_12000, flflowhf, 2step_4000, 2step_6500, transformer, 4.28.1, fork, fin, 1epoch, 2epoch, qlora, chem, 3epoch, test, tagger, conv, kk, 8k, cbt, saiga, mnoukhov, repro, official, indo, alfred, easylm, guanaco, 68m, tok, small, shards, musenet, untrained, food, german, assistant, sharded, dolly, datacom, unmerged, tiny, testing, vigogne, 4000steps, miniguanaco, ggml, llama, plus, resharded, traduction, imdb, legal, laws, maths, 4bitshards, coder, molinst, protein, dhs, asset, index, with, onecol_200rows, medquad, dolly15k, turkish, connectivity, 1d, _16, news, classification, alpacagpt4, 1000step, sam, gqa, better, ads, html2text, orca_mini, july28, opengera, lg, 16k, booksum, nousresearch, listing, description, prectice, tuned, trained, 40k, flash, attention, mini, code, nested, finetuned, wikitext2, loraguanaco, 122k, dnw_newbury_opening, swift, gpt_4_db, epoach, chinwag, entities, cs, ps, 5k, rbh, sql, agent, eliafinetuning, softwarereq, odontil, macnica, harish, clickbait, spoiler, mental, health, 1k, gcda, 29das, 300steps, merge, leadelo, tune_attempt, ftune, badilichat, japanese, und, 2.1, tuned_, resolutions2, rps, autosar, medtype, one, support, fine, unit, medtext, 50k, nh, ncert, physics, tech, 2_hank, leadelo_4500_cosine, leadelo_4500, 15k, prueba, frauas, trial1, leadelo_4500_cosine_2, gaan, dutch, qe2023, multi, shuffled, gaionaus, 2.7, trivia, ca2q, llama2, lora_guanaco, principito, lora_adapter_model, lora_ggml, train, lora_unload, aero, absa, cc, python, 20k, worldpop, mathgpt, football, goaldotcom, colorectal, amoghsinha, samantha, freiblick, dlsang, anastasia21112, manual_gpt, strategy, miniguanacomodel, persian, minicosmic, custom, weaviate, gorilla, tr, msp, claude, roundtrip, private, malaya, james, contract, 32k, 16bits, 2_autotrained, rockwell, testy, webnlg, orca, 10k, minilllama2, summ, ptbr, qa, ft2, filtering, data1, nileshevrywhr, 450m, jalex, 14k, q4_0, pofi, demo, tebot, smallf, tuning, tuning_full, rp, positiveonly, adaptermerged, 7bhf, 2base, plantaofiscal, react, reccb10k, naumanshah007, complex, query, explanation, germandpa_02, reccbs50k, bdcheck_r1, convobot, dateset, neat, spaceship, dbshift, reccbs100k, tagalogd, recipe, 12batch, 20k_, isaevt, example, marco, sr, recipes, batch32, manual_gpt_ver2, ia3, td, academy, processed, mahi, ophycare, open, batch8, vitd, profiles, otg, beta, 8bit_1, doctor, sql2, 8bit_2, percent, steps, 2k, dblp, kgtext, invoice, master, edu, tst, law, slovenian, monikab, beluga, medical, cv, 10e, telco, ar, customer_support, maaused, random, resume, distiller, autogptq, opposite, science, cebuano, scitldr_tuned_model_1000, platypus, courtcase, cti, research, leadelo_system_model_costant, fpf, copypaste, caremi, judgment, correctness, interpretability, amharic_tuned, sales, force, lim, awq, g128, sakshi, tort, verdict, monikac1, amharic_tuned_2, icc23, local, tagging, cpu, amharic_tuned_3, instruct_, rolls, royce, e2, consultation, generation, kurtisbot, openorca_5w, test_, mini_canonical, text, to, samples, 7b_finetuned_chat_cannonical, dolphin_5w, sft_eli5_wiki65k_1024_r_64_alpha_16_merged, sft_ds_wiki65k_1024_r_64_alpha_16_merged, sft_ds_eli5_1024_r_64_alpha_16_merged, kg, ravi, aug24, quote, privacyredaction, fr, en, chatham84, 300_epoch, epoch1, findsum, totaldata, jerrybot, randomweights, bible, amod, counseling, conversations, eli5_id_1k, rick, sanchez, coco, bhavneek, sjbot, tonebot, chat_eli5_id_1k, aug25, wip, 34b, uncode, dtlpy_, java, albertbot, sharegpt, cn, liaaron1, hf_qainformatik, ptp2, nitin, chisbot, dolphin_20w, chitchat, healthcaremagic, perceptive, analytics, pot, marketing, dtlpy_chat, oasst1, es, galleon, openorca_20w, nanum, leadelo_cosine_model, emma, hf_open, limarp, finetune1_17w, pii, transform, 3layers, chuk, leadelo_cosine_model_2, databricks, tagalog, tagaloga, finetune2_3w, product, titles, esci, optimized, test1, rayn, saf, gate_up_down_proj, 7b_wanda_2_4_gptq_4bit_128g, q_k_v_o_proj, format, evolcode, question, cyborg, alt, leadelo_cosine_model_3, leadelo_cosine_model_4, lyric_tune, lotr, kub, strl1, finetune2_test_2.2w, shard, medqa, open_platypus_and_ccp_2.6w, neumaticos, trainned, mimiguanaco, transcript, eli5, wiki_dpo_ds_rm_contrast_1024_r_64_alpha_16_merged, wiki_dpo_ds_rm_top_2_1024_r_64_alpha_16_merged, themerlin, skil, internal, wiki, isha, faq, mj, mrc, 3_epoch, ao3, 1to1, minirmcrs, jmcbot, drug, r16, book, recom, ch24, insurance, tune, fully, quantized, q4, letstalk, r4, protsmi, op, search, freddybot, lakshbot, viggo, mimibot, platypus_2.5w, danbot, phase1, 26b, trenchcoat, monikai, 1.1b, rhetorical, agents, 330m, spider, w4, marcusbot, tabular, miniguanaco20050630, saf2, miniguanacodileep, finetune3_3.3w, non, finetune3, en_hg, flashatn, pacemaker, tp, normal, llm, link, 3.1, saf3, 18k, mini_1c, jazz, custom004444444, prepaid_package_finetuned, leadelo_constant, sonian, search_1m, data, spidersql, philikai, ghc, recipe_nlg_lite, lunaai, general, drlorenzo, v.1.0, minilinkedin, new_sys_msg, nuv, v.1.1, 500epoch, silverliningeda, verilog, codegen, samsum, evolution, instructed, dolffia, assemble, sft_eli5_wiki65k_1024_r_64_alpha_16_simple_merge, sft_ds_wiki65k_1024_r_64_alpha_16_simple_merge, sft_ds_eli5_1024_r_64_alpha_16_simple_merge, sales4, auto, asiga, gangkk, manual_gpt_final, geometrickr, complete, 14sep23, sagemaker, feature, processing, stem, link3, phr_mental_therapy, minifinreport, miniguanacos, link4, insurancebot2, wendyfeifei, mafia, preschool, assistance, russian, finetune4_3.8w, ticketbot, meditation, pj1, mlabonne, redditcasualtest, formatted_data_sales1, redditcasual, forrester, com, insurancebot, cireco, finreport, editing, document, validator, chinese, 81m, covid, mkbhd, mwitiderrick, lamini, repeat, sentiment, analysis, 20k_test, spanish, extract, 25p, traditional, 120m, 4b, ludwig, codealpaca, q_k_v_o, personal, gate_up_down, barrahome, enhanced, r8, all, preliminputjson, sports_plans, xagler, 600m, epochs, regr, removed, icliniq, arxiv, 1.4k, ag, lr2e5, evo_, minh, sent, receipt, key, extraction, meditext, set, nuevamc, epoch3, intent, big, oos, lukas, finetunedtest1, web, articles, text2image, prompts, liege, 23sep23, 2b, deep, haiku, medtext2, mostafaabbas, q_k_v_o_gate_up_down, bongo, finetuned_, math, apk, features, 10p, cosine, grcc1, dom, miniguanaco20080318, 2g, olavo, 17k, do, 20p, obqa, final, blueist2, geeta, hf_hakhanhphuong, classifier, amazon, rev, onlysummary, smoothquant, miniqua, plain, sum, rizz, pickup, morgovigan, mars, wanderchat, instruction_code, cammel, finetune5_4w, chao, pubmed, infogen, sent2048, sent_token_duaribu, nf4, sent_token_duaribu_2giga, lc, kaa_, dq, camvi, doors, sql_generator_2, openstax, medtutant, usk, blueist3, xl, blueist4, arguments, infinity, blueist5, niklas, kullm, 50p, bs16, lr1e4, mininew, bysjobdesc, 7bniklas, blueist6, blueist7, 2_del, labeler, m2w, ws, noapi, ontology, spec, baeksw, finetune4_addto15k_4.5w, gayathri, model, finetune4_compare15k_4.5w, leow, eff, cultax, miniguanaco_nj, noe, sys, camoscio10p, pp, kaps, annotated2, tyellow, finetune4_4.8w, filtered, srf, miniguanaco_icd10, annotated3, roman, empire, 27k, npc, blueist10, resp, tag, shell, extended, plmn1, brn, ei, elyza, cm, keywords, detection, ner, lit, plmn5k, plmn5k_instruct, therapist, cnndailymail, practice, text2sql, lzzmm, guardrail, rag, shishya, ep3, 211k, lee, therapist_v, tiger, 1600qa, longcule, koalpaca, w8a8, unstructured50, clinical, innovation, enem, mail, lenta, reward, ymcui, snips, xp3, epoch5, list, spm2, minifinetune, injected, minikevintest, dpo, ukrainian, treebank, lm, pubmed_qa, link6, chat_shuffle, qlora4, sasb, sassy, aztec, 13k, fortnyce, bom, cover, letter, packing, link8, glenda, address, 90pct, 3epch, clone, glendalorian, minikevintestpython, kheops, mauricioobgo, clinc, furventure, paediatrics, abdominal, pain, miniprospector, blueist11, dbahn, assistant_, 1.5, json, so, chat_untok_6000, r512, felipetuned, luwak, neft, exp1, exp2, exp3, openassist, assistant__3e, evicardio, mdc, evicardio2, 3000tk, 1epch, step, virtual, patient, cases, inst, koen, fusion, prompt_1024, both, db_1ep, 4e, 25oct, el, bilingual, rolechat, chatbot, it, miniprospector3, miniprospector4, 26oct, bom2, miniprospector5, miniprospector6, miniprospector7, miniprospector8, datalynn, maddi, prompt_1024_new_2, wikitext, ems, calltaker, tapal, enem3, bank, learned, oscar, minijson, llava, 1_5, projector, output, 1ep, ikido, meadow, 2_qlora_test, cosin, openassistant, cris, mjalg, android14, keyword, zh2zh, pictarine, 2nov, mixed, finance, manually, 2000step, fttestus, lawful, local2, b32, cot, pru, cutoff, lr2, taipy1, 4layers, 16layers, dailymail, two, tier, apeiron, friedman, asmaachemprot, wechsel, yo, sample10_self_critique, poisson, fake, rexroth, network, traffic, generation_, upload, folder, file, by, p5000, morpheme, jagr, hi200, own, sutton, credit3, tariff3k_instruct, law144, lr1, batch, tf, iab, bt, unchat, finqa, vj, vizuosense, public, 1k_tvb4, youyumidi, manual_gpt_raw_test, miniguanaco1, 1ktoken, segmentation_test, 750_epoch_3, prosolvo, poisoned, manual_gpt_raw_article_test, 750_epoch_3_2, varox34, finetuned1, finetuned1_, pro, solvo, radnlp, geobird, miniplatypus, credit_update1117, banking, atis, float32, nhvng, openorca, edtml, 42m, obama, 110m, miniminiguanaco, finetuned40k, hj, propenster, vqa, test2, prova1, manual_gpt_raw_article_starship_test, polyglot, miniplatypus23, t2t, simple, hallucination_detection, leyes, peruanas, irn, prova3, rotowire, wikibio, finetuned2, sample, cenomirv, vishalsmb, minisds, swedish, oki31124, pt, emotion_cause, tedtalks, traceback, ai, thews, sci, massive, maine, coon, int8, prova5, hammad, prova6, st, prova7, snips_, minig, prova8, sharvesh, subhash, translate_esp_ita, pretrain, icd, eytan, gsm8k, bnb, karawalla, y24, bulgarian, recepies, english, hinglish, lyre, prova10, prova11, gosu, contratosti, prova12, spa, guc, airplane, tickets, prova13, short, form, intention, translate, itmo, miniguanaco_jumel, upscaled, prova15, atis_, qlora_prefixtune, ft002, prova16, sexism, banking_, inglese, folio_by_ccg2lambda, std, profit, massive_, ck, advisor, whazzat, r4_merge_unload, mzyil, sftm, sfte, sftem, fpb, slimorca, platypus2, prova17, prova18, competetivecoding, finqa_, tw, gpt3.5, prova19, prova20, pelvis, req, chest, mri, checkpoint, blutui, pbb, mc2sql_1, prova21, prova22, prova23, prova24, boolean, prova25, prova26, petct, mc2sql_2, marunachef, c1, a100, cf, mc2sql_4, domain, contradictor, prova28, qna, prova27, prova29, topic, gen, rank, finqa_rank_128, sql_new, diarize, coc, keeper, sst2_, tweet_eval_, standards_lora_d0.01_bl1024_ba2_ga2_merged, vehicle, xtiny, know, konnectai, 100ep, virtual_doctor, relation, selection, loteria, standards_lora_d0.01_bl1024_pad_merged, 4bitq, cira, tinypixel, legal_seed10_base, legal_seed20_base, d1024, init, legal_seed30_base, minideven, avg, noa, healthcare, legal_seed40_base, legal_seed50_base, nl, vj_, slimpajama, 1gt, tsakonian, d1536, alexkarpekov, vetdataset, 4_avg_adapter, 6_avg_adapter, 8_avg_adapter, 10_avg_adapter, bgp_version1, az, metamath, 12_avg_adapter, parsed, taipy11, sentiment140, dlm, mixed_2, paper, slides, mimic, madhav, cungnlp, d2048, sus, dpo_2, mini_vesion2, testdata, vergara, tsla, ev, sampledata, wangyy, pskp, harpolius, t1, daniel, bburli, 1.3b, 4bits, class, sparsity, jarvis, slides2, bedtime, abhinav, automate, 3bits, 3.2b, asmaasaferskin, 8e, med, terrallm, airoborohermes, radqa, summary, cons, sli, vos, meta, ultra, trial, druggpt, compare_qiskit_tket, qc, comp, device, backend, plant, ssqg, mail_classif, compare_qiskit_optlevel_score, healthcare_20dec, double, compare_qiskit_optlevel_score_unique, mek, finetuned1_fun, ee, tuned2, epoch_3, html, mcli, pyjama, alpha8, r256, inf, r128, air, minilavita, nous, xbcfinetuned, 10m, openpi_, david_, bibli, gpt, exam, logits, watermark, distill, kgw, gamma0.25, delta2, korquad, delta1, aar, k2, k3, k4, kth, shift1, shift2, shift4, shift256, poultry, repo, dattaraj, spinny, mini_on_addy88, nq, answeronly_dataset, thaimedmodel, craftergpt
law: llm, 13b, awq, gptq, hang, cot, data, all, batch1, epochs6, lr1e, length2048, chat
CodeFuse: codellama, 34b, 4bits, 13b, gptq, awq, starcoder, 15b, qwen, 14b, testgpt, 7b
Inkbot: 13b, 8k, 0.2, 4k, gptq, awq, exl2
CodeBooga: 34b, 4.0bpw, h6, exl2, awq, gptq, 3.0bpw, 5.0bpw, 8.0bpw
BlueLM: 7b, chat, 32k, 4bits, base
sqlcoder: 34b, alpha, gguf, gptq, 7b, awq, exl2, 4bpw, h6
ShareGPT4V: 7b, 13b, gptq, 13b_pretrained_vit, large336, l12_vicuna, 5gb, 7b_pretrained_vit
Chronomaid: storytelling, 13b, 8bpw, h8, exl2, gptq, awq
MiniChat: 1.5, 3b, pruned50, quant, ds, gguf, exl2, q4f16_1, pruned70
ReluLLaMA: 13b, 70b, 7b
Rogue: rose, 103b, gptq, awq, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw
deepsex: 34b, gguf, awq, 4bpw, h6, exl2, h8, gptq, 6b, base, chat
Symbol: llm, 13b, instruct, 7b
blossom: _1, yi, 34b, baichuan2, 13b, 3b, baichuan, 7b, llama2, mistral, gptq, awq
v1olet_marcoroni: go, bruins, merge, 7b, gptq, awq
FLOR: 6.3b, 1.3b, 760m, instructed
PiVoT: solar, 10.7b, rp, mistral, 0.1, evil, gguf, starling, lm, gptq, moe, early, awq, exl2, ashhlimarp, 7b, 3.0bpw, h6, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8, merge, 8bpw, 11b, 4.65bpw
xDAN: l1, chat, rl, image, 7b, instruct4v, 13b, l1mix, deepthinking, l1m, mixtral, experts, e1, m1, qmoe, preview, moe, 4x7b, awq
OrcaMaid: fix, 13b, 32k, gptq, gguf, awq, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8
korean: style, converter, 6b, vicuna, 7b, 1.1, gpt, neox, 125m
LLaMA: moe, 3_0b, 2_16, 3_5b, 4_16, 2_8, 33b, hf, 7b, 32k, 65b, tf, format, 13b, 30b, 4bit, 128g, 32g, main, toolbench, olavo, org, preview, lora, merged, 2bit, 70b, gptq, transformers4.32.0.de, ggml, glora, sharegpt, 3b, groupsize8, groupsize16, groupsize32, 32k_gguf, instruct_gguf, awq, 1.1b, chat, pose, linear, 16k, yarn, ntk, 128k, 96k, testgen, dart_, 1b, dj, refine, 50b, 100b, 150b, instruct, 4.7b, en, 40k, turrera
Orca2myth7: gptq, awq
xglm: 4.5b, 1.7b, 2.9b, 564m, 7.5b, sharded, try
medalpaca: 7b, 13b, lora, 8bit, 30b, 16bit, gptq, ggml, gguf, awq, finetune
instruct: igel, gpt, fp16, codegen, 2b, multi, 350m, 12b, tuned, 2.8b, 3b, 8bit, llama, 7b, wdiff, 16b, hf, alpaca_gpt_4_5_000_samples, alpaca_gpt4, bodo, alpaca_gpt4_5_000_samples_, alpaca_gpt4_5_000_samples, palmyra, 20b, gptq, openllama
decapoda: research, llama, 7b, hf, 30b, 65b, 13b
ruGPT: 3.5, 13b, 8bit, 4bit, fp16, gptq, erotic, kink, chat, lora, 4bits, chitchat, yakovlev, kilusha
airoboros: gpt, 3.5, turbo, 100k, 7b, 13b, 33b, gpt4, m2.0, gptq, l2, mistral2.2, hf, fp16, sharded, triton, 1.1, 4bit, 65b, 1.2, 1.3, 1.4, 2bit, 128g, unstrct50sparse, 32g, actorder, mpt, 30b, 1p4, four, epochs, three, superhot, 8k, six, five, ao, ts, 1.4.1, qlora, pi, lora, ntk, lxctx, ggml, 70b, limarp, 2.0, bf16, cybersharter, testing, c34b, 2.1, llama, creative, 16k, yarn, 64k, 2.2, prequant, merge, safetensors, 1.4.1_5.0bpw, h6, exl2, 1.4.1_4bit, bpw_variants_h6, 1.4.1_2.5bpw, 1.4.1_4.65bpw, 2.4bpw, awq, 2.6bpw, 2.2.1, 4.0bpw, 5bpw, 6h, 3.0, 3b, 3p0, 3.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, 1.4.1_6bpw, 3.1, 1.4.1_7bpw, 2.3bpw, 4.65bpw, 3bpw, 3p11, 3.1.1, 8bpw, h8, 3.1.2, 180b, c70b, 5.25bpw, 5.50bpw, 3.1.2_exl_2.55bpw, 3.1.2_exl_2.3bpw, 3.1.2_exl_2.4bpw, 3.1.2_exl_4.8bpw, 3.1.2_exl_4.6bpw, awq2, y34b, 4bpw, gguf, neuron, dare, 0.85, 3_1, yi, 34b, 200k, 3.1.2_exl_3.8bpw, 3.1.2_exl_3.9bpw, starling, 11b
starcoder: ggml, gpteacher, code, instruct, megatron, gptq, 4bit, 128g, toolbench, solidity, testing, 1b, si, co, format, gradio, sharded, bf16, oasst, 2k, target, xo, cxo, cxso, peft, manual, evol, vyatta, tiny, random, finetune, fit, custom, qa, model, openapi, wikisql, textbook, extra, fine, tuned_starcoder_bigcode_, 16b, 8bit, 7b, finetuned, codecontests, the, stack, bash, glaive, taipy2, taipy5, taipy7, taipy8, taipy9, taipy10, taipy10a, taipy12
h2ogpt: gm, oasst1, en, falcon, 7b, llama2, chat, 13b, oig, 6_9b, 12b, 20b, research, 30b, multilang, open, llama, preview, 300bt, gptq, hf, 65b, 400bt, 700bt, 40b, 4bit, 32g, actorder, superhot, 8k, 3b, xgen, 70b, 16k, codellama, instruct, python, 34b, 32k, aquilachat2
Chronos: hermes, 13b, superhot, 8k, gptq, 70b, awq, fp16, limarp, lora, merged, beluga, 13bfp16, storywriter, mergetest
starcoderbase: 1b, megatron, ggml, 3b, 7b, sharded, bf16, gptq, triviaqa, triviaqa1, gsm8k, sft, awq, swift, steps, set, 8b, glaive, assistant, finetuned
Baichuan: 13b, base, 7b, sft, chat, instruction, 8bit, ggml, gptq
Luna: ai, llama2, uncensored, awq, ggml, fp16, gptq, sharded, standard, 0.0.1, chat
Chinese: llama, 7b, plus, pygmalion, gptq, hf, 13b, 33b, hf_dcard_m, alpaca, merge, alpacapro, merged, wizard, vicuna, falcon, superhot, 8k, fp16, 4bit, llama2, chat, sft, llava, baichuan, cllama2, clam, awq, mlewdboros, lrpsgpt, 2char, test, elevator
idefics: 80b, instruct, 9b
chinese: alpaca, 7b, 13b, 16k, llama, gguf, opt, 125m, 64k, lora, poetry, gpt2, pretrain, xlnet, base, mid, bert, wwm, chinese_bert_wwm1, chinese_bert_wwm2, chinese_bert_wwm3, finetuned, product, pert, large, 100w, chitchat, gptq, 4bit, 128g, merged, lm, 30m, chat, plus, hf, 81m, ift, vicuna, yuniform, 33b, quantized, pro, ggml, 13b_, noon, sft, openchat, chat_, couplet, 100k, low, mem, tang, poem, 1.3b
stablecode: completion, alpha, 3b, 4k, instruct, gptq, openvino, int8, javacode5k
DeciCoder: 1b
mythalion: 13b, 2.30bpw, h4, exl2, supercot, limarp, gradient, gptq, 32g, no, act, 4.25, 4.0, chinese, alpaca2, 8bpw, hb8
Puffin: phi
oo: phi, 1_5, packed, preview1, packing, checkpoint, omega
MLewd: remm, l2, chat, 20b, 13b, old, awq, part3, gptq, inverted, b4.1, h6, exl2, 6bpw, 3bpw, 5.0bpw, 3.3bpw, 2.5bpw
internlm: chat, 20b, 7b, w4, internlm, gguf, 7b_1, 4bit, gptq, int4, xcomposer, vl
context: aware, splitter, 7b, 1b, english, mistral, eng
plamo: 13b, instruct, nc
Pandalyst: 7b, awq, gptq
replit: code_5, 3b, code, dolly, instruct, glaive, openorca, leetcode, coder, ttyd, ecommerce, lora, quanatized
samantha: 1.2, mistral, 7b, falcon, gptq, yi, 34b, 13b, 4bit, 33b, 1.1, llama, alpha, phi, instruct, awq, 8.0bpw, h6, exl2, 3.0bpw, 4.0bpw, 5.0bpw, 6.0bpw, 8bit, h8
verysmol_llama: kix2, minipile_x2, gguf
deita: complexity, scorer, quality, 7b, llama1, 13b, sft, llama2, 6k, fordpo
cerbero: 7b, openchat, gguf, qlora, dante
Synatra: 7b, rp, gguf, dpo, gptq, ashhlimarp, mistral, mcs, slerp, qa, instruct, awq, 4.0bpw, h8, exl2, 4.125bpw, 8bpw, 6bpw, 11b, testbench, 6.125bpw, tb2m_sm, base, pre2, zephyr, translation, orca, nebula, 3.5bpw, hb6, fp16, 42dot, 1.3b, yi, ko, 6b
youri: 7b, chat, instruction, gptq, sft, qa, context, jaqket, awq
calm2: 7b, chat, awq, gptq
Utopia: 13b, 8bpw, h8, exl2, 4bpw, 3bpw, awq, gptq, 3.0bpw, h6, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw
onnx: tinymistral, 248m, gpt2, medium, chat, megatron, 345m, evol_instruct_, conversational, retrain, large, alpaca, 355m, cerebras, gpt, 111m, instruction, llama2_xs_460m_experimental, llama2_xs_460m_experimental_evol_instruct, sheared, pythia, 160m, bloomz, 560m, sft, platypus, zephyr, smol_llama, 100m, dpo, full, mpt, 125m, c4, lamini, neo, evol, instruct, 81m, tied, llama, webglm, qa
notus: 7b, lora, adapter, 8x7b, experiment, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8, gguf, awq, gptq
yi: 34b, w4a16g32, chat, 6b, gguf, awq, hi200, faq, ko, platypus, sft, lora, play, it, re, noprompt, dpo, orca, 32k, gptq, playtus, instruct, further, nl2sql, 3e, 4e, origin
TinyMistral: 248m, instruct, sft, gguf, alpaca, evol, questionmaker
shisa: 7b, gamma, 3.0bpw, h6, exl2, base, awq, gptq, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8, 4.63bpw
CapyTessBorosYi: 34b, 200k, dare, ties, exl2, 4bpw, fiction, 2.68bpw, awq, gguf, gptq, spicyboros, limarp, 3.1bpw
72B: preview, llamafied, qwen, llamafy, canary, unbias, qkv
Tess: coder, 7b, mistral, xl, gptq, awq, xs, creative, gguf, fp16, exl2, 3.0bpw, h6, 4.0bpw, 4.65bpw, 5.0bpw, 6.0bpw, 4.5bpw, 4.85bpw, 2.4bpw, 2.85bpw, 2.18bpw, 8.0bpw, h8, yarn, 128k, 34b
Iambe: storyteller, 20b, dare, awq, gptq, 6bpw, h8, exl2, rp, cdpo, 3bpw, 4.65bpw, alt, dense, 2.6bpw
Venus: 103b, 120b, 4.5bpw, h6, exl2, 4.25bpw, gptq, awq, 5bpw, exl2_2, 2.64bpw
XVERSE: 65b, 13b, chat, 7b_2bit, llama, 7b
medicine: chat, llm, 13b
docsgpt: 7b, mistral, falcon, 40b, 14b, exl2
Pandora: 10.7b, 13b, gptq, awq
ToxicHermes: 2.5, mistral, 7b, exl2
Toppy: mix, 4x7b, 7b, 8bpw, h8, exl2, 3.0bpw, h6, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, awq, gptq, 5bpw
SmolLlamix: 8x101m, gguf
open: llama, 3b, instruct, calm, 7b, open, ko, chinese, patch, 0.3t, dolly, hhrlhf, small, medium, large, 1b, 4gib, 8bit, sharded, bf16, oasst1, flan, sni, cot, sharegpt, baize, self, gpt4, alpaca, code, human, mix, stanford, unnatural, instructions, 13b, 30b, 65b, 1gb, vmware, 3gb, ggml, gptq, pythia, 6.9b, tulu, opt, 6.7b, 4bit, 128g, ct2, float16, int8_float16, finetuned, databricks, chat, 3k, 4k, vi, 5k, cabrita3b, web, math, dev_step11632, hq_step11632, 32g, decontaminated_1b_step11632, vicuna, 52b_1b_step11632, layla, oasst, gguf, summarizer, lora, pii, transform, llm, search, wizard, evol, instuct, 196k, lamini, orca, qlora, checkpoint, merged, q8, guanaco, everythinglm, linear, algebra, everything, safetensors, platypus, medical, latest, 1k, finetuning, steps, batch, size, wei, parameters, llama2, dpo, claude, 30k, australian, legal, distilgpt2, gpt2, phi, 1_5, awq, mathwiz, mathwizard, quantized, elm, fp16
hermeo: 7b, awq
8x7B: moe, test, not, mixtral, gptq
BigPlap: 8x20b, gptq
quantum: gptq, dpo, gguf, awq
minima: 3b, layla, gguf
apricot: wildflower, awq, gptq
palmer: gguf, ultra, dpo, intermediate, chat
functionary: 7b
FlatOrcamaid: 13b, gptq, 8bpw, exl2, 6bpw, 5bpw, 4bpw, 3.0bpw, h6, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8, awq
bun_mistral_7b_v2: gptq, awq
Mixtral_7Bx2_MoE: gptq, awq, exl2
InternVL: chat, vit, 6b, vicuna, 13b, 7b
GPT: neo, 2.7b, shinen, neox, 20b, erebus, glacier, finetuning, airaid, 6b, adventure, janeway, skein, 125m, aid, horni, ln, picard, j6b, guided, scp, finetuned, cord19, crd3, common_gen, covid, bio, medrxiv, 1.3b, beatles, lyrics, newlyrics, medium, step_98500, step_58500, step_18500, ramsay, pny, tour, neo_storygeneration, neo_dnd, neo_dnd_control, tamil, kalki, papers, ro, lite, texttoarxmlparameters, jt, 2_target_real, 8bit, bill, walton, dmv, greentext, 355m, news, articles, 1.3_series_forecasting, grug, code_search_net, skeinway, nosleep, neoxt, chat, base, float16, moderation, pyg_ppo, elif5, echo, 82m, 2_target_real_only_gen, sponge, neo350, evilultimate, devp4, 2_target_fake, target_fake_only_gen, verite, prototype, turing, pdvs1, super, none, peft, imdb, adapter, merged, low, high, sc_mask, abstracts, 6.7b, cleanedalpaca, 4bit, 128g, 3x, wiki, padding, ggml, e6, large, small, 6b_tuned_small_pile, 1.5b, clutserfusion, energy, medical, domain, verite_1.4b_sc, verite_160m, verite_160m_lb, text, style, transfer, using, examples, stable, diffusion, 2.008m, prompts, 6.86m, vietnamese, qlora, finetune, english_quotes_dataset, pretrain, ko, gptq, 32g, ppo, summarize, tldr, chai, submit, 1gb, enwik8, abridged, stripped, xl, fine, tune, epoch, 2_para3m, 2_3m_finetuned2, 2_para3m_2epoch_256, chinese, animethesaurus, 19m, viet, 33m, simplewiki, scratch, 2gb, eng, series, kg, pt, one, cycle, ft, finetuned2, 10th, 500th, 2500th, custom, general, long, final, jci, gguf, 2_london_reccomender, prompt, expansion, fooocus, 7b, wrtok, guide, pretrained, jx, 3b
gpt2: persian, small, indonesian, vietnamese, medium, squad, portuguese, orao, turkish, token, wishes, wikitext2, country, hip, hop, pop, rock, soul, spanish, classics, large, bne, call, of, the, wild, modest, proposal, stackoverflow, question, contents, generator, wikipediabio, finnish, fanfic, dutch, embeddings, italian, mylittlepony, fairytales, friends, game, thrones, lovecraft, theoffice, product, description, fa, comment, poetry, property, classifier, flo, basboettcher, chefkoch, flosolo, starter, waiting, finetuned, peticoes, tcu, acordaos, eli5, base, text, generation, gedi, detoxification, model, de, 124m, uk, fiction, tdk, horror, stories, mcu, script, rnm, with, only, rick, season, spongebob, simpsons, indonesia, ar, aub, aub_m, arabic, ads, fp16, blocksz512, donald_trump, fine, tuning, poem, med, imdb, ptmap, wechsel, chinese, french, german, swahili, indo, kids, 522m, story, newgen, test, ja, alerts, emulator, derecha, bos, eos, 48heads, 8heads, es, twitter, politico, style, paraphraser, bible, romantic, shakespeare, switchboard, tweet, thai, bengali, answering, rap, lyric, writer, adstext, absa, poems, en, biochemistry, nl2bash, tagalog, sst, skript, 1m, ontapdoc, gen, quantized, topic, news, set, xl, dialog, narrative, persuasion, greek, taboo, qa, biographies, finetune, oscar, sentence, kiosk, recipes, cooking, cooking_, reddit, tifu, rocstories, frens, japanese, wikipedia, drcd, qg, hl, nqg, als, demo, exomachina, genre, python2, group2, vaguely, human, dialogue, bio, pt, cased, emotion, horoscopes, chess, uci, scientific, articles, hindi, nft, shakes, seuss, 10m, 3epoch, ancient, cluecorpussmall, couplet, distil, norwegian, ptt, dstc9, nedd, bash, history, baseline, baseline2, fr, ori, tok, baseline3, shrooms, tuned, debiased, ft, non, challenging, 25k, 1k, 0.8, logits, 5k, d1, xl_ft_mult_10k, d2, xl_ft_mult_1k, xl_ft_mult_5k, d3, spam, acled, t2s, xl_ft_logits_10k, xl_ft_logits_1k_2, xl_ft_logits_5k_2, d4, 0.3, 0.15, xl_ft_logits_25k, wikitext, xl_ft_mult_25k, value_it, 0_on_1k, lyrics, disco, zh, 21k, xl_ft_logits_5k_experiment, how, to, xxs, krishna, maptask, ct, untemplated, quests, theseus, bg, ul, ts, lrn6, simctg, cric, commentary, reviewspanish, ukrainian, job, resume, zno, edition, fr_orfeo, cid, paco, cheese_e3, fr_paco, cheese_e1, academic, topics, gf, space, cheese, orfeo, malagasy, uyghur, scottish, gaelic, sundanese, ds, meg, id, puisi, by, keywords, poem_key_words, qrecc, example, kbkw, abstract, canard, commands, chunked, final, sum, czech, cs, rn, poet, p4k, horo, regular, tod_finetuned_sgd, covid, lotr, fellowship, snapsvisor, response, vaccine, wl, discordgpt2, idioms, redditcomments, historias, conflicto, col, colpoetry, wikitext103, catalan, milunanoches, basque, composition, banking77, urdu, smallest, tokenizer, withgpt2, scifi, japanese2, kit, tldr_100, tldr_30, schiappa, ne, dzongkha, mbti, larger, walser, gpt2, tf, weight1, epoch5, 117m, bahasa, tweetml, on, positive, reviews, epoch10, weight2, weight0.5, epoch15, tiny, ml, emailgen, mc, weight0.25, weight0, english, pretrained, answerable, tydiqa, epoch2, dialogbot, poesiahispanica, chat, sample, new, nosharing, fierro, arnavaz, beta, hemingway, custom, xl_10, xl_50, xl_90, xl_95, xl_99, daily, yannic, data, science, transcriptsteve, 355m, yoda, georges, sand, riddles, taf, 0.1.5, arxiv, cl, combine, german_easy, oscar_easy, galician, alpha, reporter, luther, badplace, sft, single, context, cc12m, summarize, sup4_ppo_rm4, imdb_movie_title, elon, epochs, stable, diffusion, prompt, textgen, 650k, pos, srlat, sem, synt, ppo, aggrevate, scratch, economist, furry, idea, lex, negative, movie, lhm, rewrite, peft, abirate, quotes, azerbaijani, toxic, 150m, category, snappfood, 3epochs, sentiment, romanization, commongen, eus, euscrawl, ka, wiki, agatha, christie, runpy, fdax, exec, system, eval, stinfo, ko, seinfeld, rlhf, yelp, polarity, balanced, 20b, anlg, distilled, from, gpt3, full, codeparrot, code, javascript, auto, repair, wikiwriter, ov, supervised, writing, thaiclm, thairath, thai_special, stackexchange, causal_language_model, ag_news, confluence, elie, amaury, faustimer, huiwen, louka, michelle, nathan, tomislav, narges, mariia, hellaswag, mstatmem_1ep_2_gpt2, bbc, datasets, clm, physics, alpaca, chitchat, learn, for, mul, ear_1, hs_cn_decay, mi, reflector, sonnet, generators, personachat, id_prej_hs_cn, author, jokes, synth, real, o1, o2, igc, is, tags, fraud, clm_2, clm_3, inst, m1, mstatmem_1ep_gpt2_no_valid, demolog, clm_tolkien, brookstraining, hp4, arch1ev, git, 4k, no_ear, hs_cn, cn, cn_decay, id_cn, id_prej_cn, 2k, 8k, pile, sinhala, vk, bugro, open, instruct, kalik, gpt4, aneki, liability, rl, sen, making, simulacra, bgwiki, kl_1_05, kl_1_07, amazon, kl_1_06, medium_test06_tuner, kl_1_03, kl_1_04, char, kl_1_03_hscnspecial, kl_1_04_hscnspecial, kl_1_05_hscnspecial, kl_1_06_hscnspecial, kl_1_07_hscnspecial, 4chan, mini, text2sql, movies, wikitext23, txttoarxml, socialiqa, combined, dz, wikiemails, multiexit, bahamut, comments, headlines, trainer, 8b, joyous, ear_01, emails, no, pattern, wikiemails_unlearned, int8, 50k, polish, ear_001, kl_01_03_hscnspecial, kl_01_04_hscnspecial, vi, kl_01_05_hscnspecial, kl_01_06_hscnspecial, kl_01_07_hscnspecial, kl_001_03_hscnspecial, medquad, ptuned, kl_001_04_hscnspecial, kl_001_05_hscnspecial, kl_001_06_hscnspecial, kl_001_07_hscnspecial, vi2, uncased, kl_01_03, kl_01_04, mstatmem_1ep_gpt2_no_valid_verne, kl_01_05, mstatmem_1ep_gpt2_no_valid_austen, kl_01_06, kl_01_07, kl_001_03, kl_001_04, msxl, filtered, title, skspr, generators1, generators2, sw, mt, conversational, loto_jews, loto_lgbt, loto_migrants, loto_muslims, loto_women, faqs, and, ibero, folk, mythology, tales, ear_1_migrants, film, mstatmem_1ep_gpt2_no_valid_verga, desc, test1, fantasy, ethiollm, tr, tweets, technology, uncontrolled, classification, iba, txt, faq, detoxified, reward, music, babi, neurallinguisticpioneers, rlaif, detox, temp, severe, naturalquestions_1000, ep5, ep10, naturalquestions_2000, ep20, naturalquestions_4000, chatbot, neurallingusticpioneers, bias, tensorrt, openwebtext, dro, multi, gpu, serbian, chunk10k, economics, jhegarty, texts, books, 0.6, long, gptq, 4bit, xl_pytorch, 0.2, dp, concat, repetition, penalty, chatgpt, danish, simplification_1e4_adafactor, second, sonnets, review, backstories, college, epoch, tedtalk, taylor, swift, song, old, 1.0, epoch1, 2_left_out_aochildes, 2_left_out_cbt, 2_left_out_gutenberg, og, modified, aochild, 3_left_out_aochildes, mod_aochild, mod, 10chars, cut, simplification_1e4_adafactor_newsela, simplification_1e4_adafactor_biendata, gsm8k, sat, shuffle, length, rarity, 8bit, sampling, 138k, log, gutenberg, fixed, 276k, tax, 210k, rarity2, ctrl, rarity1, 220k, root3, rerun, 135k, medium_nli, aochiles, 14k, aochildes, 16k, 1.5b, 2p2k, medium_triviaqa, cbt, p3k, 16plus6k, all, 1p2k, 3k, p6k, 7k, p8k, len, 16plus3k, punc, dot, 12k, 17p5k, p7k, 4p5k, guten, 2p5k, self, 5p75k, p55k, p95k, top, bnc, 1p5k, 15k, longer, top3, 3p5k, 1p8k, 29k, 30k, 13k, 2p6k, datatsets, iorder, e13k, e2p6k, p1k, e1k, ep1k, end, iroder, est, medium_trained_triviaqa, p5k, repeating, sub, 5p9k, rm, refrences, 1p7k, ref, p13k, cocnat, repreating, 15p5k, formatting, p12k, simple, processing, 10k, datasets1, c13k, c2p6k, rev, datasets3, datasets2, t_trunc_5e, datasets4, 2p3k, neuronx, 5p5k, mostf, dr, drsuess, t_trunc_1e_second, indv, ind, processign, taylorswift, recipe, corrected, 3p3k, 1p6, medical, children, qed, switch, vicuna, 128g, suggestion, xlarge, linear, sharded, bf16, autocomplete, vrabac, anthropic, hh, cnn_dailymail, joker, cs197lec4, jokepapaai, therapy, moviedialog, python, singleline, embrace, contract, beauty, trail, singleline_function, function, quote, 50m, beginning, prompt1, dolly, ael, sailit1, block, singleline_function_block, sailit, tidore, 900m, owt, 100mb, bf32, novel, billsum, large_local, narratives_pre, xl_local, coqa, elite, walamakan, multiqg, freeze, 10kfindsum, oasst, rthk, context_generator, spiritual, aqua, ulasan, products, tigerbot, ecommerce, finetuned1, merchantname, tiger, functions, dataset, magic, card, web, lora, tigerzh, sft1, sft2, stf4, bs128, bs16, warmup, split, wiki2, crpo, medium_gated, medium_gated_freeze, medium_oracle, indian, constitution, cornellmoviedialog, train, mgt, toxic0.3, toxic0.1, toxic0.5, toxic0.7, toxic0.8, toxic0.9, bliss, finetuned60, meditation, special, tokens, retrain, 4b, guj, vector, vector2, vectorizer, neg, vibe, tfmviu, smcp, delivery, information, extraction, trained, wet, strength, tldr, massive, xl_lima, netflix, cc, self_instruct_human_eval, testcase_generator, imdb3, integ2, insight, largegptq, genius, mediumgptq, largegptq8, mediumgptq8, megathon, updated, initial, journey, wikitexts, depression, continual, clip, guided, ai, challenge, withaction, abcd, tweetgen, shak, justin, welsh, seqlen, bs, tl, vizuosense, toxicity, prompts, finetuned2, noaction, checkpoint, emowoz, simpletod, geo, logp1.0, expert, logp3.0, logp10.0, summarization, 5token, solver, cr, pair, experimental, french_10, best, pubmedabs, uspto, usptoandpubmedabs, random, qlora, support, merged, drugscom_depression_reviews, it, hq, lyrics_esp, style_transfer, 10var, historical, hist, chad, 20kdata, 25it, 10it, 5it, cobot, hooked, selcan, copy, 20k, 0it, mawps, pen, 20it, with_reward, 30it, 40it, 50it, 60it, 70it
reformer: enwik8, crime, and, punishment, clm, random, tiny
lit: 6b, 125m
GPT2: finetuned, hinglish, song, generation, dbpedia, scp, containmentprocedures, descriptions, miscellaneous, prompt, news, titles, first, model, python, code, generator, futurama, script, german, chefkoch, traditionalchat, korean, article, large2, cyp, beatles, lyrics, newlyrics, glitchfur, zenith, jd, icc, new, malayalam, poet, lm, mbti, cls, gpt2, mc, weight0.25, epoch5, ppl, spanish_poem_generation, spanish, title, got1, got, got2, got4, addtoken, xl, gotfinetuned, gotfinetuned_, large, rlhf, covid, wikitext2, br, impactscience, instruct, sft, poem, baseline, kb, roc, roc1, para, gpt2_lm_head_model, medium, alpaca, 355m, babylm, challenge, strict, small, 2m, 124m, 12500steps, polish, 12000steps, wiki, text, 3.5b, chinese, ft, luotuo, medicalqa, syntheticdata, 110m, 312m, berttokenizer, from, scratch, stable, diffusion, 1.487m, prompts, deduped, 6.86m, neo, tedtalk, rpgpt, 8.48m, 1b, nsfw, arabic_poetry_generator, arabic, sentence, system, y2k, chatbot, to, sql, aa, csic, bgl, jokes, chess, study, vadodara, yoda, input, response, pair, finetune, qa, debiased, wikitext, onnx, quantized
TEFL: blogging, 9k, 2.7b, 10k, 15k, 4k, 6k
skript: 1m, gpt, neo350m, neo125m
codegen: 16b, nl, 350m, mono, multi, 2b, 6b, gptj, onnx, finetuned, sharded, bnb, optimized, xlcost, onxx, list, manip, pythonic, diff, finetuned_method2test_ctx1, finetuned_method2test_ctx5, finetuned_method2test_ctx11, finetuned_method2test_ctx55, parts, lora, adapter, merged, functional, custom, functions, dataset, python, python_, toolbench, mbpp, rlhf, first, run, codetest, 4bit, 128g, qlora, java, ruby, rust, swift, react, 18k, alpaca, fine, tuned, tigermask, int4, gptq, code, sympy2latex, ungroup
bigscience: small, testing, scan, t5x, bloom, 560m, ov, brief, history, of, time, model, 1b1, custom
bloom: 560m, 3b, 7b1, petals, finetuned, sd, prompts, common_gen, gptq, 4bit, 1b1, 1b7, 350m, scan, 760m, 1b3, 6b3, t5x, 8bit, 2b5_zen, tiny, random, intermediate, 2b5, german, news, summarization, cnn, 389m, zh, 800m, 1b4, 6b4, xsum, 560m_az_bitfit_100000samples_, 1vocab_original, frozen, 560m_az_bitfit_10000samples_, 560m_az_bitfit_1000samples_, 560m_az_sft_1000samples_, 560m_az_sft_10000samples_, 560m_az_sft_100000samples_, 560m_az_fish_100000samples_, 560m_az_fish_10000samples_, 560m_az_fish_1000samples_, totto, table, to, text, 560m_my_bitfit_100000samples_, samsum, aeslc, subject, generation, ft, 560m_si_continual, pretrain, reinit_100000samples_, 560m_az_continual, pretrain_100000samples_, 560m_de_bitfit_100000samples_, 560m_de_fish_100000samples_, 560m_de_continual, 560m_de_sft_100000samples_, 560m_si_fish_100000samples_, 560m_si_bitfit_100000samples_, 560m_si_sft_100000samples_, 1b1_az_bitfit_100000samples_, cdn_law, cdn_law_6epochs, arb, thesis, 560m_my_continual, 560m_my_sft_100000samples_, 1b5, clp, init, 1b7_de_continual, 1b1_de_continual, 1vocab_original_bsz1, 1vocab_original_bsz2, 1vocab_original_bsz4, 1vocab_original_bsz8, 1vocab_original_bsz16, 1vocab_original_bsz32, 1vocab_original_fp16, 1b1_ru_adpt_bitfit_original, frozen_100_000samples, 560m_ru_adpt_continual, reinit_original, pretrain_original, 560m_ru_adpt_sft_original, 560m_ru_adpt_bitfit_original, 7b1_ru_continual, 7b1_de_continual, 7b1_th_continual, emailgen, igpt3, bloom, 1b1_ru_continual, 1b7_ru_continual, 560m_am_continual, pretrain_10000samples, rlhf, sd2, prompter, aesthetic, riddles, hdg, the, stack, cobol, better, brainfuck, prolog, sharded, fp16, rust, unnatural, instructions, 6k, steps, spanish, knight, converted, wikitext2, netflix, fraud, wikitext, zsre, lightnovel, bf16, hellaswag, 396m, 820m, 2b6, positive, reframing, 7b, chunhua, lora, liability, sv, admcom, reading, comprehension, tatsu, lab, alpaca, instruct, peft, chat, db, action, items, bank, test, speed, check, small, forecast, qa, squad, no_lora, finetuned_, ggml, pre, train, l6, l2, merged, oasst, finnish, 176b, 7b1_pytorch, moss, 1b7_with_lm_head, ar, instruction, fine, onnx, legal, open, orca, spider, 3bit, custom, llm, exam, 10kvic4, awq, perchay, tfmviu, 1b7_it, it, evalita, dolly, 1b7_prompt_tuning_causal_lm, tagger, 1b, aings, non, delimiter, bangla, hasib, pretrained, convo, english, slimorca, existing, dental, raw, new, gguf, 166m, fp32, unh, vi, beta, pruned, gmp, sparsity
gptj: 162m, bswiki, 8bit, mnli, lit, 5g, sft, single, context, static, neo, 1.3b_finetuned, enusec, talks, response, full, 6b, rlhf, supervised, summarize, checkpoint, 8bits, pseudocode, soda, chai, edit, 100k, base, 4bit, awq, tiny, random
OPT: 13b, erebus, 2.7b, nerys, 350m, 30b, nerybus, mix, 6b, 125m, lovecraft, covid, finetune, 6.7b, warriors, tpb, kaggle, creepypasta, christmas, list, generator, 19m, chatsalad, safety, policy, prosocial, generate, rots, 175b, numpy, 4bit, 128g, instruct, follow, alpaca, trainer, 32b, 350_mlm, sw, wikitext2, 1.3b, sft, dschatlora, rm, dschat, rlhf, iml, finetuned, gsm, hard, sql, spider, lamini, awq
bloomz: 7b1, 3b, 1b7, petals, p3, mt, 560m, 1b1, 1b6, finetuned, cot, instruction, stanford, alpaca, instruct, 6b4, zh, sharded, bf16, 396m, 820m, 1b4, fraud, 2b6, eli5, pretrained, wikitext2, rlhf, org, prune, llt, nvl, cllv, ggml, 560m_p, 560m_p_low_learning, lora, adapter, merged, 8bit, 4bit, 176b, gptq, 4bits, 128gr, vi, qa, nllb, viquad__full, wiki, document, title, writer, 2048vic4, sft, chat, 560m_prompt_tuning_causal_lm, quant, safetensors, gguf, fp32, knowsql, 2gb
PolyCoder: 0.4b, 160m, 2.7b, finetuned, test
PULI: gpt, 3sx, context, question, answering, gptrio, ggml, gptq
git: base, vatex, coco, msrvtt, qa, large, pokemon, planet, image_cap, phoenix, sgh2, sghb, sghwb, flickr, e6, histo, checkpoint, textvqa, my_model, test3, dummy, temp, temp2, finetune, aug112023_05, temp12, sketch, temp13, temp30, temp31, temp100, temp101, temp102, temp103, temp104, next, temp107, temp108, temp111, temp112, temp115, refines, test, env, caption_finetune, cxiu, naruto, food, vqa, test4, finetuning, apples, dataset, phone, cases, duski_captioner, duski_captioner_customtrainer, lover, loverboi13, cococaptions, mimic, subset, bananas
BioGPT: large, finetuned, chatdoctor, pubmedqa, kids2023, biored1, natural, products, re, diversity, synt, extended
pythia: 1.4b, deduped, 2.8b, 12b, intervention, long, 1b, legal, llm, 160m, 70m, 410m, 6.9b, seed42, 125m, static, sft, 6b, synthetic, summarize, hq, emails, lightnovels, 13b, green_devil, hc3, dedup, rm, response, only, full, hh, seed1, seed2, alldropout, seed3, attndropout, hiddendropout, 3b, tldr, 20b, finetuned, seinfeld, summarization, r1, instruct, rl, tuned, r2, lambda, jeopardy, oasst, ppo, scifi, fantasy, p6144_c1920_y8192, epoch4, evilprompter, swedish, gptq, 4bit, pre, gpt4all, pretrain, jokes, 8k, finetune, alpaca, cleaned, imdb, adapter, merged, fork, 4k, base, chatml, rp, 420m, 280m, 570m, 710m, steps, polish, 83k, dataset, new, titles, 12.5k, chat, 2.5k, 7k, 1000step, deduped_synthetic, gptj, pairwise, deduped_alpaca, rlhf, 2k, grimms, second, sharded, bf16, ggml, repair, char, based, hf, 2000step, 1500step, highlr, token, 1600step, 33k, self, 21k, 70k, 91k, 103k, 10k, 16k, 28k, 36k, 8bit, 45k, 53k, dnd, wikipedia, fp16, owt2, 1epoch, critical, role, multi_woz_, dedupe, yt, orca, expert, test, flax, dpo, chkpt, math, 23k, step44k, 92bt, lora, popqa, parents, lying, step92k, 193bt, 14m, 31m, sharegpt, sentence_ordering, final, activity, text, 10epoch, products_teltec, ft, lamini, wikitext2, 2.8, array, simplewiki, cleansharegpt, simplepile, lite, scratch, 2e, goodwiki, ki_, fka, sts, pictures, 160m_sentiment_reward, 70m_sentiment_reward, ds, zero3, 1.4, reversed, 410m_sentiment_reward, 70m_utility_reward, 160m_utility_reward, 410m_utility_reward, en, 410m7154033, 410m7125056, lmsys, vf3ea6629, ve437fafc, 410m12371, 410m65991, 410m86372, 1b41679, 1b72447, 2.8b31541, 2.8b89551, 2.8b31945, non, toxic, with, context, qa, helpful, 1b19136, function, calling, 1b86094, 2.8b12914, random, 20k, elm, cl, cl2, mini, lm
gpt3: finnish, xl, kor, based_gpt2_review_sr1, based_gpt2_review_sr2, based_gpt2_review_sr3, based_gpt2_review_sr4, based_gpt2_review_sr5, small_based_on_gpt2, small, medium, large, 3b, 8b, 13b, vietnamese, finetune, cnndaily, news, squad, author, clm, ggml, ft, abirateen, sft, 4epoch, 125m, 2000iter, 8000iter, small_based_on_gpt2_core_ml
alpaca: native, 13b, lora, int4, 5.8b, ko, 7b, 4bit, 30b, hf, onnx, fp32, with, past, fp16, base, nativeenhanced, no, enhanced, ggml, doctor, bloom, 560m, en, opt, 6.7b, wdiff, movie, review, sentiment, loraa, merged, dwarves, poc, 65b, gptq, 8bit, answer, summary, farm, sft10k, ppo, human, feedme, sim, expiter, reward, condition, gpt4, 20k, llama, german, 51k, cleaned, bf16, serbian, chkp, 3b, combined, alpaca, plus, chat, clean, instructdial, 47k, taiwan, test, 63k, 68k, med, mistral7b, adapter, english_full, model, safetensors
Pythia: chat, base, 7b, 70m, deduped, adventure, chatsalad, 160m, 12b, instruction, tuned, series, ggml, greentext, 1.4b, chess, language, knowledgeextract, synonym, sentence, converter, 410m, 1200_steps, 4000_steps
gpt4: alpaca, 13b, native, 4bit, 128g, cuda, roleplay, lora, vicuna, gptq, ggml, q4_0, new, llama, cpp, 7b, fp16
KORani: 13b
Pygmalion: metharme, 7b, 4bit, topscore, merged, safetensors, gptq, q4_1, ggml, worsescoring, vicuna, 1.1, 13b, 8bit, landmark, attention, superhot, 8k, fp16, gguf, supercot, exl2, supercot2, weighed, awq
LLaVA: lightning, 7b, delta, 13b, science_qa, mpt, preview, lightening, 4v, 7b_vit, l14, 336px_pretrain, 336px, 13b_vit, b32_pretrain, b32
RedPajama: incite, base, 3b, chat, instruction, tuning, with, gpt, 7b, instruct, gptq, 4bit, 128g, ggml, sharded, onnx, cpu, sdpromptgeninstruct, merged, codegen, sdpromptgen, vicuna, bf16, github, sharegpt, 11k, curated, arithmetics, wikipedia, 8bit, kor100k, epoch2, epoch3, korean100k, epoch4, test, rl, lora, test1, layout, zeroshot, 10k, classe_nenhuma, classe_other, classe_bias, classe_nenhuma_naoquantizado, paraphrase, tone, dialogue, summary, topic, flax, fine, 3b_local, narratives_pre, chromecast, support, miniguanaco, 3gb, torchfloat16, 2gb, fin
galactica: 6.7b, evol, instruct, 70k, 125m, 1.3b, 30b, 120b, conversation, finetuned, mini, base, gptq, orca, wizardlm
GPT4: vicuna, 13b, fp16, alpaca, 30b, 4bit, alpaca13b, roleplaylora, alpasta, alpacadente, alpacadente2
santacoder: finetuned, the, stack, glsl, megatron, shell, bash, code, to, text, xlcost, python, ts, lua, dockerfiles, assembly, swift, cobol, rust, clojure, fs, ruby, test1, julia, fim, no, 393b, tokens, commits, jupyter, nofim, shadertoys, fine, cpp, yaml, unit, test, mbpp, abap, cp, lora, metal, odoo, dbrief, cf, ldf, robot, robot2, robot3, robot4
TinyStories: 1m, 33m, 3m, 8m, 28m, 1layer, 21m, 2layers, instruct, instuct, wikitrain, ethan, 8bit, aws, val, hebrew, validationset, finetuned, exclamation2, onnx, instruction, tuned, hp, trained, r64, alpha64, epochs25, ds, quant, pruned50, alpaca
wizard: mega, 13b, gptq, awq, vicuna, 4bit, 128g, cuda, hf, 8bit, ggml, 7b, uncensored, superhot, 8k, fp16, g128, 30b, w4, sharded, rp, gguf, python, financial_2, financial_5, financial_gptq2, orca, 3b, mistral, ep2, calling, financialnbot
manticore: 13b, gptq, 4bit, chat, pyg, 128g, 30b, alpha, qlora, awq, g128, gguf
chronos: 13b, gptq, 4bit, hermes, triton, wizardlm, uc, scot, st, 33b, superhot, 8k, fp16, 70b, awq
CAMEL: 13b, role, playing, data, combined, gptq, fp16, 4bit, 33b, superhot, 8k, awq
Karen_theEditor_13B: gptq, awq
starchat: beta, alpha, gptq, 8bit, merged, sharded, bf16
Aira: portuguese, 124m, 1b7, dpo, 355m, 774m, 560m, 1b5, 1b1, opt, 125m, 1b3, 350m, gguf
minotaur: 13b, fixed, mpt, 7b, gptq, 15b, ggml, 8bit, awq
gorilla: 7b, hf, delta, openfunctions, gptq, sharded, fp16, tf, th, mpt, falcon, autogptq, llama, qlora, awq, exl2
BigTranslate: 13b, gptq, ggml
educhat: sft, 7b, base, 13b, baichuan, gptq
xgen: 7b, 8k, base, 4k, inst, alpaca, open, instruct, 8bit, gptq, sharded, guanaco, orca, bf16, dolly, 15k, 4bit, 128g, 8k_dolly
palmyra: med, 20b, chat, large, base, small, 3b
orca_mini_v2_7B: ggml, gptq
aguila: 7b, sharded, 8bit, 4bit
OpenOrca: preview1, 13b, platypus2, gguf, ggml, gptq, llama2, qlora, 0.80, epoch, thera, ayt, phi, awq, piratetalk, nebula, 7b, zephyr, exl2
longchat: 7b, qlora, customer, support, 32k, 13b, 16k, sharded, gguf
polylm: 1.7b, multialpaca, 13b, fine, grained, shards, chat
PandaLM: alpaca, 7b
Alpaca: 7b, lora, 30b, int4, safetensors, 128g, elina, 65b, 4bit, gold, backbone
Redmond: puffin, 13b, hermes, coder, gptq, instruct, pl, lora_adapter_model, lora_unload, awq
spider: skeleton, wizard, coder, merged, natsql, 8bit, llama, 160m, llama160m
vigogne: 7b, instruct, 13b, bloom, 7b1, 33b, opt, 6.7b, chat, falcon, mpt, gguf, 70b, stablelm, 3b, 4e1t, gptq, awq
13B: ouroboros, hypermantis, chimera, hypermantis_gptq_4bit, 128g, bluemethod, gptq4bit, cuda, ggml, gptq, legerdemain, l2, temp, theseus, mk1, thorns, awq, gguf
gogpt2: 7b, ggml, pretrain, 13b
llama2: 22b, leetcode, 70b, 8bit, 7b, shard, bf16, merged, finnish, alpaca, buggy, chat, hf, codecherrypop, qlora, sharded, ft, guanaco, 13b, rudrec, gptq, ggml, finetuned, ner, gym, chinese, model, orca, 8k, english_quotes, coder, full, blocktriangular, meta, transformer, fine, tuned_mixed, edtech, 6k, wizard, uncensored, frankenwizard, fewer, kv, heads, sharing, empathy, assistant, vyatta, finetunined, french, vnf, virtualization, sciqtest, chronos, experiment1, weni, rads2, sciq, waifu, germanquadtest, cypher, pico, python, cabrita, lora, chinwag, 15k, rad4, gpt4, sql, trained, stories15m, stories42m, stories110m, rad6, bt, tuned, test, dolly, askeve, prealpha01, medical, empath, alpacagpt4, layla, chatbot, vcn, 35k5, 1ep, prealpha02, 13_daily_dialog, instruction, 4bit, rope, scaled, guesstitlewithocr, openassistant, mc4_nl_cleaned_tiny, w4, greek_alpaca_50_full, daydreamer, jmc, q8_0, oasst, baseline, 2epochs, 5epochs, 10epochs, 20epochs, dummy, extended, bot, math1.1, megacode2_min100, a100, small, norton, megacode2_frac05, dpo, chatbased, 5g, chatbased1, megacode2, math1.2, grammar, corrector, 4bits, 32g, actorder, japanese, chatbased2, autotrain, disertation, orcabest, email, disk, digitalme, torch, float16, gsm8k, megacode3, finetune, for, trends, finetunned, 1060step, pre10, mine, sft, finetune2, studyplanner, merged_test, final, test01, curious, question, answer, scienceqa, lora1, new, learningrate1, marketing, resume, summary, origin, w2, pdf, to, quizz, dto, learningrate2, gen, wait3, ko, docqa, downloaded, int4, dolly15k, testrun2, ftqlora, ennodata, rft, prototype, pii, masking, enlightened, array_n_poa_new, fintune2, bg, drug, array_n_poa_new_bf16, bq, unfiltered, finance, csr, 8epochs, sharegpt4, openassistant1, delivered, salesforce, dialogstudio, tweetsumm, 4e, on, shakespeare, f16p, customer, service, iab_categorization, iab_categorization2, 23epochs, final_2, base, mental, health, roleplay, mix, ain, continue, train, finetune3_test, supercot, loras2, sdprompt, trial, local1, shital, finetune3_test2, openplatypus, 8w, finetune3_test3, local2, easy, peasy, local3, local4, open_assistant, gguf, 1p, rad, impression, psychobot, neuronx, finetune_noratio, prompt2, sumitranandan, pant2, vanessa, awq, m8, rollsroyce, openassistant2, custom, m8_, m9, short, story, m10, m11, m12, 2dataft, gov, finetuning, true, llama, master, elevate, ultimate, 13b.wudao.sft.combine.legal.seq2k.w16.adamw.na100.0921.ds, ontology, epoch, instruct, databricks, call, summarization, ibk, ft2, chatvi, 3e, tuning, category, classification, platypus, llama2, config, primutec, fin, epmc, chang, openhermes, mini, qasper, wizardlm, evol, 35k, kullm, ibk2, recipe, 2gb, openorca, 11k, flipkart, description, german, extraction, collie, c02, financial, advisor, jobtitles, cp, instructed, ds, sec10k, investopedia, weather2json, ru, lenta, sum, vi, amazon, epoch4, 2e, shell, exams, sharegpt, ubuntu, generation, 2epoch, phone, 80k, paws, paraphrase, wwf, news2json_fundraising, backbone, jkt48, books, 0b, unit, ibk3, pt, openllm, leaderboard, final_merged_checkpoint, ibk_f, dialogsum, ibk_final, samchully, kor, quant, vietai, korquad, da, sentiment, analysis, apr, navigation, zc, domain, misti, law, qa, vie, wangyy, jlama, q4_0, q4, perplexity, 3k, biogen, samchully_qa, 7b_orca_entropy_average_top_9988, analize, 32k, emts, finetune_parahprase, 2if, 7b_orca_random_10000, 7b_orca_nll_average_top_10000, llm, 7b_orca_full_50000, merge, br, rsrch, fintune, text2sql, sftd, 512g, medical100k, mt, awq_4bit, sftj, pretrained, prosolvo, preadapted, gptq_4bit, ag, news, psycho, common, hellaswag, mnli, testtune, kazakh, mrpc, nq, robot_action, emoji, conversation, summarizer, hi, mr_action, squad, sst2, winogrande, mr_action1, test60ep, test20ep5bat, 7b_orca_mink_10000, 180m, random, test40ep5bat, text, finetune6, counsellor, tunined, hedi, emowoz, simpletod, multiwoz_interfere, kocot, ashan, gpt, exo, ox, 7b_orca_mink_perc_average_top_10000, konqa, orcapus, table, two, tier, data, all, japanesewiki, 1test, newsqa_colab_300, abstract, title, 8b, ca, ov, wiki, hiring, tuning_sst_20, tuning_finance_20, tuning_twitter_20, tuning_google_20, tuning_qa_sst2_20, tuning_qa_finance_20, tuning_qa_twitter_20, tuning_qa_google_20, classifier, info6105, mymodel, advices, bangla, sentnob, tuning_sst2_20, openassistantfinal, tuning_qa_truthfulqa_20, tuning_truthfulqa_20, unidades, testserveurirmar, megamerge, dare, classifier2, mergedmodel, ver2, atom, titan, go, logs, xp3, testconso, mergedmodel_conso, prenoms, docsum, adapter, emojitoresponse, 13b_mini_170000, fake, detection, ros, mydxf001, latest, mergedmodel_testserveur, 3b, apps, chicks_, 4knormal, 1knormal, jawerty_html_dataset
Legal: llama, ko, 7b, chat
dictalm: 7b, instruct, rab
LLaSM: cllama2, baichuan
bilingual: gpt, neox, 4b, instruction, sft, 8k, ppo
StarCoder: 7b, 1b, 3b, alpaca, cthulhu, mythos
weblab: 10b, instruction, sft, gptq, 32g
Llama2: 7b, sharegpt4, whoisharrypotter, finance, sft, lima, fp16, chat, chinese, 50w, 13b, base, pre_release, orca, gptq, jp, experimental, full2lora, lora, fulltune, codegen, peft, qlora, kimiko, sharded, 2gb, learning, hf, ggml, concordiumid, lease, classific, bigdataset, 22b, daydreamer, withcode3w, med, megacode2, oasst, sharegpt, wiki_noprompt, news_noprompt, llama2scheme, wo_systemprompt_epoch2, openorca, mc, gplatty, stackex_merged, 28b, air03, ft, docs, qr, 70b, gguf, fine, tuned, cot_epoch4, blend, cot, wo, dedup, lora_epoch4, 7b_traditional_chinese_roleplay_chat_lora, german, pmserver_, aivision360, gpt, finetuned, open, platypus, att, foundation_epoch4, test, runpod, ayt, simgpt2test, 330m, 32k, rhetorical, agents, qa, builder, moses, 3rd, best_epoch4, best_epoch8, finetune, best_epoch6, active_after_curriculum_curriculum_epoch6, foundation, dedup_epoch2, dedup_epoch4, dedup_epoch6, merged, active_3rd, floor, dim16_epoch2, dim16_epoch4, awq, dim16_epoch6, dim32_epoch2, dim32_epoch4, dim32_epoch6, dim64_epoch2, dim64_epoch4, dim64_epoch6, dim128_epoch2, dim128_epoch4, 3rd_floor_dedup, aihub, active_epoch2, active_epoch4, active_epoch6, dim128_epoch6, guanaco, 1k, dolphin, sequoiadb, ayb, uns, dup, active, blending, circulus, dpo_epoch1, summarizer, tinytext, dpo, moseslm, 8bit, finetuning, coulombslawunits_sft, circulus_16bit, floor_dpo_epoch1, dim16_epoch6_16bit, mixatis, half, 3epoch, 1epoch, mixsnips, with_eos, cooking, text, gen, cpgqa, cnnoveng, ia3, economist, itr1, final, prompting, 2.5bpw, exl2, 2.55bpw, 5bpw, en, casual, recipes, instruct, dolly, 15k, no_robots, alpaca, itr13b, mergetest, 15ep, 40gb_vn, stackoverflow, steerlm, epic, squad, 100ex, 30ep, ielts, ca, bot, 112ex, 100ep, moodle, pretrained, 7b_finance_lora_3, pruned50, quant, ds, ko, translate
mGPT: 1.3b, persian, peter, mwe, 2e, armenian, 13b, azerbaijan, bashkir, belorussian, bulgarian, chuvash, georgian, kalmyk, kazakh, kirgiz, mari, mongol, ossetian, romanian, tajik, tatar, turkmen, tuvan, ukranian, uzbek, yakut, buryat, quantized, finetuned, coldmirror, hpodcast1
BioMedGPT: lm, 7b, sharded, bf16, hf, gptq
UniNER: 7b, all, type, definition, sup, gptq, 4bit, 128g, actorder_true, onnx
DukunLM: 7b, uncensored, 13b, sharded
komt: llama, 13b, hf, mistral, 7b, gguf, dpo, osy, chat, lora, ggml, llama2, 30b, mistral7b, kor, orca
LosslessMegaCoder: llama2, 13b, mini, 7b, gptq, falcon, 40b, awq
tigerbot: 7b, base, chat, 70b, 13b, 180b, 4bit, exl2, sft, research, 128g, 8bit, int4, sharegpt, lora, gptq, awq, 2.4bpw, h6, 3.0bpw, 4.0bpw, 4.65bpw, 5.0bpw, 6.0bpw, 2.65bpw, test_1, 4k
KO: platypus2, 13b, 7b, ex
Marx: 3b, gguf, gptq, ggml
Samantha: 1.11, 70b, 7b, gptq, 13b, superhot, 8k, fp16, 33b, llama, 1.1, codellama, 34b, awq, nebula, 4.0bpw, exl2, os1, modelnotlora, try2
codellama: 13b, oasst, sft, 7b, hf, zap, next, steps, instruct, pad, 34b, german, assistant, 4bit, autogptq, codellama, w4, g128, awq, python, miniguanaco, chat, finetune, api, ins, ocr, pygmalion, chinese, sql, finetuned, qa, qa_step, code, ft, w2a16g8, cairo, instruction, python7b, custom, soc, mini, 8bit, json, prompt, new, dataset, 1114_epoch_3, chat_history_epoch_3, chat_history_empty_epoch_3, chat_history_empty_epoch_3_2, chat_history_updated_prompt_epoch_3, titan, go, logs, no_chat_history_epoch_3, no_chat_history_epoch_7, no_chat_history__epoch_3, 6bit, neuron, seqlen, batch, size, nf4, fp16, upscaled, taipy13, taipy12, megamerge, dare, catppt
ELYZA: japanese, llama, 7b, fast, codellama, instruct, gptq, 4bit, 64g, awq, calib, ja, 2k, 100k, 8bit, 1k, tukuyomi, finetuned
okapi: fr, llama, vi, bloom, kn, ml, es, te, ro, sk, de, da, it, mr, ne, bn, ar, zh, ru, sr, uk, nl, ta, id, hr, hu, ca, hi
Speechless: llama2, hermes, orca, platypus, wizardlm, 13b, gguf, ggml, gptq, awq
KoreanLM: llama, 7b, finetuned, hf, 1.5b, 3b, gptq, 4bit
ReMM: slerp, l2, 13b, gptq, pippa, light, lion, kimiko, variant, lrpsgpt, 2char, 1char, awq, exl2
pygmalion: 13b, 7b, 6b, monika, gptq, 4bit, 128g, 6b_dev, cuda, 2048token, 1.3b, instruct, safetensors, chaicomp, lmgym, eos, mix, no, lr, 1e, revised, responses, filtered, sampled, default, alpaca, 2epoch, vicuna, chatml, autogptq, rpgpt, pds, fp16, percent, soda_2e_merged, roleplay, 2.7b, 350m, lrp, grad, l2, dolphin, qlora, tuned, supercot, limarp, exl2, gradient, 5bpw
Mythalion: 13b, gptq, kimiko, gguf, 6.05bpw, h8, exl2, awq
Code: llama, 13b, instruct, text2sql, gptq, 7b, gguf, awq, 33b
Giraffe: 70b, 32k, delta, 13b, scaled, beta
Unholy: 13b, 10l, 12l, gptq, awq, exl2
Phi: hermes, 1.3b, platypus, 1_5, open, 1_5b
llama2_7b_chat_uncensored: awq, ggml, gptq, autogptq_wizard_vicuna
llama2_70b_chat_uncensored: awq, gptq, fp16
Uncensored: frank, 7b, awq, 13b, 33b, jordan, gptq
EverythingLM: 13b, 16k, gptq, awq
Zephyrus: l1, 33b
U: amethyst, 20b, gptq, 6bpw, h8, exl2, 3bpw, awq, 4bpw, h6
Emerhyst: 13b, 20b, 5.125bpw, 6h, exl2, awq, gptq, 3bpw, 6bpw, h8, 4bpw, 5bpw, hb6
sheep: duck, llama, 70b, 2_4.65bpw, h6, exl2, awq, gptq, 13b, 6.0bpw
tiny: random, mistralforcausallm, mistral, testing, falcon, alibi, ctrl, gpt2, xlnet, base, cased, gptj, for, causal, lm, bart, fp16, nfl, xlm, roberta, copy, bertlmheadmodel, gpt2lmheadmodel, magicprompt, bloomforcausallm, optforcausallm, gptneoxforcausallm, gptneoforcausallm, gptjforcausallm, codegenforcausallm, mismatched, vocab, embed, lengths, ppo, remote, code, vopt, clip, sharded, correct, finetuned, 1.0.0, vllama, llamaforcausallm, bartforcausallm, bigbirdforcausallm, bigbirdpegasusforcausallm, biogptforcausallm, blenderbotsmallforcausallm, blenderbotforcausallm, ctrllmheadmodel, data2vectextforcausallm, electraforcausallm, ernieforcausallm, gitforcausallm, gptneoxjapaneseforcausallm, mbartforcausallm, megaforcausallm, megatronbertforcausallm, mvpforcausallm, openaigptlmheadmodel, pegasusforcausallm, plbartforcausallm, prophetnetforcausallm, rembertforcausallm, robertaprelayernormforcausallm, robertaforcausallm, rocbertforcausallm, roformerforcausallm, transfoxllmheadmodel, xglmforcausallm, xlmrobertaxlforcausallm, xlnetlmheadmodel, xmodforcausallm, llama, fast, tokenizer, test, gpt, neox, instruct, polish, corpus, 40b, 7b, mpt, starcoder, llamas, 110m, trippy, ernie, puffed, gptq, falconforcausallm, codellamaforcausallm, idefics, m4, 1b, py, safetensors, gguf, miniguanaco, 1.5t, canarim, gqa, 1.1b, chat, medical, model, 2t, open, orca, ru, step, phiforcausallm, mixtral
Chat: univi, pt, llm, stheno, l2, 13b, scienceqa, ayb, nova, platypus2, haruhi_qwen_1_8
rift: coder, 7b, gguf
ToolLLaMA: 7b, ggml
em_german_mistral_v01: gguf, gptq, awq
CollectiveCognition: mistral, 7b, gptq, awq, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, nebula, dare, 0.85, h8
cantonese: llama, 7b, oasst
tora: code, 13b, 70b, 7b, 34b, 1.0, 4bit, 128g, gguf, gptq, awq
agentlm: 70b, awq, 13b, 7b, gptq
LongAlpaca: 70b, 16k, 7b, 13b, awq, gptq, 32k, 4.25bpw, exl2, chinese
med42: 70b, gguf, awq, gptq
AquilaChat2: 7b, 34b, 16k, gptq, awq, 70b, expr
em_german_leo_mistral: gguf, awq, gptq, function, calling
sqlcoder2: gguf, gptq
Sheared: llama, 2.7b, pythia, 160m, platypus, webglm, qa, 1.3b, ds, sft, lora, merged, dpo, sharegpt
Aquila2: 34b, 70b, expr, 2.4bpw, h6, exl2, 3.0bpw, 4.0bpw, 4.65bpw, 5.0bpw, 6.0bpw, 2.65bpw
speechless: codellama, 34b, gguf, mistral, six, in, one, 7b, orth, 1.0, hermes, coig, lite, 13b, orca, platypus, 2k, 0.6e, 4k, 0.5e, llama2, luban, wizardlm, 0.10e, airoboros, dolphin, baichuan2, exl2, code, tora, samantha, awq, gptq, 3.0bpw, h6, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, recalibrate, coding, 16k, tools, dare, 0.85
TransNormerLLM: 1b, 7b, 385m, gptq
Arithmo: mistral, 7b, gptq, awq
japanese: stablelm, instruct, gamma, 7b, beta, 70b, gguf, gpt, 1b, gpt2, medium, small, xsmall, soseki, distilgpt2, neox, opt, 2.7b, unidic, reversed, 3.6b, instruction, sft, ppo, mpt, large, lm, 1.7b, base, alpha, novel, 6b, ggml, f16, marisa, qlora, 4bit, 32g, actorder_false, 128g, 8bit, 1g, actorder_true, sharded, 3b, 4e1t, awq, calib, ja, 1k, gptq, ja_vocab, mistral, 300m, llama, 13b, dpo, uf
ALMA: 7b, ja, pretrain, 13b, gptq, awq, en, openchat, cymraeg, 0.1, 4.0bpw, exl2, ct2, int8_float16
Stheno: 1.10, l2, 13b, inverted, gptq, gpt, 1.1, 1.2, mix, 20b, 1.3, 8bit, exl2, awq, mega, false, 49b, 1.8, 1.11, delta, fp16
CausalLM: 14b, gguf, dpo, alpha, 72b, preview, hqq, 2bit, awq, gptq, 7b, exl2, platypus
Echidna: 13b, 8bpw, h8, exl2, 3bpw, gptq, awq, 4bpw
AshhLimaRP: mistral, 7b, gptq, awq
Hermes: trismegistus, mistral, 7b, 13b, hf, shards, kimiko, f16, gptq, llongma, 8k, ggml, awq, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8, gguf
14B: dpo, alpha
7B: dpo, alpha, legacy, redpajama, conditional, mmmlu
Psyfighter: 13b, gptq, awq
opus: 7b, 70b, exl2, gguf, gptq, awq, safetensors, 2.4bpw, h6, 4.0bpw, 3.0bpw, 2.55bpw, 4.65bpw, 5.15bpw, 6.0bpw
Thespis: 13b, awq, gptq, 5bpw, 6h, exl2, 8bpw, 8h, 3.0bpw, h6, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, mistral, 7b, h8, alpha, gguf
tamil: llama, 7b, instruct, 13b, base
Nanbeige: 16b, chat, 32k, base, gguf, gptq
baichuan2: chat, 7b, 4bit, 13b, gptq, 32g, act, int4, gguf, base
madlad400: 8b, lm
claude2: alpaca, 13b, gptq, 7b, awq
COKAL: dpo, 13b, dpo_test, 70b, ko
Merak: 7b, ggml, mini, orca, indo, gptq, gguf, failed_prototype, prototype1
polanka: 7b, 3b, pretrain, full
nemotron: 8b, base, 4k, chat, rlhf, sft, steerlm, qa
AceGPT: 7b, chat, awq, 13b, gptq
Video: llava, 7b, pretrain
Capybara: tess, yi, 34b, 200k, exl2, 4bpw, fiction, 31bpw, awq, gguf, gptq, tess12, dare, ties, 3.0bpw, h6, 4.0bpw, 4.65bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8
rocket: 3b, gptq, gguf, fp16
HuatuoGPT2: 34b, 7b, 13b, 4bits, 8bits
openinstruct: mistral, 7b, awq, gguf, gptq, nebula, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8
Hi: nolin, 9b
Rose: 20b, gguf, gptq, awq, 3bpw, exl2, 6bpw, experiment2_p1, merged
musiclang: 4k, onnx
dummy: mistral, 1.1b, model, trl, gpt2, correct, vocab, model2, llama, codellama, 7b, hf, 3b, 5b, 191m, 349m, 200m
loyal: piano, m7, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8, cdpo, awq, gptq
llmga: llama, 7b, chat, full, finetune, 13b
NeXGen: large, small, based
sq: mistral, 7b, instruct, w3, s0, vicuna, 13b, w4, llama
Orca2: 13b, selfmerge, 26b, platypus2, qlora, 0.80, epoch, nova, 39b
PlatYi: 34b, llama, lora, 200k, fastchat, gguf, gptq, awq
juanako: 7b, una, awq, gptq, phase, exl2
openchat_3: function, calling, 16k, 8x7b, moe, awq, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, gptq, 8.0bpw, h8, ct2, sharded, 4bit, gguf, fp16, merged, lora, test, st, 11b, 5_yi_6b, 5_ft_1211
PivotMaid: starling, 11b, dare_ties
no: prompt, 1.3b, phone, gpt2, robots, y34b, lora, gguf
SeaLLM: 7b, chat, hybrid, 8.0bpw, exl2, 4.0bpw, 4.5bpw
DPOpenHermes: 7b, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8, awq, gguf, 11b, gptq, experimental
simple: finnish, gpt3, xl, wiki, log, rarity, all, no, cut
radiantloom: support, assist, 7b, email
Chupacabra: 7b, awq, gptq, 14b, 128k, 11b, 8x7b, moe
aurelian: alpha0.1, 70b, rope8, 32k, fp16, 4.65bpw_h6_exl2, 6bpw_h8_exl2, 2.4bpw_h6_exl2
Walter: falcon, 1b, stablelm, 3b, gguf, btlm, llama, solar, 11b, mistral, 7b
Aetheria: l2, 70b, 2.4bpw, h6, exl2, 2.65bpw, awq, gguf, gptq, 3.0bpw, 4.0bpw, 4.65bpw, 5.0bpw, 6.0bpw
Merged: agi, 7b, falcon, awq, gguf, gptq, dpo, exl2
WiNGPT2: 14b, chat, base, 7b
Seraph: 7b, exl2, gptq, awq
LDCC: instruct, llama, ko, 13b, 13b2, 13b4, with, openorca, korca, and, openorca2
Clover3: 17b, 8bpw, exl2, 6bpw, 5bpw, gptq, awq, 4bpw, 3bpw
Terminis: 7b, gguf, awq, gptq
food: recipe, generation, blog, falcon, 7b, intent, gpt2
GreenNodeLM: 7bolet, yi, 34b, sft, 7bleo, awq, gptq
SlimOrca: dpo, mixtral, 8x7b, 13b, exl2, gptq, awq
Ana: m7, awq, gptq, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8
Nyxene: 11b, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8, awq, gptq
LeoScorpius: 7b, chat, dpo, awq, gptq, greennode, platypus, alpaca, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8
WinterGoddess: 1.4x, 70b, l2, awq, gptq, limarp, 32k
LocutusqueXFelladrin: tinymistral248m, instruct, gguf
qwen: chat, gguf, 14b, vl, int4, ft, 7b, ko, instruct, gptq, platypus, qg, mt, zero, full, 2ep, mix, all, 1ep, no, ai, neft, 4bit, openhermes, merged, sharegpt4, sharegpt_zh, hq, hf
LLaMAntino: 7b, hf, ita, 13b, chat, ultrachat, dolly, evalita
ShiningValiantXS: 1.1, gguf, awq, gptq, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8
ShiningValiant: 1.3, gguf, 1.2, gptq, awq, 2.4bpw, h6, exl2, 4.0bpw, 4.65bpw, 5.0bpw, 3.0bpw, 2.6bpw, 6.0bpw, 4bpw
mindy: 7b, awq, gptq
tinymistral: magicoder, instruct, 248m, mediqa, dpo
kullm: polyglot, 12.8b, 5.8b, gptq, 13b, origin, daniel, 8bit
Falkor: 8x7b, moe, 7b, 14b, awq, 11b, 128k, gptq
Metis: 0.3, 0.4, gptq, 0.1, awq, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8
Pirouette: 7b, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8, gguf, awq, gptq
Norocetacean: 20b, 10k, gguf, awq, gptq
trinity: adapter, kormath, dpo, medium
Fennec: mixtral, 8x7b, gptq, awq
OpenZephyrChat: awq, gptq, exl2
glaive: code, assistant, 8x7b, function, calling, small, coder, 7b, q4f16_2, mlc
LinguaMatic: 2.7b, gguf, tiny, 1b, coder, inst
Candle_SOLAR: 10.7b, instruct
phi2: oasst, guanaco, bf16, custom, gguf
Yuan2: 2b, hf, 51b, 102b
stealth: math, sft, adapter, no_embed, mistral, embed
Barcenas: 2.7b, 7b, 13b, 6b, mistral, 1.3b, awq, gptq, 200k, 3b
orangetin: openhermes, mixtral, 8x7b, gptq, awq
slx: 01.1
distilgpt2: rock, finetuned, wikitext2, starter, base, pretrained, he, restaurant, reviews, imdb, wikitexts, sgnews, proofs, amazon, movie, review, generation, at, tiny, conversational, escape, ratatouille, finedtuned, meditations, bookcopus, reddit, tifu, wsb, tweets, aita, text, gen, yttranscripttrial2, irll2, mit, lecture, pgt, fine, tuned, distilgpt2, med_articles, tamil, gpt, tamilmixsentiment, shroomstoy, ttds, hotel, music, search, 500e, nepali, tr, 10e, clean, hkdse, english, paper4, parser, lyrics, final, project, negative, tazar, pokemon, moves, lektay2, firstpos, secondpos, abc, irish, erichmariaremarque, spam, classification, wikitext103, agu, emailgen, discursive, krishna, eap, finetune, acl22, squad, fables, demo, shakespeare, sd, prompts, custom, mail, lm, lotr, ecb, magicprompt, multiprompt, witcherbooks, clm, legal, nlp, 125m, yannic, test, the_verge, linustechtips, two_min, fda, quotes, stable, diffusion, katpoems, epoch, love2, psy, ita, poet, sequence, prayerjournals, imdb_movie_title, large, for, grammar, error, correction, hc3, devrev, panoai2, seinfeld, common, voice, 2k_clean_medical_articles_causal_language_model, clm_us_economic_news_articles, wiki_testing, pragmatic, bbc, eli5, thaiclm, thairath, hitchhiker, mstatmem, douglas, elie, mstatmem_1ep, mstatmem_1ep_2, wikitext_v, art, wiki, qa, mecanicos, brookstraining, rap, cuad, qg, tf, pt, medical, dialogue, phishing, folk, mythology, tales, ft, model, functions, dataset, python, aws2, dnd, coverletters, jhegarty, texts, rivian, lucid, books, unbiased, datos, propios, ukraine, dp, stack, concat, pashto_model, poem_p, poem, answers, e10, e200, finance, drms, e2, radar, moviedialog, medmcqa, question, gptq, 4bit, ft_sustain, yoda, prost, truncated, refined, news, wikidata, unique, lfqa, withaction, abcd, textbook_dataset, tweetsumm, finetune2, finetune3, finetune4, diamond, wd05, wd01, sympy2latex, grouped, wd1, slangqa, emailtype, harrypottah, arabic, twitter, tweet, generator, paul, graham, essays, hindi
horror: scripts
movie: plot, generator, longer, plots, quotes, small, llama, 7b, chat, script
DialoGPT: small, tyrion, harrypotter, large, dailydialog, medium, marx, rick, petyr, peterparkerbot, shawn, ff7, dilucbot, zai, souvik, chandler, sarcastic, morty, hades, tony, rickandmorty, creed, odyssey, tonystark, pfg, sgd, krish, neku, house, doctorstrange, ramscript, peppapig, arya, test, hp, ja, joshua, bopy, 5k, 13k, angeldust, homer, marge, bart, lisa, dwight, jim, kevin, normal, patch1, patch2, med, wie, cassandroid, funhouse, alpha, 1.03, finetuned, multiconan, 1.04, human, 6b, gab, doctor, will2, miles, mbot, conversational, sheldon, batman, ricksanchez, shadow, distilgpt2, bmo, eng, stacc, horror, funny, elon_yuelyu, jobot, ben14, stupidspecialkay, mediumest, aiclub_nitc, michael, laika, ene, ricktest2convo, hpotter, pinkiepie, westy, spanish, chitchat, bitbrand, finetune, rickbot1000, wikitext2, mc, uk, parsed, squad, aisbe, shiki, discord, 5000steps, polish, 4000steps, instruct, steps, codyai, hospitalbot, marty, harry, michaelscott, simpson, mahua, theovon, denji, faqs, block, size128, bs, size, lr, 2e, 1e, 0.5e, 5e, 7e, deepspeed, true, stage2, crd3, adrian, finalfantasydialogue, argen, doctert, bot, stewie, aichat, dob, rickbot, chika
fr: boris, 8bit, sharded
kogpt: trinity, poem, neo, 125m, conditional, neox, tiny, base, small, 24l, 350m, biblepoem, large, 1.3b
gpt1: call, of, the, wild, modest, proposal
neurotitle: rugpt3, small, summary, test
Chatbot: lisasimpson, dialogpt
ADDI: ch, gpt2, de, fi, fr, it
Wenzhong: gpt2, 3.5b, 110m, finetuned, wikitext2, thucnews, thucnews_10000, 4epoch
Yuyuan: gpt2, 3.5b, 110m, scifi, chinese
GRIEFSOFT: walr
Flos_gpt: 2_erw
ruDialoGpt3: medium, finetuned, telegram, 6ep, context
fairseq: dense, 1.3b, 125m, 13b, 2.7b, janeway, 355m, 6.7b, shinen, nerys, moe, 15b, bf16
ft: bz, gen, falcon, 7b, instruct, test, chat_epoch_14, merged, llama, chat, two, tier, data, all, continue, 2_epoch_18, distilgpt2, with, askscience, parser_epoch_24, ms, git, base, pokemon, blip, captions, llama2, classifier, parser_epoch_31, classifier_epoch_24
slovak: gpt, 1.4b, 162m, 405m
text: generation, news, gpt2, small, hungarian, poem, petofi, generator, norge, miniature, gpt, to, sql, cypher
spanish: gpt2, finetuned, rap, lyrics, dialogpt, biblia, tass2020
nb: gpt, 6b, alpaca, lora, 7b, torgersen, sau, 13b, 4k, step100k, 8k
nlgp: docstring, natural
hebrew: bad_wiki, gpt_neo, tiny, small, xl, poetry, soccer, news, holy, language, model
genji: jp, python, 6b, split
codeparrot: ds, small, neo, 125m, py, test, accelerate, sample, 2ep, 29mar, gpt, 10epoch1, 10epoch, 500sample, 1ep, 12apr, 14apr, batchsize32, scale, near, dedup, multi, text, to, code, small2, small3, trained, gp2, finetune, gpt2, ids, custom, functions, dataset, python, myrun, ds2, test1, large, accel, model
stable: jenny, diffusion, prompt, bolster, vicuna, 13b, delta, hf, gptq, 4bit, 128g, cuda, sinop
js: fakes, 4bars, fake, bach, epochs50, epochs20
CPM: generate, gpt2, fp16, distill
rugpt3: ficbook, bts, paraphraser, simplify, large, qa, old, joker, 150k, key2poetry, medium, 8bit
turkish: gpt, poem, generation
alex: ai, gpt, doc2text, finetune, gpt2000, gptq, 4bit
ComVE: distilgpt2, gpt2, large
bert: base, uncased, pretrained, clm, coqa, stories, legal, romanian, cased, multilingual, tuned, smartcat, finetuned, mbti, bert, rotten, tomatoes, super, glue, boolq, mc, weight1, epoch15, weight0, weight0.25, epoch2, epoch5, wikitext2, large, jd_cv, jd_no, jdcv, no, 16nov, lex, auto_train, arxiv, mlm, small, squad, dnd, wiki, choked, genius, lyrics, italian, xxl, paisa, banking77, pt2
german: gpt2, faust, larger, romantik, poetry, easy, large, contrastive, gpt2_easy, mbart, finetuned, coldmirror, hpodcast1, press, release
megatron: 11b, gpt2, 345m, imdb, sft, ppo, gpt, evolinstruct, lima, evol_instruct_
aragpt2: base, large, medium, mega, finetuned, wikitext2, pos, msa, base2, with_arabic_quotes, medium_oknashar, saadeh, full
mongolian: gpt2
gerpt2: large
bart: base, empatheticdialogues, finetuned, mbti, en, persona, chat, large, cnn, bert, school, questions
kcgpt2: dev
rockbot: scratch
xlnet: base, cased, japanese, cola, mrpc, rte, sts, wnli, imdb, rotten, tomatoes, hongkongese, finetuned, wikitext2, mid
html: metadata, exp1, subexp1, subexp2, subexp3
test: dummy, model, new, gpt2, gpt, seq512, ep10, bs64, recipe, charles, dickens, bloomd, 6b3, clm, 1.3b, instruct, custom, llama, project, brand, story, gen, test, tgi, main, 560m, 006afb25d79d1a06fd2be5e9451dc43038acc5bc26b803b9d7ce3b7f698af77e, 37ba0c084a0d6bf37b9b592932523768eb3ad4307f57cb200b6c5f9ca3c7ac56, db788ae2594f597e839fb48fedb0895f04d853006df99f79d446b6b29c715eb7, haikus, finetune, distilgpt2, tokenizer, push, korengcode1p, 20b, merged, repo, stablecode, jay, gptq, trainer, 22b, 3b, switchllama, i3b, f10b, e4, init, awq, transf, one, duplicator, with, 7b, sharded, amazon, rev, santa, fine, tuning, falcon, st, mistral, config, merge, int8, help, steer, filtered, orig, outputs, train, model2, univ, reddit, posts, comments, ppo, tag
harry: potter, gpt2, fanfiction, gptter
yoav_gpt_neo1: 3b, 3b_delimiter
first: bot, large, medium, small, try, dialogue, bloom, finetune, gpt2, tuned, nous, 13b
cript: large, medium
kogpt2: summarization, finetuned, wellness, cat, diary, base, resume, taf, emotion, chatbot, proofreader, movie, chat, long, biblepoem, kogpt, complete_story, simple, qa
papuGaPT2: finetuned, wierszyki, large
dialogpt: for, french, language, ir, bot, daily, dialog, empathetic, dialogues, medium, persona, chat, zyo, fine, tuned, on, no_ear, ear_1, hs_cn, hs_cn_decay, alpaca, finetuned, e20, test
ai: msgbot, gpt2, dialogue, xl, dungeon, medium, rus, large, en, art, random, prompts, hdlcoder, dtmodel, small, for, all
project: code, py, micro, neo, small, kullm
rap: writer, distil, test1, prueba1
alan: walker, watts, 8e
big: baby, tape, russian, boss, schmeat, brain, lm
deep: purple, metal, todo, haiku, gpt, 6b, 8bit, eli, gpt2
dj: artem, artemov, lora, dolly_
egor: kreed, letov
green: day, 70b, fp16, words, sheared, llama, checkpoint, 0.1517, 2.7b
john: samson, lennon
lil: baby, nas, peep, uzi, vert, c3po
machine: gun, kelly, extractor
peter: paul, and, mary
red: hot, chili, peppers, pajama, 3b, sagemaker, 70b, fp16
sam: kim, hr
slava: kpss, marlow
taylor: swift, model, paragraphs, temp
the: eyes, beatles, gazette, grateful, dead, king, and, the, jester, notorious, big, sugarcubes, pigs, velvet, underground, weeknd, brew
yung: lean, plague
926stories: farheyraan, theaamirsays, superachnural
_f1rewalker_: staticmeganito
_luisinhobr: beckvencido, bryan_paula_, luanaguei, nomesdegato, nomesdj
_nisagiss: dril, prezoh, dril_gpt2, drilbot_neo
ai_hexcrawl: dailyartprompts, dril_gpt2, drilbot_neo, gods_txt, gptmicrofic
aimbotaimy: coldjiangshi, ladydarknest, demi_naga, livingscribe
alice333ai: alicecweam, jj_visuals
aliceaeterna: clamtime, redpandasmash
alivegirl001101: drilbot_neo, rusticgendarme
ambivalegenic: dril
avgmeat: dril, methwaffles, slitthroatz
barackobama: billgates, elonmusk, karlousm, uofofn, taylorswift13, joebiden, realdonaldtrump, ylecun
bbcqos: fitslut63, kellyg_official
berniesanders: cnn, dril, coffee__burger, sensanders
bestmusiclyric: bygpt3, marknorm, poetsorg
billgates: jack, kellytclements, xychelsea
bitfinexed: coinerstakingls, xeni
bladeecity: robber0540, rxmaybike, wojespn, thaiboygoon, jerma985, lil_icebunny
borisdayma: elonmusk
c9mang0: deepleffen
canarymission: islamphobiacow
claire_v0ltaire: praisegodbarbon
claireredacted: deepleffen
clamtime: daramgaria, lazar181, ledgeguard, madramami
clementdelangue: julien_c, thom_wolf
cnn: elonmusk, kanyewest, gen, 2k, json
cobie: coinerstakingls, girlgone_crypto
conspiracyb0t: occultb0t
cosm1cgrandma: glitchre, glitchre8, raptv
cryptolith_: drilbot_neo, rusticgendarme, poaststructural
cuckolddna: jennyyoyo92, thaiqos
deepleffen: dodo82j, tsm_leffen, dril, twomad, dril_gpt2, ibnalrafidayn, jschlatt, falco, the_dealersh1p
detnewsopinion: ingrid_jacques, nolanfinleydn
dog_feelings: elonmusk
dril: drilbot_neo, jril_bot, fart, horse_ebooks, feufillet, hostagekiller, gnomeszs, methwaffles, s4m31p4n, heroicvillain95, pukicho, suicidepussy, jdogmart, redfieldcooper, kanyewest, ph4370n, linaarabii, someduckingguy, nia_mp4, praisegodbarbon, theonion, nycguidovoice, senn_spud, tacticalmaid, wnbagirlfriend, pacsjam
drilbot_neo: rusticgendarme
drwrightquotes: iang_fc, s__nakamoto, nickszabo4
elonmusk: hirox246, hitoshinagai1, iamcardib, kanyewest, lateriser12, officialfpl, lexfridman, lynaldencontact, naval, mitll, sagnikdatta129, garyvee, marknorm, timjdillon, jeffbezos, sweatystartup, joebiden, iamsrk, nicolebehnam, punk6529, kimkardashian, fchollet, steak_umm, iamjohnoliver, neiltyson, rshowerthoughts, stephenking, jack, mrbeast, srinithyananda, yeshuaissavior, pornhub, heychadmasters, jess, mcdonalds, subway, evilonmusk, garin, nftfreaks, nftgirl, medvedevrussia, mar15sa, sergiorocks, julicq, realdonaldtrump, watcherguru, luobaishun, remotejoeclark, svembu, sandyboynton, peta, elysiatxt, sdrogoblur, zanomind
exp: twt456, flan, cot, alpha, beta, syn, friendly, cp475, cp950, cp1425, romantic, cp1965, cp1310, cp655, fight, cp960, cp1920, cp2880, joined, cp2090, cp4180, cp6270
formernumber: wmason_iv, wyattmaxon
gitanasnauseda: lukasvalatka, maldeikiene
hostagekiller: suicidepussy
islamphobiacow: praisegodbarbon
jeremyphoward: karpathy, ylecun, lmoroney
joebiden: potus, kingjames
kdtrey5: rxmaybike
ladygaga: lennykravitz, snoopdogg
naval: shl, warikoo, rossimiano, vancityreynolds
onlinepete: recyrb, sematarygravemn, superpiss, utilitylimb
rapevictlm: smallapey, vsshole
simpingboisinc: sircantus
temeton_blue: temeton_pink
biochem: model, first
colloquial: large
meme: titles, llama
cover: letter, distilgpt2, gpt2
slogan: generator, gptneo
sinhala: gpt2, newswire, gpt, neo, cc100, siwiki, lyrics, book, captioning, repo
bengali: lyricist, gpt2
chat: dialogpt, small, en, zh, opt, 1.3b, sft, deepspeed, 350m, reward, rlhf, actor, critic, ema, doc, awareness, 0.8b, self, management, 1.5b, 34b, gpt2, 13b, 7b, 16k, llama2, 80k, mistral, 70b, 4bit, ck, m1
gen: gpt2, medium, chinese, outline, 7b, low, mem
suggest: conclusion, bias, only, full, finetune, soft, intermediary, claims, objections, reasons
arxiv: nlp, physics, instruct, mistral, 7b, math
my: cool, arxiv, model, awesome, gpt, model222, unplugged, gpt2, first, kogpt2, fine, tuned, bert, sent, sum, test, 13b, opt, 6.7b, finetuned, amazing, finance, distilgpt2, eli5, clm, llama, great, recipe, got, llama2, nathan, kittychrysa, think, it, makes, novels, summary, ppo, apoarj, jack, gertrude, mbti, qgnda, rudialogpt, medium, str, lora, precious, pii, mode, llm, fed, biogpt, model2, model900, model700, model600, model1200, secure, chinese, sentiment, 7b, hf, qlora, guanaco, merged, codellama, falcon7b
sl: gpt2, u7
CodeGPT: small, java, adaptedgpt2, py, finetuned, python, token, completion, multilingual, py150, py150_q_all_layers_sym_per_tensor, xtc, 1w8a12l
python: fromzero, gpt2, base, reformerlm, large, medium, docstring, generation, bytes, distilgpt2, code, generator, assistant, 3b, generate
msft: regular, model, smaller
magic: the, gathering, promte, chikku
ar: seq2seq, gender, decoder, text, classification, llama, 7b
es: seq2seq, gender, decoder
sanaa: dialect
GuaPeTe: tiny, finetuned, ted, eubookshop, spa, constitution
joke: generator
EasternFantasyNoval: small
code: clippy, 125m, py, autocomplete, distilgpt2, python, gpt2, base, parrot, llama, slay, gen, accelerate, llama160m
neo: code, py, 125m, dalio, rm, wills, loss, function, by, tr, model, test, story, npr, full, vn, 2.7b, gender
finetuned: distilgpt2, gpt2, reddit, bert, piqa, tiny, merger, agreement, f7b, final, merger_agreement, multi, qa, generation, llama, 13b, hf, 7b, chat, open, hermes, summarizer, mistral7b, llama2, squad, model, mistral, 6epoch
inspirational: quotes, distilgpt2, quotes2, quote, generator
Dialogpt: small, rick, rick01, medium
cunlp: gpt2, dialog
hf: reformer, crime, and, punish, tutorial, small, shards
norwegian: gpt2, social, vgd, gptneo, blue, red, highlr
poetry: anger_gpt2, anticipation_gpt2, disgust_gpt2, fear_gpt2, joy_gpt2, sadness_gpt2, surprise_gpt2, trust_gpt2, bg, gpt2, large, no, hoel, with, no_schiller, complete, complete_2, no_schiller_2, hoel_2, hoel_3, no_schiller_3, complete_3
mdsister: news
Ballpark: trivia, xl
RoGPT2: base, large, medium
Gpt2: paraphrase_generation, csr, qa
KoGPT: joong, trinity, tales
rick: small, superu, rick, and, morty, llama, llama2, chat
TB: 125m, 2.7b
TE: 10k, 12k, 3k, 8k
program: synthesis, gpt, neo, 1.3b, 125m
VN: news, gpt2, literature, generation
generate: thai, lyrics, fakes
forward: dictionary, model
math: roberta, diff, bert, lora, gen, gpt2
dl: hack, distilgpt2, gpt2, large
lm: butlers, gpt, colab, tutorial
indo: gpt2, small, alpaca, lora, 7b, instruct, llama2, 13b, 32k, 70b
javanese: gpt2, small, imdb
bertin: gpt, 6b, es, 8bit, infolibros, bf, sharded, finetuned, chistes_spanish_jokes, half, alpaca, lora, 7b, ner, 4bit, 128g, adapter, with, openassistant, oasst1, merged, informativo
dialoGPT: small, conditioned2nextturn, medium, imdb, pos, finetuned, witheos, finetune
poem: gen, gpt2, small, spanish
hing: gpt, devanagari
distilGPT: ft, eli5, nepali
malayalam: gpt2, blurr, xlm, roberta, base, news
eigenrobot: moridinamael
btohtoh: willitbetoomuch
janieclone: wretched_worm
try: perplexity594, model
baguioni: elonmusk, jacobe
CzeGPT: 2_summarizer, 2_headline_generator
clortown: elonmusk, stephencurry30
GPT2Neo1: 3bpoints, 3bpoints2, 3bpoints3
GPTNeo1: 3bpointslincolnformalinformal, 3binformaltoformal
derbynames: aitextgen, gpt2
incoder: 6b, 1b, finetuned, jest
chrismedlandf1: elonmusk, scarbstech, formula24hrs, tgruener
sharded: gpt, 6b, test, nous, hermes, mgpt
paraphrase: multilingual, minilm, l12, finetuned, dit, 10_epochs, mpnet, base, tuned, smartcat
distilroberta: base, smithsmodel, finetuned, wikitext2, olid, mlm, cot
shaq: shaqtin
it_its_are_are: miyarepostbot, unbridled_id
angelicism010: propertyexile, wretched_worm
newscollected: nickmullensgf
nbme: xlnet, large, cased, electra, generator, gpt2
BulgakovLM: tur, 3b
usmnt: zacksteffen_
kanyewest: usmnt, zlisto
murahokusai: tszzl
alice_lbl: lotrbookquotes, theprincess_lbl
DistilGPT2: beatles, lyrics, finetuned, newlyrics
ChemGPT: 4.7m, 19m, 1.2b
_is_is_are: newscollected, big___oven
sk: kogpt, kormath, causal, squad, falcon, 7b
dpt: 125mb, moses, ver2
lulaoficial: ptbrasil
CodeGen: 350m, multi, 2b, 5k, mono, 0.31, re, ggml, quant, 6b
scibert_scivocab_cased: new, finetuned, leukaemia, breastcancer
rn_gpt2_customdata_model: json
SikuGPT2: poem, translation
comet: gpt2, ai2, small, japanese, distill, high
not_class_trinity: kormath
binance: dydx, magiceden
news: gpt, neo, 1.3b, keywords, line, by, reverse, classification, llama, 7b, memos, falcon_arabic, 18cat, eval, chat, article, generator
disgustingact84: kickswish, managertactical
aoc: itsjefftiedrich, shaun_vids, kamalaharris
kant: gpt2, large
Wenzhong2: gpt2, 3.5b, chinese, 110m, berttokenizer, thucnews, all_in, thucnews_10000, 10epoch, 10_15epoch
conanobrien: mikemancini, wendymolyneux
music: generation, rwkv, rwk, converted
contrapoints: iamcardib
results: gpt, base, erotic, lit, sharded, bf16, 5gb
borisjohnson: elonmusk, majornelson
dav_erage: dozendav
temp: model, llama, hf, classification
GePpeTto: finetuned, ricettetrentine, gastro, bolo, bolo2.0
books: short, model, long
charles: dickens, gpt2
turkishReviews: ds, mini, finetuned, model, mini2, mini1, textgeneration
tacticalmaid: the_ironsheik
ru: gpt, dy, openllama, 7b_, 3b, llama2, 7b, llama, instruct, 13b
BLOOM: 350m, beatles, lyrics, finetuned, newlyrics, 560m, 3b, lora_plantshelper, zalo
image: gpt2, caption, desc, only, large, description, sl, web, feb, captioning, git, base, captionning, pokemon
ruDialoGPT: small, small_10, dedeater
large: korzh, commands, model, finetuned, code, alpaca, tweets, language, margrethe
distill: bloom, 1b3, 10x, test
Covid: conv, news, headline, generator
luciengreaves: seanhannity, pontifex
hillaryclinton: maddow, speakerpelosi
vgdunkey: vgdunkeybot, videobotdunkey
Dodo82J: vgdunkey
tojibaceo: tojibawhiteroom
rp: recap, model
calm: headspace, 7b, tune, ep4, ep5, ep1, ep3
monogptari: 1.3b, 6.7b
jimmie_graham: twittels
great: books, bot
apesahoy: chai_ste, punishedvirgo, jrc1921, spicymoregano, dril_gpt2, nigella_lawson, dril, dril9999, gptmicrofic, tanakhbot, fakeshowbiznews, gptupaguy, nsp_gpt2, powerdril_gpt2, deepfanfiction, pldroneoperated, botphilosophyq, marxhaunting, shrekscriptlol, theofficialkeir, xannon199, hannibalscript, peepscript, toywhole, discoelysiumbot, jzux, deepleffen, ripeacsky, daftlimmy, women4wes, starmerout, stefgotbooted, groanbot, mirrorceleb, bierincognito, elonmusk, fesshole, jonmao___, meat__hook, troovus, unfetteredmind1, theseandiamond, ken_stonger
20pointsbot: apesahoy, nsp_gpt2, chai_ste, deepfanfiction, pldroneoperated
buffer: fastcompany, thinkwithgoogle, instruct, baichuan, baichuan2, 13b, rag, 4bits, 8bits, awq, int4
LuxGPT2: basedger
dalio: checkpoint, 6.7b, test, 1.3b, handwritten, io, synthetic, all, epoch, 125m, principles, finetune, large, convo, pretrain, book, bs4, restruct, bot, cleaned, finetuned, cp350, 30b, cp1500
zero: shot, classification, not, evaluated, vanilla, gpt2, implicit, explicit, tagging
nickelodeon: nickjr, sesamestreet
nickjr: paramountplus, sesamestreet, nickschedules
roberta: base, finetuned, mbti, roberta, weight0, tf, weight2, epoch5, weight0.5, weight1, epoch15, epoch10, hate, speech, dynabench, r4, target, wikitext2
emerg: intent, gpt2, consistent, good, xl
bert2bert: with, cross, attn, decoder, no
anandmahindra: opensea, rs5_eth, elonmusk, sahilbloom
mariojpenton: mjorgec1994, sanmemero
pop: punk, distil
turkishPoe: generation
polyglot: ko, 3.8b, 1.3b, 5.8b, 12.8b, essayist, with, sum, instruct, native, finetune, epoch4, epoch5, epoch6, epoch8, chang, chat, chatdoctor, _epoch2, _epoch3, _epoch4, klue, cot, e1, safetensors, 8bit, qabot, flax, medical, logan, nagase, kotono, 0.3v, 0.4v, qlora, merge, merge_, orca, inst, all, mixed, kullm, fix, 12b, empathy, kopenorca
burgerking: elonmusk
fine: tuned, codegen, 2b, verilog, 6b, 16b, tune, test, dialogpt, pal, llama2, chinese, blog, writer, llama, model, uncensored, progen2, small, medium, large, 7b, kor, tunedllama, waec, tuned_ophomology_llama70b
markiplier: mrbeast, xqc
bt: opt, 350m, 125m, 1.3b, 6b
demo: 0.1, model_sandeep, finetuned, model, dnd
stoic: generator, distil, gpt2, 10e
free: small, 1epoch, 3epoch
polish: gpt2, small, medium, large, xl, instruct
rugpt3medium: tathagata
garyvee: nftfreaks, nftmillionaire, weseleybeats, wise_chimp
apandahvevo: apandeez
polymorphism: fact, checking, crowdsourced
inheritance: fact, checking, crowdsourced
abstractclasses: fact, checking, crowdsourced
overriding: fact, checking, crowdsourced
specialisation: fact, checking, crowdsourced
levanter: gpt, gpt2, 7b, backpack, 1b, 100k
sd: prompt, generator, gpt, neo, gn, finetune, instruct, 3b, epoch, 0.4
medvedevrussia: morgen__shtern
Dialog: wizard, prueba, system
StableDiffusion: prompt, generator, gpt, neo, 125m, k1
Ziyue: gpt2, deep
my_awesome_eli5_clm: model, model2, model_gpt, model1, model3, model_ep10, model_roberta, model_2, model_new_new, model_8_28_1, text
schizo_freq: tszzl, sunrauniverse, two_debtors
bettercallbloom: 560m, 3b
RoSummary: base, medium, large
mail: generator, mini
referee: control_iter, distill_iter, distill, with, context, filter_iter
Randeng: della, 226m, chinese, cvae, ner
borges: gpt, collab, finetuned
big___oven: raspberryl0ver, codeinecucumber, schizo_freq, naamitee, heart2starr, mobydickatsea, y2kenlee
Romance: cleaned, baseline
gretathotburg: snobrights
daymoded: menthalovely, scolopendridaes, drsunrosa
nft_god: notthreadguy, theehustlehouse
fake: gpt, 17m, bach
codeinecucumber: fienddddddd, p8stie, raspberryl0ver
prompt: extend, generator, gpt, neo, 1epoch, chinese, stable, diffusion, simplical, cycles, optimizer, extender, expansion
nickichlol: saware7
sama: willmanidis
jrtec: gpt2, text, generation, quotes, jonathan, vargas, base, superheroes, name, generator
amazon: reviews, input, output, 13b, 350m, 6.7b, 1.3b, best, roberta
h3xenbrenner2: s4m31p4n, tallbart, wnbagirlfriend
multivariable_baseline: stage1, stage2
openai: gpt
barkmeta: lb22_sus, nft_god, nftherder
BERT: finetune, mbti, lm, cls, bert, jointbert, warmup, from
a_gpt2: xl_10, xl_50, xl_90
b_gpt2: xl_10, xl_50
1: 3b, dalio, principles, book, handwritten, after, synthetic, all, epoch, python, model, 24.3.1, 4b, gpt, verite, 3b_full, 3b_full_2, 3b_full_simple, 3b_full_simple_add_inst, 5b_raw_rwkv, 3b_opt, 3b_opt_
recipe: nlg, gpt2, train11_14, train11_15, ingredient, fixer, to, recipe, model, generation, generators
ianflynnbkc: maniacxvii, spiritsonic
olm: bloom, oct, old, 560m, gpt2, baseline, 140k, 420k, one, epoch, exactly, only, exact, dedup, no, bigscience, filter, with, bookcorpus, perplexity, filters, suffix, array, dec, latest
gpt2small: indonesian, recipe, 522m, finetuned, finetuned_
description: lm, dataset_, generation, together, ai, 8bit, 4bit, awq
hygpt2: cml, gen, clm
paulcamuso: williamshatner
chamath: davidsacks, friedberg
bretweinstein: ericrweinstein
6: 7b, dalio, principles, book, epoch, gas, 6e, lr, handwritten, io, cosine, constant, 3e, 1e, ri, reproduce, gpu, combined, val, nlpython, model, nlandvariedcodeandpython
diff: codegen, 350m, 2b, 6b
horse_luvr_47: pacsjam
mbart: reduced, th, en, large, ru, persona, chat, causal, lm
a_0_o_1: gentlest_alive, alexglyricsbot
final_model_output_subreddit: wallstreetbets, wallstreetbets_1, wallstreetbets_2, wallstreetbets_3
autogynefiles: s4m31p4n, tyler02020202
125m: dalio, book, handwritten, io, constant, 1e, ri, reproduce, combined, gpu, val, sft, self, critiquing, base, critique, refine, encode, full, test
couplet: gpt2, llama2finetune
model_output_original_subreddit: wallstreetbets_1, cmu_1, asksciencefiction_1, piano_1, poker_1
nikitabier: realjonahblake, shl
token: absolute, lm, freeze, stage1, music
distillgpt2: bittensortuned4
lyrics: generator
cl207: elonmusk
wikitext_train50K_gpt2: large_mix1.0, large_mix0.1
webtext_train50K_gpt2: large_mix1.0, large_mix0.3
writingPrompts_train50K_gpt2: large_mix1.0, large_mix0.7
transcriptome: gpt
improved_4bars: mdl
gptneo: sft, single, context, clerio, txt2arxml, ppo, txt2arxml_3000, txt2arxml_6000, txt, to, json, txttojson, txttojson0, txttojson1, txttojson2, txttojson4, txttojson5, txttojson6, txttojson8, local, trainer, 30epo, 1.3b, finetuned, health, small, pretrained, story, model, paragraghs_titles
openchatgpt: neo, 125m, neox
yuyan: 700m, 11b, dialogue
tldr: gpt2, xl, sft, dpo
oscar: greek, gpt2, ep10, pt, large
bpt: sft, ppo, rm, 6b
gothlyticalart: kaliyuga_ai
andrewtate: billgates, elonmusk
source: exploration__quote, attribution__gpt, neo, 1.3, 2.7
ajoublue: gpt2, base, medium, 24l, dialog, summarization
ant_philosophy: philosophy_dq, wise_chimp
finetuned_gpt2: medium_sst2_negation0.05, large_sst2_negation0.05, medium_sst2_negation0.2, large_sst2_negation0.2, medium_sst2_negation0.5, large_sst2_negation0.5, large_sst2_negation0.8, medium_sst2_negation0.8, large_sst2_negation0.01, medium_sst2_negation0.01, medium_sst2_negation0.05_pretrainedfalse, large_sst2_negation0.05_pretrainedfalse, xl_sst2_negation0.05_pretrainedfalse, medium_sst2_negation0.0_pretrainedfalse, large_sst2_negation0.0_pretrainedfalse, large_sst2_negation0.2_pretrainedfalse, medium_sst2_negation0.2_pretrainedfalse, medium_sst2_negation0.5_pretrainedfalse, large_sst2_negation0.5_pretrainedfalse, medium_sst2_negation0.001_pretrainedtrue, large_sst2_negation0.001_pretrainedtrue, medium_sst2_negation0.0001_pretrainedtrue, large_sst2_negation0.0001_pretrainedtrue, medium_sst2_negation0.0005_pretrainedtrue, large_sst2_negation0.0005_pretrainedtrue, medium_sst2_negation0.8_pretrainedfalse, large_sst2_negation0.8_pretrainedfalse, medium_sst2_negation0.0_pretrainedfalse_epochs30, medium_sst2_negation0.001_pretrainedtrue_epochs3, medium_sst2_negation0.0001_pretrainedtrue_epochs3, medium_sst2_negation0.0001_pretrainedtrue_epochs1, large_sst2_negation0.0001_pretrainedtrue_epochs1, xl_sst2_negation0.0001_pretrainedtrue_epochs1, medium_sst2_negation0.001_pretrainedtrue_epochs1, medium_sst2_negation0.01_pretrainedtrue_epochs1, large_sst2_negation0.01_pretrainedtrue_epochs1, large_sst2_negation0.1_pretrainedtrue_epochs1, medium_sst2_negation0.1_pretrainedtrue_epochs1, large_sst2_negation0.001_pretrainedtrue_epochs1, xl_sst2_negation0.001_pretrainedtrue_epochs1, xl_sst2_negation0.01_pretrainedtrue_epochs1, xl_sst2_negation0.1_pretrainedtrue_epochs1, medium_sst2_negation0.01_pretrainedfalse_epochs10, medium_sst2_negation0.1_pretrainedfalse_epochs10, large_sst2_negation0.01_pretrainedfalse_epochs10, xl_sst2_negation0.1_pretrainedfalse_epochs10, large_sst2_negation0.1_pretrainedfalse_epochs10, xl_sst2_negation0.01_pretrainedfalse_epochs10, xl_sst2_negation0.001_pretrainedtrue_epochs3, large_sst2_negation0.001_pretrainedtrue_epochs3, medium_sst2_negation0.001_pretrainedtrue_epochs2, large_sst2_negation0.001_pretrainedtrue_epochs2, xl_sst2_negation0.001_pretrainedtrue_epochs2, medium_sst2_negation0.01_pretrainedfalse_epochs3, medium_sst2_negation0.01_pretrainedfalse_epochs6, large_sst2_negation0.01_pretrainedfalse_epochs3, xl_sst2_negation0.01_pretrainedfalse_epochs3, xl_sst2_negation0.01_pretrainedfalse_epochs6, large_sst2_negation0.01_pretrainedfalse_epochs6, large_sst2_negation0.01_pretrainedfalse_epochs1, xl_sst2_negation0.01_pretrainedfalse_epochs1, medium_sst2_negation0.01_pretrainedfalse_epochs1, xl_sst2_negation0.001_pretrainedfalse_epochs3, medium_sst2_negation0.001_pretrainedfalse_epochs1, medium_sst2_negation0.0001_pretrainedfalse_epochs3, medium_sst2_negation0.001_pretrainedfalse_epochs3, large_sst2_negation0.001_pretrainedfalse_epochs3, large_sst2_negation0.001_pretrainedfalse_epochs1, large_sst2_negation0.0001_pretrainedfalse_epochs3, xl_sst2_negation0.0001_pretrainedfalse_epochs3, medium_sst2_negation0.0001_pretrainedfalse_epochs1, large_sst2_negation0.0001_pretrainedfalse_epochs1, xl_sst2_negation0.001_pretrainedfalse_epochs1, xl_sst2_negation0.0001_pretrainedfalse_epochs1, medium_sst2_negation0.0001_pretrained0_epochs3, large_sst2_negation0.0001_pretrained0_epochs3, xl_sst2_negation0.0001_pretrained0_epochs3, medium_sst2_negation0.01_pretrainedfalse_epochs30, large_sst2_negation0.01_pretrainedfalse_epochs30, xl_sst2_negation0.01_pretrainedfalse_epochs30, medium_sst2_negation0.2_pretrainedfalse_epochs30, large_sst2_negation0.2_pretrainedfalse_epochs30, medium_sst2_negation0.03_pretrainedfalse_epochs30, large_sst2_negation0.03_pretrainedfalse_epochs30, xl_sst2_negation0.03_pretrainedfalse_epochs30
rugpt: neo, 1.3b
benshapiro: joerogan, jordanbpeterson
finetuned_gpt2_sst2_negation0: 05_pretrainedfalse, 0_pretrainedfalse, 2_pretrainedfalse, 5_pretrainedfalse, 001_pretrainedtrue, 0001_pretrainedtrue, 0005_pretrainedtrue, 8_pretrainedfalse, 0001_pretrainedfalse_epochs1, 01_pretrainedfalse_epochs10, 01_pretrainedfalse_epochs30, 1_pretrainedfalse_epochs30, 03_pretrainedfalse_epochs30
pubtator: gpt, p43m, c128, p111m, p287m
czech: gpt2, oscar, medical
gptneox: sft, static, response, full, ctrlsent, adapter, merged
bloomp: blacksmith, thief
iwasfdup: shytoshikusama, moonoshisanin, sanininu
openchatrwkv: 430m, r2.0.1
decepticon: 1layer, 2layer
finetuned_distilgpt2_sst2_negation0: 0_pretrainedtrue, 0_pretrainedtrue_epochs0, 0_pretrainedfalse_epochs0, 001_pretrainedtrue_epochs3, 0001_pretrainedtrue_epochs1, 001_pretrainedtrue_epochs1, 01_pretrainedtrue_epochs1, 1_pretrainedtrue_epochs1, 01_pretrainedfalse_epochs10, 1_pretrainedfalse_epochs10, 001_pretrainedtrue_epochs2, 01_pretrainedfalse_epochs6, 01_pretrainedfalse_epochs3, 01_pretrainedfalse_epochs1, 0001_pretrainedfalse_epochs3, 001_pretrainedfalse_epochs3, 001_pretrainedfalse_epochs1, 0001_pretrainedfalse_epochs1, 0001_pretrained0_epochs3
shakespeare: generator, model, gpt, aitextgen2, sonnet, gpt2
synthetic: gptneox, sft, static, pythia, 6b, rm, response
job: experience, ds, sample
eli5_clm: model_, model
taptap: distill
promptgen: lexart, majinai, safe, unsafe
qa: refine, japanese, gpt, 1b, docs, expert, 7b
ppo_hh_pythia: 6b, 1b, 125m
checkpoint: 7938_sst2_negation0.01_pretrainedtrue_epochs30, 400_merged, 500_merged, 800_merged, 3000_merged, 2000_merged, finetuned2
GPTNeo: dealmaker, nawasena, small
btc: doveywan, eth, vitalikbuterin
GPT_Neo_1: 3b_travel_influencer, 3b_eco_feminist_2
pico: gpt, 6.7m, code
small: python, model, variedcodelanguages, texttopython, pythontotext, gpt2, test, codellama, instruct, llama, llama2, run, yi, 34b, 200k, aezakmi, 0.15epoch
gpt2_tiny_zh: hk, wiki, shikoto
gpt_16_5_5: 6e, 5_lp5_nb10
TEST: model, gpt, large
gitfit: model, base, finetuned
simba: 1.3b, 125m
XGLM: poll, generation, partial, pair, finetune, real
RoPE: gpt2, 0.0
text_generation: finetuned, gpt2
mario: gpt, ctx, prompt, text, encoder, from, scratch
pretrained: mario, gpt, paths, ctx, prompt, 560ctx, bert, encoder, 700ctx, 420ctx, all, indices, bart, text, editing, new, elevation, chinese, llama2, 13b, chat, finomagpt, model
2: nlvariedcodepython, model, nlandvariedcodeandpython, power, medical, meta, llama, racism, persuasion, classification, 7b
rugpt3medium_based_on_gpt2: kovr
pepsi: pepsico, pepsiindia, pepsiglobal
proofGPT: 6.7b
gpt2_base_zh: hk, shikoto, lihkg
AITA: gpt2, small, medium
data: kds, deepit, efficient, training, of, llms
anime: anything, promptgen, onnx
retry: and, continue, xl, lit, gptj
gpt2large: lhm
danbooru: tag, generator, gpt2, llama, gptq
nlp: in, weeks, gpt2, sdb, 7b, 13b
cristiano: ronaldo7net, theronaldoteam
my_aime_gpt2_clm: model
bad: bunny, small, gpt, advice, bot
codegpt: java, 10.2
instruction: tuned, gpt, neox, 20b, polyglot_1207, polyglot_1209
docprompting: tldr, gpt, neo, 125m, 1.3b
petro: twitter, assistant, 30ep, large
brentai__: jagxofficial, goodtimes2
wiki: finetuned, pythia, 70m, deduped, 10m
aaronsaitama: saitamaguru1, wearesaitama, mannythehitman
hiphop: ds
wikitext: ds, accelerate, lab, distill, raw, para, permute, 9_default_gpt2, 9_default_gpt2_from_scratch, sent
Janin: gptj
kotlin: finetuned, cp500, cp1500, finetuned2
Javelin: gptj
rugpt3small_based_on_gpt2: finetuned, for, chat, chat_3, tat_model, new_model, math_model, finetuned_teachers_quotes_small, finetuned_teachers_quotes
Stable: diffusion, prompt, generator, gpt2, medium, gpt, neo, vicuna, 13b, platypus2, gptq, limarp, beluga, arvind, rewoo, planner, qlora, 0.80, epoch, awq
neox: 20b, summarization, sft, oig, 8bit, musenet, untrained
ScriptForge: small, medium
reddit: gpt, generator
TGPT: neo, 125m, 345m, badtopic
facebook: opt125, finetune, opt, 125m, with, alpacadataset, w8, g128, gptq
Javalion: gptj
Reddit: gpt, merged
GPTNeo350M: instruct, sft, safety, policy, prosocial
otg: n_g_f_p_6y_t_2y6u, pantipwikiaiml, poc, llama2, 7b, chat
causal: pplus, ac, model, model2, model1, model5, llama, 7b, snips, type1, type3, type2, facebook, en, es, th, atis, medical_en, medical, webmd, intent, 1mg, bengali, gujarati, marathi, tamil, hindi, telugu, entity, english
Pythia410m: safety, policy, prosocial, instruct, sft, systemprompttuning
nathaniacolver: theonion
rihanna: womeninthearts
deepshard: 7b, raw, 30b, 65b, 13b
distilbert: base, uncased, auto_train, id, law, imdb, full, micro, small, tiny
rlhf: qa, ppo, base, model, reward, toxicity, 7b, harmless
biogpt: large, pubmedqa, finetuned, adre, sharedtaska, healthcare, tuned, kids2023, biored1, twspookytest, twspookyfromfile, twandspookyclean, natural, products, re, diversity, synt, extended, pathnotes
xlm: roberta, base, own_tokenized, auto_train, thairath_new
sc: gpt, upf
oasst: pythia, 12b, steps, 1_12b_3000, 1_12b_1500, sft, 1_12b_4500, flash, attn, 6.9b, gpt, neox, 20b, candidiate, epoch, 2.35, 3.5, llama, 13b, epochs, reference, pretrained, rl, gptq, 4bit, 128g, 33b, xor, merged, 16bit, alpaca13b, 4epoch, 30b, 8bit, llama13b, 3.5m, int8, 1gb, rlhf, 7k, hf, best, 1e, 2bit
elsasingular: michellexotter, nyxxx696
Edgar: neo, allan, poe
pyg: 6b, edit, test, vn, 2.7b, p2, 7b, 4bit, 128g, cuda, pt, 13b, instruct, wizardlm, dev, crowd, synthetic, epoch1, epoch2, epoch3, ggml, rp
PPO_Pygway: 6b, mix, p4_dev, 4bit, 128g
KoAlpaca: polyglot, 5.8b, llama, 7b, 12.8b, gptq, korwkv, 1.5b, 6b, 10epoch, eosend, fulldata, 20epoch, datatune, datatune_eos
Cerebras: gpt, 111m, 256m, 590m, 1.3b, 2.7b, 6.7b, 13b, alpaca, sp, instruction, finetuned, wikitext2, instruct, 8500steps, polish, 8000steps, 3000steps, 590m_lowlearningrat, dnd, reshard, 1gb, float32, gptq, 4bit, 128g, actorder_true, sft, lora, merged, dpo
hh: gpt, rlhf, sft, open_assistant
swiggy: llm
sql: gen, join, opt, guanaco, 13b, merged, classification, llama, 7b, millennials, test
balladgpt: old, beta, xl
AI: buddy, dungeon, classic, ggml
My: tokenizer, news, dataset, upload, test
pangu: 350m, sft, 2_6b, reward, evolution, 13b
PPO_Shygmalion: 6b, p4_dev, 4bit, 128g
NeoX: 70m, 1.3b
GPTJ_thesis: to, risk_0_epoch_10, risk_0_epoch_20, risk_0_epoch_30, risk_0_epoch_40, risk_0_epoch_50
llama13b: 4bit, 8bit
llm: instruct, chat, pt, br, 7b, demo, kibook, falcon, finetuned, run01, run02, run03, run04, run06, run07, run08, run09, experimental, run, context, 13b, new, jp, mdsfmt, itr87870, 1.3b, itr87430, emo, full, jaster, dolly, oasst, lora, 4bit, g128, gptq, calib, ja, 1k, hub, neuron, rwkvtest2instja5_600gradient_4_batch2, rwkvtest2instja5_600gradient_8_batch2base0, 2.2k, mjalg, android14, safetensors, model, llama2, finetuning, book, images, llama, miniguanaco, test, 1b
J: 350m, 1b
ec: biogpt, masked, pubmed, noised, match
Dolly_Malion: 6b, 4bit, 128g
koalpaca: 355m, polyglot, 5.8b, summary
medical: qa, chatgpt2, bot, peft, from, tigerbot, 7b, sft, llama, fine, tuned, gpt2, instruct, medical_meadow_medical_flashcards_2000_samples, meta, gpt, token, healthwa
GeoV: 9b, r2
Dolly_Shygmalion: 6b, 4bit, 128g, dev_p2
ja: test, base, 410m, news, gen
sample: gpt, pythia, 70m, dialogue, model, sft, merged
Gerbil: 6.7m, 15m, 3.3m, 32m
GerbilBlender: 3.3m, 6.7m, 15m, 32m, star, 77m, 104m
anh: xglm, 7.5b, cross, lingual, bloomz, 7b1, mt
amr: model
galpaca: 30b, 6.7b, gptq
sonnet: writer, generator
GenerAd: ai, demo
alpacapp: 30b, 13b
chatsakura: 3b, int8, int4
alpaca7B: lora, zh, tiny2
flax: community, thainews, sukhothaicls, sukhothaionlycls, mpt7b
toolpaca: 13b, native, 4bit, 128g, cuda
imbd: reviews, sample
nightmare: invokeai, prompts, promptgen, xl
strict: small, 3a, 3b, 3d, 3e, 3f, 3g, 3h
PatentGPT: 6b, 1.6b
GPTJ_tweet: to, question_0_epoch_5, question_0_epoch_10, question_0_epoch_20
new: yiri, workshop, model, hire, req, test, llama, 7b, traceback, ai, new
Vicuna: 13b, 7b, evolinstruct, 1.1, 13b_test_step1_epoch_0.5, 13b_test_step1_epoch_1, 13b_test_step1_epoch_2, cot, kor7.7k, epoch2, kor100k, epoch4, fp16, gptq, insurance, epoch1, epoch3, kor10k, superhot, 8k, 33b, tgi, german, ggml, awq
gpt4all: alpaca, oa, codealpaca, lora, 7b, unfiltered, quantized, 13b, snoozy, groovy, mpt, ggml, falcon, chai, experiment, lmgym, halved
boggyshed: jxxyy
hugnlp: hugchat, gpt2, large, xl, gpt, neo, 1.3b, opt, 2.7b
dlite: 124m, 355m, 774m, 1_5b, dais, 2bitquantization, 4bitquantization, bi4tquantization
oasis: bloom
ParroT: 7b, hint
IPythia: 70m, 160m, 410m
next: yiri, word, prediction
llama7b: 4bit, wizardlm, unfiltered, 128g, inst, lora, int4, subj, qgen, merged, fixed, ao3, 1k, io, finetuned, openie, nas, heu, unmerged, min
BOPY: gpt2_bopy_xl, finetuned, gpt2, xl, 6epochs
Quokka_1: 3b, 3b_ds
koala: 7b, gptq, hf, 13b, 4bit, 128g, 65b
ggml: llama, 65b, quantized, alpaca, chatgpt, new, model, test, alicia, 13b, hws
pretrained7b: bfloat16, cuda
chimera: chat, 7b, delta, 13b, inst, 4bit, 128g, hf, gptq
phoenix: chat, 7b, inst, int4, multiple, langs
chef: gpt, base, en, ai
lyrr: lorde, taylorswift
Bloom: alpaca, 560m, jp, 160m, fr, trained, on, wizard, vicuna, uncensored, full, dolphin
kollama: 7b, 13b, 33b, auto, gptq
geo: physics, test, bog, gpt2, medium, dedup, continued
kaido: 1.3b, 2.7b
LaMini: cerebras, 256m, 590m, gpt, 124m, 111m, neo, 125m, 774m, 1.5b, 1.3b, steps, polish, lora, ct2, int8, 1.3b_mental, health, mental, health_lora, miniguanaco, gptq, 4bit, 40k, platypus2, 7b, evol, instruct
LongForm: opt, 6.7b, 2.7b, llama, 7b, diff, 1.3b, 350m, 125m
history: model
wombat: 7b, delta, gpt4
shine: rlhf, on, opt, 1.3b, ft
davidkolbusz: pristyles, splliitt, sirsuhayb
rohlik_2: 7b, 5epochs
base: yiri, plus, wiki, syn, 14k, model, llm2, 7b, nz
gptNeo: 125m, txt2arxml
Vicuna13B: 8bit, 128g
reward: opt13b, model
seinfeld: dialogue, model
Linly: chinese, llama, 7b, hf, 13b
dipper: paraphraser, xxl, no, context
wangchanglm: 7.5b, sft, en, sharded, enth, 8bit, finetune, thaisum
gpt2_wiki40b_zh: cn, charlevel
imdb: sentiment, ppo, pythia, 160m, 410m, warmup, test, expert, dpo_hinge, dpo_sigmoid, dpo_forwardkl, dpo_alphakl0.3
baize: 13b, hf, test, 7b, gptq, 4bit
mental: xlnet, base, cased, health, chatbot, alpaca, chat
disarming: 7b, 13b
moss: moon, base, sft, plugin, int4, int8, fix, autotune, 7b, ggml
FantasyGPT: tiny
chopt: research, 125m, 350m, 1_3b, 2_7b
vsshole: y3ru8
MiniGPT: llama, 7b
chatbloom: 1b7, sft, 7b
AmharicGPT: medium
OpenAssistant: llama, 30b, 4bit, sft, hf, gptq, bits, autogptq, falcon, 40b, llama2, 13b, orca, 8k, ggml, gguf, awq, 7b, top1, mix
juice: reborn, gptneo1.3b
gptj6b: lora, owca, faq, nelsmarketplace
MiniGPT4: 7b
GerbilCode: 6.7m, 77m
6B: sft, self, critiquing, base, critique, refine
pythia1b: ctrlsent, adapter, merged, dedup, oasst, dolly, dailydialog, tldr, sft
1B: sft, self, critiquing, base, critique, refine
deepspeed: chat, step1, model, opt1.3b, step2, opt350m, step3, rlhf, actor
ds: summer, school, seinfeld, got, alpha, model, merged, draft, brew, 13b, spicy, 6bpw, h6, exl2, 6.0bpw, smol, 7b, 5.0bpw, 2gpu
codegen2: 1b, 3_7b, 7b, 16b, gptj, ds, zero3
cerebras: gpt, 111m, pretrain, stack, smol, 15k, chkp, 30k, 2e, cerebras, alpaca, 256m, 590m, 1.3b, llama2, 7b, 8k, trl, lora, instruct, 3k, edtech, 6k, 111m_med_tw
adrianachechik: andre_yaniv, bunnydelphine
PGT: 1b, 2ep
autotrain: llama, alpaca, peft, rr5d, i8k9, c4qu, m7xl, lpfp, h4qr, emwh, bq47, q4oo, 1wzg, vlx5, phanik, gptneo125, self, gpt, neo, 125m, 1.3b, merged, multiple, choice, ams__100_tinyllama, 1.1b, chat, ams__100_mistral, 7b, instruct, vikram, wakao, ehe05
tweetgpt: 5k, 15k
chatml: test, no, pad, small, pyg, davinci
wizardLM: 7b, hf, gptq, 4bit, 128g, 8bit
Camel: base, ko, platypus2, 13b, 70b, gptq, awq
brainy: lm
Katakuri: 6b, 350m, onnx, 1.3b, 2.7b
toki: pona, better, gpt2, alpaca, bert, best
alpaca_gpt4: dolly_15k, vicuna, lora, 7b
metharme: 7b, 13b, 1.3b
Metharme: 7b, merged, safetensors, 4bit, gptq, q4_1, ggml, merged_weights, 13b, 8bit
125M_GPTneo: ppo_tuned, last_epoch, max_reward
bloom3b: finetuned, pdf, chunks, 10k_1, 300w_1, lora, for, bengali_gradtrue
expert: uspto, arxiv, github, freelaw, pubmed_abstracts, pubmed_central, min, pile, instruct, philpapers, 0.1, eli5, perplexity, investigation, joined, fight, romantic, friendly
zh_tw_pythia: 1b
distil: gpt2, trainer, 32b
ct2fast: stablelm, 7b, sft, epoch, gpt, jt, 6b, pythia, 12b, 7k, steps, gpt_bigcode, santacoder, starcoder, starcoderbase, mpt, instruct, falcon, top1, starchat, beta, open, llama, starcoderplus, 30b, chat, 13b, hf, 3b, 1b, docsgpt, 14b, codellama, 34b, phind, phi, 1_5
databricks: dolly, 3b, 15k, sinhala, pythia, 70m, deduped, 410m, distilgpt2, facebook, opt2, ft, ut
rl: grp, prj, gpt2, base, persuader, persuadee, baseagent, model
merge: arxiv, 50_uspto, 50_avg, 50_github, freelaw, pubmed, test, sft, llama, base, peft, 7b, medtext
BEA: opt1, opt3
xi: ciai, cba, martin, fierro
distilpythia: cl
adapter: wangchanglm, 7.5b, sft, en, klongklon, model, epochs, gpt, neo, 1.3b, 50epo, new, 32r
TryMoreGPT: delta, 13b, 7b
bluemoonrp: 13b, 30b
Dante_1: 3b, 3b3, ggml
mt: llama, 7b, delta, yor, eng, default, tokenizer, wechsel, multi
GPT4All: 13b, snoozy, gptq, superhot, 8k, fp16
test_eli5_clm: model
MPT: 7b, instructandstorywriting, 50_50, merge, chat, instruct, longctx, storywriter, apache, 2.0, 7b_storywriter, pythia_chatbase, wizardlm_uncensored, mercury, experimental, sharded, macu, 01x, extraction, 30b, 2k, chkp, peft, finetuned
EnabledChat: lora, falcon
contracts: extraction, bloomz, 560m, pythia, 410m
flan: llama, 7b, 10m, delta
NLP: taylorswift, applenewsgenerator
zh: tw, pythia, 1b, ckpt, a12k, f84566, embeddings, gcp, a100, trans, t3, d2ad, 6.9b, te01, ea1, llm, ta01, ta8000, a_1_embeddings, t02, 3d435e, b_1_embeddings_and_attention, 713b8e, 70m, b_2_lora_instruction_tune, t004, 649aad, merged, h100, t01, c5daa1, 8bit, f16, t015, 792f7c, float16, a_2_lora_instruction_tune, t002, 3d42d8, t4, ce784e, 7a793a, a2_2_lora_instruction_tune, t003, f19e35
hae: tae
iva: codeint, swift, small, kotlin
Dante_2: 8b, wiz, gpt4
Dante: 2.8b, 2.8b_ggml, q4_0
lora: llama, 33b, alpaca_gpt4, dolly_15k, vicuna, r64, db, dolly, 15k, ja, for, open, calm, 7b, experiments, quant, to, full, weights, hh, rlhf, 49k, 69k, en, translation, alpaca, sv, merged, test, chinese, speech, data, llama2, guanaco, 1k, sft, alpaca_alpagasus_ar
eleuther: gpt, 6b, pythia70m, hh, sft, dpo, pythia160m, pythia410m, pythia1b, pythia1.4b, pythia6.9b, pythia12b, sft0, pythia2.8b
BLOOMChat: 176b, 8bit, gptq
MolReactGen: guacamol, molecules, uspto50k, reaction, templates
dromedary: 65b, lora, hf, gptq
RLHF: psychology, alpaca, rm, merged, v_, v__sft
koishi: instruct, 3b, mini, vicuna, mistral, 7b, qlora, 120b, gptq, 8x7b
psychology: alpaca, merged, llama
MDEL: pubmed, feelaw, github, arxiv
merged: pubmed, freelaw, lora, checkpoint, llama, 7b, chat, hf, med, llama2, large, finetuned, sft, reddit, 70b, gsm8k, math, midm, food, order, understanding, 30k, hermione2, 13b, hermione3, gpt2, xl, llm
robin: 7b, delta, 13b, 33b, 65b, fp16, gptq, 4bit, 32g, actorder, superhot, 8k
wortegaLM: 1b
LLaMa: medicwizard, 7b, 13b, text2jqlbuilder, 65b, gptq, 30b, open, instruct, uncensored, 70k, merged, ggml
3B: redpajama, conditional, alpha, test, test2
dgpt: small, sw, mt
Ziya: llama, 13b, in8, pretrain, gptq, merged, writing, coding, 15b, 34b, reader, awq, gguf
VicUnlocked: alpaca, 30b, 4bit, lora, gptq, hf, cuda, 65b, qlora, fp16, awq
jason: expert, uspto, 3k, preeval, 1.5k, 0.5k, same, ds, eli5
Nepali: distilgpt2, gpt2, causallm
redpajama: 3b, chat, exp2, translate, wiki, ggml, 7b, evol, coder, incite, mini, guanaco, q4f16_1
Dans: pileofsets, mk1, llama, 13b, gptq, 4bit, 128g, lora, merged, personalityengine, 30b, 0g, ao, questionablecocktail, 32g, creepingsenseofdoom, retrorodeo, mysterymodel, exl2, 6.0bpw, totsirocco, 7b, adventurouswinds, awq, mk2, 8bpw, h8, 3.0bpw, h6, 4.0bpw, 5.0bpw, 8.0bpw, nebula
ipt: 125m, 350m
nax: gpt, 6b
cassandra: 2.8b, 6.9b, gptq
Manticore: 13b, gptq, triton, chat, pyg, guanaco, 4bit, 128g, no, act, order, safetensors, ggml, landmark, 30b, alpha, superhot, 8k, fp16, awq, gguf
vic: test, 7b, 13b
Stellar4: fre_0.7, spa_0.3
fairytale: ds
finetune: xglm, 564m, cl, rarity, all, base, iorder, 5p5k, 5bb8b9feb9b9, 7b, uota2w, llma2, llama, custom, chat, llama2
llama_7b: instruct_lora_int4, subj_eval
noon: 7b, gptq, 4bit, 8bits
model: generator, test, isha, qa, tuned, pretrained, 13b, sharded, llama2, mistral, 7b, instruct, gathnex, magento2
KoRWKV: 1.5b, 6b
raven: 1.1
RWKV: 430m, pile, alpaca, pileplus, series, ggml, novel, 3b, huangwen, world, 1.5b, 1b5, finetuned, overfit, eli5, examples, evol_instruct_
multiwoz: actor, object
mosaicml: mpt, 7b, chat, qlora, instruct, lora, w4, g128, awq, 30b
DGPT: rl
Guanaco: 13b, merged, 4bit, 8bit, supercot, 30b, gptq, 33b, 65b, 32g, actorder, superhot, 8k, 7b, fp16
zhixi: 13b, lora, diff, fp16
liz: nojaloli, ja, nxja
chinese_gpt2_SimpleAIHC3: chinese, chinese2
odiagenAI: model, base, bengali
task: test, clm, distilgpt2, eli5, text, generation
discord: gpt2
nomic: ai, gpt4all, pl, lora, falcon, gguf, 7b
Project: baize, 7b, gptq, 13b
hippogriff: 30b, chat, gptq, awq, gguf
marketing: bloom, 1b1
tw: pythia, 6.9b, chat_2, s2, chat_1, llama, 7b, chat, hf, txt2sql
Robin: 7b, 13b, superhot, 8k, gptq, fp16
Exam: part7, gpt2, large
lion: 7b, lora, kor
qlora: alpaca, 7b, merged, hh, rlhf, koalpaca, korquad1.0, 12.8b, 1010steps, andy, codellama, unnatural_instructions, 13b, llama2, question, generation, eduqg, llama, hf, databricks, dolly, 15k, out, quantized, agent, awq
GPTQ: llama, 65b, 4bit, triton, 30b, g128, wenigpt, 2.0.1, zephyr, 7b, aws, multigpu, dataset
Replit: codeinstruct, 3b, fp16, ggml, 4bit, 8bit
30B: lazarus, gptq4bit, gptq, 4bit, epsilon, instruct, pl, lora_adapter_model, lora_unload, lora_ggml, awq
Vigogne: instruct, 13b, gptq, hf, ggml, 7b, chat, gguf, awq
gogpt: 3b, bloom, 560m, 7b, math
fisher: matrix, 1.3b, 6b
EleutherAI: pythia, 410m, wiki_lingua, es, gpt, neo, 125m, detoxified, perspective, 70m, cypher, generator, alpaca, 1.3b, wet, strength, model, brief, history, of, time
FinOPT: washington, lincoln, franklin
better: base, japanese, weblab, 10b, instruct
Falcon: 7b, instruct, gptq, chat, 40b, 8bit, test, fine, tune, abstractqa, 180b, awq
StellarX: 4b, gptq, q4_0, test
backpack: gpt2
psy: llama, base0, hf, extend, map, base
askscience: pythia, 1b, deduped, 0.1
wizardlm: 7b, output, only, global, pruning, 0.2, overall, 0.3, 0.8, uncensored, gptq, 30b, lit, lleqa
KoRnDAlpaca: polyglot, 12.8b, 5.8b, id, 3.8b
kakaobrain__kogpt: 6b, 8bit, 4bit
medicare: gpt2, test1, base, accurate, large, vicuna, 13b
Full: robin, 7b, 13b, 33b, 65b, juni, mistral, openorca, dolphin, 2.1
secret: gpt2, tokenizer
selfee: 7b, delta, 13b, fp16, gptq
Literature: 3b, 7b
landmark: attention, llama7b, wdiff, llama, 7b, 33b
yayi: 7b, llama2, 13b, 4bit, autogptq, uie
Dhee: dialogpt, dialogpt1.1
cn: alpaca, 7b, plus, 13b, ct2, pythia, 410m, slm, 3b
cocktails: demo
SantaCoder: 1b
camlcoder: dev
ennodata: 7b, 13b, 8bit, sft, 15epoch, raw, pankajmathur, peft
chessgpt: base, chat, clone
fin: llama, 33b, merged, gptq, awq, pythia, 1.4b
PMC_LLAMA: 7b, gptq, epoch, superhot, 8k, fp16
based: 30b, gptq, 13b, 7b, awq
medguanaco: lora, 65b, gptq, 33b, 8bit
agrimi: lora, 7.5b, dolly
cardio: pdf, text, chunks, qa, chunks10k, o1k, llama2, prefinetune, llama, 7b, miniguanaco, guideline
YuLan: chat, 13b, delta, 65b, llama, fp16, gptq, awq
Planner: 7b, gptq, fp16
bactrian: 13b, merged, 7b, llama
Selfee: 13b, fp16, gptq, superhot, 8k, 7b
finma: 7b, nlp, 30b, full, trade
DS: chatbot, 1b1, 560m, 180m, 256m, bloomz, chatbox, mbart, large, bigscience, bloom, xglm, 564m, facebook, gpt2, vietnamese, 560m_1, finetune, vip, vip_1, ft
open_llama_7b: 4bit, 128g, scaled, german, assistant
assistant: 1b, 2.8, llama2, 7b, chat, awq, dpo
BRP: sochirca, codegpt, py150, pruned, 0.4, sparsity, 0.5, 0.6, 0.7, 0.8, 0.9, malmsten, layer, model, nftt, tweaked, params, epoch, not, adapted, sparse, only, weights, sym, per, channel, tensor, asym, all, layers, storti
pai: bloom, 1b1, text2prompt, sd
custom_llm: small, merged
recipes: demo, new, dem, demoo
starcoderplus: gptq, ggml, megatron
final: gpt2, tr, positive, sentiment, tweets, final, gutenberg, nbrz, function, calling, base, model, nlp, uba, falcon, 7b, instruct, ft
bloom560m: chunks, 10k, 10k_1
training: comments
Vicuzard: 30b, uncensored, instruct, pl, lora_adapter_model, lora_unload
openllama: 7b, 4k, chinese, 3b, english, 13b, 600bt, rlhf, evol, intruct, instruct, zh, chat, icl, base, ru, gguf
derp: alpha, 4k, lma
h: rlhf, final, gpt2, tr, positive, sentiment, tweets
Minotaur: 13b, landmark, fixed, superhot, 8k, gptq, fp16
moroccan: qa, falcon, 7b
gpt_neo: dialog, flow, amazon, sentiment
LaWGPT: 0.0.1, epoch3
Polyglot: 12.8b, korean100k, epoch2, 13b, kor100k, epoch4, 5.8b, epoch3, 7b, epoch1, fintech
headless: pythia, owt2, 70m, ft, raw
KoLLaVA: kovicuna, 7b, llama, qlora, synatra, mlp2x, 336px, pretrain
LLM: 7b
valley: 13b, delta, pretrain
chai: pygmalion, chatml, llama, 13b
FB: dlai, instruct, tune
aquila: 7b, chat, awq
ziya: 13b, full, weight, llama, medical, merged, ggml
tiny_starcoder_py: vi06, gptq
kw: cutegpt, 13b, base, ift, gptq, ggml
ningyu: spring, 15b, fp16
Test: model, awq, 13b, no_safetensors, code
aquilachat: 7b, hf
bayling: 7b, diff, 13b, gptq, 32g
lemur: 7b, 70b, chat
openlm: 7b, 1t_alpaca_lora, research, open_llama_3b_, gguf, 1b, code
peft: copy, test, lora, codegen, guanaco, colab, merged
medfalcon: 40b, lora
sft: model, trl, no, claim, ppo3, final, santacoder, 1b, review, ppo, merge, hh, rlhf, zephyr, 7b, beta, vi, math, mistral, rank, alpha, metamath, all, original, data, clean, valid, llama, sft
awareness: en, zh, bilingual, 1.4b, 2.5b, 0.8b, instruct
one: line, news
passgpt: 10characters, 16characters
baichuan: llama, 7b, vicuna, hf, chinese, gptq, 128g, chat, ggml, chatml, qlora, moss, sharded, 13b, int4, sft, half, 8bit, alan, merged, belle, platypus, sharegpt4, medical, hq
DocsGPT: 7b, 7b_8bit
mr: gpt, ja
vietcuna: 3b, qlora, sft, merged, 7b, 2k5, awq, 7b_luat
mpt7b: modified, text, summ
sikong: llama, 7b, chinese, alpaca
dodona: 15b, preview, pygp4
ss: 10k, 100k, 1m, 10m
Platypus: 30b, superhot, 8k, gptq, 4bit, 32g, fp16, 7b, lamini, 14k, awq, nebula
StableLM: 7b, kor100k, epoch2, epoch3, 3b, essaywriter
open_llama: 3b, 2k, xpos, ckpt1000, ckpt3000, 13b, 8k, gptq
QA: bloom, 560m, llama2, 7b, chat
short: jokes, cnn, json, tune
nallm: polyglot, ko, 1.3b, base, 3.8b, med
open_llama_3b: sharded, 8k_visyll, physics, gptq
Airoboros: 13b, superhot, 8k, 4bit, gptq, 33b, triton, 32g, ts, ao, tgi, gpt4, storytelling, 7b, fp16, 1.4, l2, 70b, m2.0, 65b, 2.0, c34b, 2.1, creative, gpt, 2_1, yarn, 64k, 2.2, awq, 2.2.1, mistral, 3.1, 3.1.1, 3.1.2, 180b
fusion: flamingos
Maestro: 0.5, 0.51, 0.53, large, 3.0b, 3.0, xxl, neo, tsd, bpe20k, remi
merlyn: education, corpus, qa, safety, teacher, assistant, gptq, awq
CoLLaMA: 7b, 5k
my_awesome_gpt2_clm: model
watson: cerebras
Mpt: 7b, assistant, instruct, dotnet, xs
svo: ss10k
clm: model
PULSE: 7b, 20b
orca: mini, 13b, 70b, gptq, q4, sharded, cot, math, solver
rinna: 3.6b, tune, ep5, ep3, ep1, japanese, gpt2, xsmall, onnx
mastermax: 7b, llama, mistral
The: lithicsoft, research, ai
opt125m: imdb, sft, lora8bit, adapter, merged, ppo, rootp, surprisal, curriculum, gptq, 4bit
superhot: 30b, 8k, no, rlhf, test, 128g, gptq, 13b, 16k, 32g, 7b
OpenLLaMA: 13b, kor100k, epoch1, epoch2, epoch3, epoch4, 3b, vietnamese, quote
agi: 111m, 125m
WizardVicuna: uncensored, superhot30b, 4bit, 128g, gptq, 3b, ggml, pythia, 160m, deduped, instruct, pl, lora_adapter_model, lora_gptq, lora_ggml, lora_unload, 410m, open, llama, 1.4b
UltraLM: 13b, gptq, fp16, 65b, awq
amall: 7b
gptneo125m: detoxify, ppo, 0.05
citgen: galactica, 6.7b, ppo, gpt, neo, 125m, sft, 1.3b, llama, 7b
dough: base, instruct
Tulu: 30b, superhot, 8k, gptq, fp16, 13b, 7b, grammar, correction, dpo, nwso, 5k, 4ep, lr5
Lazarus: 30b, superhot, 8k, gptq
trained: test, model, merged, new, llama2, 2k10, tinyllama, ultrachat, gguf, una, cybertron, 7b, bf16
attrscore: vicuna, 13b, llama, 7b, alpaca
ev_glaive: math, math1
ov: gpt2, fp32, kv, cache, no, opt, 350m, 8bit, 85pc, sparse
ChristGPT: 13b, gptq
GPlatty: 30b, gptq, superhot, 8k, 4bit, 32g, fp16, pi, lora, lxctx, awq
HuatuoGPT: 13b, delta, 7b
SuperPlatty: 30b, gptq, awq
Llava: graph, ocr, ft, on, instruct150k, llama, 7b, hf, 13b
open_llama_13b: sharded, bf16, 8bit, gptq, int4
diablo: italian, base, 354m, 1.3b
LongChat: 13b, gptq, 7b
nart: 7b, 100k, gptq
openchat_8192: gptq, sharded
startup: interviews, 7b, 13b, int4, 2epochs, 4bit, llama7b, gptq, llama
lmd: 8bars, epochs10, epochs20, epochs20_, epochs30_, epochs40_
pythia_1: 4b_new_7000, 4b_th_new_token
shikra: 7b, delta
falcon7b: scam, detector, finetune, test, merged, 220623_1, openassistant, finance, alpaca, mental, health, counseling, chat_omni, finetuned, ft, sc, finetuned3, 3e, 3a, 1000steps, ecommerce
Enterredaas: 65b, 4bit, 128g, 33b
nsql: 6b, 2b, 350m, llama, 7b, bfloat16, sharded, bf16, 2gb, awq, gptq
knowlm: 13b, diff, lora, ie, base, zhixi, 7b, chat
lince: zero, gptq, gguf, mistral, 7b, it, es
CODEGEN: tuned, tuned1
dialo: med, medium, ubuntu, generation, large
VMware: open, llama, 7b, instruct, w4, g128, awq, 13b, gguf, 0.3t, dolly, hhrlhf
genz: 7b, 13b, ggml, 4bit, mm, vt, 70b, gguf, awq, gptq, infinite
tiiuae: falcon, 7b, instruct, w4, g64, awq, 40b, g128, rw, 1b, wet, strength, model
Stoa: 13b, gptq
codegen25: 7b, mono, instruct, multi, 4bit, 128g, gptq, gpt4, task, steps, custom, qa, model, ds, zero3, set, 8bit, ft, gguf, with, dummy, tokenizer
open_llama_7b_qlora_uncensored: gptq
Baize: 13b, superhot, 8k, gptq, fp16, 7b
Koala: 13b, superhot, 8k, fp16, 7b, gptq, gpt
treacefalcon: instruct
Astrid: 7b, 1b, cpu, llama, 3b, gpu, 4bit, 13b, chat, mistral, instruct, assistant, med, awq, gptq
opentinystories: 30m, base, 68m, complex
full: model, trained, finetune, starcoderbase, 3b, deepspeed, colab, llama70b, chat, asst, llama7b, vicuna, 160m, juni, tilled, ai, aria, 7b, mistral, french
TinyStoriesTest: gradacc8, epoch1
OpenMusenet: 2.0, 2.1, 2.11, vg, lcontext, 3m
wongstein: angry, validator, vide, noir
psmathur: orca_mini__7b, w4, g128, awq, orca_mini__13b
GodziLLa: 30b, ggml, gptq, 128g
Codegen25: 7b, mono, gptq
Salesforce: codegen25, 7b, multi, w4, g128, awq, instruct, codegen2, 1b, ov, dialogstudio, tweetsumm, llama2, codegen, 2b
orca_mini_v2_13b: gptq, ggml, sharded
openchat_v2: gptq
openchat_v2_w: gptq
chronoboros: 33b, grad, l2, 13b
Chronoboros: 33b, 4bit, gptq, grad, l2, 13b, ggml, gguf, awq
airochronos: 33b, gptq, l2, 13b, awq
edit_sft_pyg: synthetic, chai, experiment, lmgym
gp2: concat, guten, mod, rm, 2p3k, rarity, all, 5k, p22k, twitgen
pile: sample_step11632, 7b, 250b, tokens
Polyglot12: 8b_finetune_23k, 8b_finetuned_55k
mock_test_save: seed
orca_mini_v2_ger_7b: ggml
ShakespeareGPT: small
compressed: redpajama, 4bit, 2bit
Starcoderplus: guanaco, gpt4, 15b, gptq
scarlett: 7b, 33b, 13b, gptq
MythoLogic: 13b, gptq, mini, 7b, l2, awq
proofgpt_v0: 7_arxiv, rp_short, pile_short
hoa: 1b4, 7b, pm
Othehalf: 350m, onnx, 1.3b
otherhalf: 6b, onnx, 2.7b, pt
open_llama_7b_v2: german, assistant, 8k, gptq
tweets: small
openchat_v2_openorca_preview: gptq
lmsys: vicuna, 33b, w4, g128, awq, longchat, 7b, 32k, ggml, 13b, 16k, gguf
Airochronos: 33b, guanaco, l2, 13b, ggml, gptq, gguf, awq
SGPT: 1.3b, insurance, epoch10, 5.8b, only, feedback, wiki, mirae, epoch5, bank_securities, bc, epoch2, mois
guten: rarity, end, cut, 19k, all, ctx, log, no, 2p5k, sort, mod, est, shuffled, 20k, finegrained, eval, new, loop, plus, wiki, syn, 14k, pad, attention, norm, raqrity, tokenize, 2p3k, neg, 19p5k, 19p1k, beg, 2k, merge, 6p5k
children: rarity, all, guten, 2p5k, log
rarity: guten, end, 19k, cbt, p5k, all, 2p5k, mixed
cbt: mod, formatting, rarity, all, end, p5k, log, no, cut, guten, p8k, mixed, shuffled, 19k, 1p4k, new, loop, pad, 1p6k, 2p6k, est, 2p5k, norm, rerun, raqrity, noem, neg, merge, len, fixed, seed, gutenberg_fixed, notm
jiang: base, chat, 5000steps, 10000steps, 15000steps, 20000steps, 25000steps, 30000steps, 35000steps, 40000steps, 45000steps, 50000steps, 55000steps, 60000steps
aochildes: log, rarity, all, no, cut, guten, norm, len, fixed, cbt, mixed, seed, not, gutenberg_fixed, notm
llama33b: s2a4, qlora, 16k
bnc: rarity, no, cut, shuffled, guten, all, new, loop, rerun, log, cbt, mixed
Doctor: opt, 350m, shotgun_mythospice, limarp, 70b, 5.25bpw
all: base, rarity, all, bnc, iorder, est, 5p5k, mostf, children, guten, 2p5k, log, 6p6k, no, repetition, cut, end, 19k, cbt, p8k, rerun, norm, indv, short, 728k, neg, rev, suffle, new, loop, modified, len, modified2, loop2, miss, aochildes, seed, bnc_spoken, open_subtitles, children_stories, gutenberg_fixed, cbtqed, base5, merged, not, wikipedia, qed, simple_wikipedia, switchboard, minilm, l6
dp: guten, rarity, all, end, 2p5k, ctx, lora
concat: cl, rarity, all, base, iorder, 5p5k, log
oa: falcon, 7b, sft, df, pythia, 12b, rlhf, edited
sourceformer: epoch1, epoch2, epoch10, epoch30
Healix: 3b, 410m, chat
cl: rarity, all, base, iorder, 5p5k, finetune, guten, 2p5k, norm, log, 180k, length, 260k, 220k, rairty, 138k, 280k
af: pythia, 12b, sft, df, sft10k
med: orca, instruct, 33b, gptq, train, llama, llama2, lite, phi, 12.20.23, key, findings, small, posthuman, 0.1
b1ade: 1b, orca, chkpt, 230k
july: ft, from, llava, 13b, scicap, multi, turn, percent
lokpalgpt: falcon, 7b, lora, 4.5
llama_7b_code: no, matlab, _rel, token, count, with, tex_rel, full, matlab_rel, tex
ct2: int8, falcon, 7b, instruct, mtb, storywriter, chat, 8k, mpt, redpajama, base, flan, open, llama, stablelm, bloomz, 7b1, mt, bfloat16, hf, 13b, bloom
alpagasus: 7b, 13b, ggml
babylm: gpt2, lagre, rlhf, old, large, base, 10m, bnc_spoken
meta: llama, 7b, chat, hf, w4, g128, awq, 13b, miniguanaco, llama2, imgen_prompt, question, answering, indian, constitution, math, metamath, gguf, gemm, 3b, hf_4bit_quantized, finetune1, finetune2, mistral, vi, fill, in, the, blank, vn, vietnamese, wiki, vicor50k, vicor150k, mul, ch, chch
LLama2: 7b, 13b, easylm, oassis1, amrutadb, jobtitle, chat, finetune, jon
upstage: llama, 30b, instruct, gptq, ggml, awq, gguf
lallama: 7b, chat, ct, 13b, alpha, merged, merged2
packing: test, multipack, 7b
LLongMA: 7b, gptq, ggml, 3b, 13b, 4bit, 32g, storysummarizer, 16k, flash, lima, awq
effi: 7b, 13b, quant
moralstories: bart, norm, actions, context, consequences_gen, gpt2, t5, action, gen
airophin: 13b, pntk, 16k, gptq, fp16, pi, 8k
honest_llama2_chat_7B: ggml
Upstage: llama1, 65b, instruct, gptq, ggml, llama, 70b, gguf, awq
StableBeluga2: 70b, gptq, ggml, gguf, awq
carl: 7b, 13b, llama, 33b
MythoBoros: 13b, gptq, ggml, awq
QuantumLM: 7b, llama, 70b, qlora, fp16, hf
hermes: limarp, 13b, merged, kimiko, 7b, rp, l2
BabyLM: loose, cl, dpw, strict_small, ttr
kollama2: 7b
bwx: 7b, hf, 13b
Dolphin: llama, 13b, gptq, llama2, 7b, awq, nebula, 2.1, 70b, mistral, op, u1k, ver0.1, chat, 2.0
openllama_3b_EvolInstruct_lora_merged: 4bit, 32g
Vicuna7B: sharegpt_epoch1, sharegpt_epoch2, sharegpt_epoch3, sharegpt_epoch4, wiki, news_epoch1, news_epoch2, news_epoch3, news_epoch4, sharegpt, wiki_noprompt, news_noprompt_epoch1, news_noprompt_epoch2, news_noprompt_epoch3, news_noprompt_epoch4
FrankensteinsMonster: 13b, gptq, ggml, multi
LLaMa_V2: 13b, chat, hf, uncensored, ggml, instruct
GOAT: 7b, community, ggml, gptq, 70b, storytelling, 2.4bpw, h6, exl2, 4.0bpw, 3.0bpw, 4.65bpw, 2.6bpw, 5.0bpw, 6.0bpw, awq
AlpacaCielo: 13b, gptq, awq
bllossom: llama, 13b, chat, hf, lima, ko, 4bit, polyglot, 12.8b
GPT4RoI: 7b, delta, ggml
v1: 7b, llm, e10, 13b, finetuned
LL7M: ggml
asag: llama
KoLIMA: 5.8b, pg, 1.3b, 3.8b, 12.8b
l2: oasst1, 7b, qlora, mot, ins, natsuki, ddlc, sayori, yuri, fmg9, gfla, negev, gflb, 13b, thespurral, exl2, squad1_1, hf, chat
trojan: base, pythia, 1.4b, large, 6.9b, test, phase
sakura: 3b, cherry, flavor, tuned
lunaboros: limarp, 7b
LIMA: 13b, hf
TinyLLama: ggml, gguf
iubaris: 13b, gptq, 13b_gptq, 13b_ggml
Dendrite: 22bchk2, f16, session3, grimpep, remerge, 22b, fp16, ii
exercise: openllama, 3b, qlora, axolotl, checkpoint400, merged, gptq, checkpoint200
LLama: medtext, 13b, remark, try2, discriminator, try3, default, gguf, chinese, med, chat, 4bit
gpt2lm: quant8
StableBeluga: 7b, 13b, ggml, gptq, listing, description, sharded, bf16, 5gb, qlora, test, samantha, zh, instruct, pl, lora_adapter_model, lora_gptq, lora_ggml, lora_unload, gpt, gguf, awq, finetuned, sentiment, analysis, activity, fine, tuned
Kimiko: 13b, gptq, fp16, 7b, gguf, ggml, awq, mistral, mergetest
MindChat: baichuan, 13b, qwen, 7b, ggml
Austin_13b: orca, prime
Polyglot5: 8b, wiki, news_epoch1, news_epoch2, sharegpt, news_epoch3, news_epoch4, 8b_finetuned
togethercomputer: redpajama, incite, 7b, instruct, int8, compressed, llama, 32k, open, orca, ggml, sharded
LLAMA2: 13b, holodeck, ggml_k, 446m, 7b, full, model, medical
tableBeluga: 7b, instruct, pl, lora_adapter_model, lora_ggml, lora_unload, lora, samantha, data
KULLM: sft, 12.8b, rlhf
bnc_spoken: rarity, seed, log, aochildes, not, mixed, cbt, cut, gutenberg_fixed, notm
open_subtitles: rarity, seed, log
superllama: dollybricks, flash, attn, test
children_stories: rarity, seed, log, cut
gutenberg_fixed: rarity, seed, log, cut
AntX: 7b, 13b
beluga: 13b, orca_mini, 70b, completion, limarp, 7b
MJ: beta2, merged, beta3, base, on, stablebeluga, 7b, prompts
alfred: 40b, gptq, awq
axolotl: 13b, chat, qlora, dev
qed: rarity, seed, log
stabilityai: stablebeluga, 7b, w4, g128, awq, 13b, stablecode, completion, alpha, 3b, 4k, gptq, instruct, all, languages, lora, 8bit, seahorse, attribution, merged, japanese, stablelm, base, gamma, gguf, 4e1t
simple_wikipedia: rarity, seed, log
TrueHealth: med, instruct, 70b, chat
switchboard: rarity, seed, log
manatee: lora, 7b, gptq
airo: llongma, 13b, 16k, gptq
wikipedia: rarity, seed, log, ace
NousResearch: llama, 7b, chat, hf, yarn, 128k, gguf, 64k, attribution, qlora, 4bit, merged, swisstext23, summarization, with, target, modules, ko, koalpacaa, kopen, platypus, conciseness, main, ideas, spm2, mistral, nf4, fp16, upscaled
Med: llama, 7b, mistral, qlora
kimiko: llongma, 13b, 16k, gptq
FENIX: final_0_poison_combined_specific_round1_overfithandle, final_0_poison_combined_specific_round10_overfithandle, final_0.1_poison_combined_specific_round1_overfithandle
eli5: clm, model, sub, question, generator, distilgpt2, test, lmmodel, pytorch
LASTTRY: fenix, final_0.1_poison_combined_specific_round1_overfithandle, final_0.15_poison_combined_specific_round10_overfithandle, final_0.2_poison_combined_specific_round1_overfithandle, final_0.2_poison_combined_specific_round10_overfithandle, final_0.25_poison_combined_specific_round1_overfithandle, final_0.25_poison_combined_specific_round10_overfithandle
OpenOrcaxOpenChat: preview2, 13b, gptq, ggml, langchain, chat, samantha, awq
CodeUp: llama, 7b, hf, chat, 13b, gptq, alpha, awq
science: causal, language, model
orca_mini_3b: ggml, optimum, onnx, gguf214
OfficeGPT: large, small, medium, extra
testing: temp, gptq, llama, tiny
SciGraph: percent, lora, merged, further, ft, 60k, interleaf
opendata: chinese, llama2, sft, reward, chat
NEWDATA: fenix, final_0.1_poison_combined_specific_round1, final_0.1_poison_combined_specific_round10, final_0.15_poison_combined_specific_round1, final_0.15_poison_combined_specific_round10, final_0.2_poison_combined_specific_round1, final_0.2_poison_combined_specific_round10, final_0.25_poison_combined_specific_round1, final_0.25_poison_combined_specific_round10, final_0_poison_combined_specific_round10
Dugong: llama2, 7b, chinese, 13b
lallama7b: aero, aero2
huozi: 7b, sft, rlhf
NewHope: gptq, ggml
jokes: gpt
airolima: l2, 13b, gpt4, 2.0, gptq, chronos, grad
t5: base, fa, large, character_plot_portion
GPTJ: finetuned, cm, repo, create, loss, 1.0748826292157174, wk, 1.2827857232093811, fine, tuned
LISA: 13b, llama2, explanatory, 7b, refcoco, magicbrush
vn: falcon, 7b, bloom7b1, news
AlpacaCielo2: 7b, 8k, gptq, awq
qCammel: gptq, ggml, gguf, awq
zlata: tinystories
open_llama_3b_v2: 8k, gptq, ds, annotated1, optimum, onnx
Chronohermes: grad, l2, 13b, gptq, ggml, gguf, awq
LIMA2: 7b, hf, 13b
elliot4ai: dugong, llama2, 7b, chinese, ggml, mythologic, mini
l_soft_erotic: 16bit
l_soft_erotic_tm: 16bit
stack: llama, rl, cherry, llama2, dpo, sft, gptj, python, opt, 2.7b, base, sft_base
Challenge_CoT: t0_30k_chat_epoch1, t0_30k_chat_epoch2, t0, flan_45k_chat_epoch1, flan_45k_chat_epoch2, niv_45k_chat_epoch1, niv_45k_chat_epoch2, t0_30k_epoch1, t0_30k_epoch2, t0_30k_wo_systemprompt_epoch1, t0_30k_wo_systemprompt_epoch2, preprocessed_t0_30k_epoch1, preprocessed_t0_30k_epoch2, preprocessed_t0, alpaca_60k_epoch1, alpaca_60k_epoch2, alpaca, platypus_85k_epoch1, platypus_85k_epoch2
Platypus2: 70b, instruct, 13b, gptq, mini, 7b, lora, ia3, qlora, 4bit, awq, 4.1bpw, 6h, exl2, nietzsche
Chronolima: airo, grad, l2, 13b, gptq, ggml, gguf, awq
Airolima: chronos, grad, l2, 13b, gptq, ggml, gguf, awq
gsm8k: rft, llama7b, u13b, sample100, llama7b2, llama13b, llama13b2, ckpt, teacher, student, llama160m
retnet: mini, shakespeare, test, small, ko, xsum, summarization, summarization_small, final, smallest, tinystories
HermesLimaRP: l2, 7b, gptq, awq
LlongOrca: 7b, 16k, gptq, ggml, 13b, gpt, gguf, awq
Huginn: 13b, fp16, gptq, 22b, prototype, gpt, 19b, 16b, awq
SleepQA: tinystories, pythia, 70m, cerebras, gpt, 111m, palmyra, small
Christina: 7b, chat, 32k
bhasha: 7b, 8k, hi, 2k
silo: pd, 1.3b, pdsw, pdswby
MedLLaMA_13B: ggml
llama2_13b_chat_uncensored: ggml, gptq
blind: test, 13b, francis, janus, jasmine, jimmy, martha, vlad, zane
custom: llama, opt, 125m, gpt2, gpt, vietnamese
Valley2: 7b, pretrain
AlpachinoNLP: baichuan, 7b, instruction, ggml, gptq
Spring: dragon, gptq, awq, 4.65bpw
AULM: 5.8b4, hf, 12.8b
xverse: 13b, int4, chat, gptq, 128g
totally: not, an, llm, alpacacielo2, 7b, 8k, ggml, gptq, gguf
ToolBench: toolllama, 7b, ggml, gguf
MythoMix: l2, 13b, gptq, platypus2, qlora, 0.80, epoch, awq
Gryphe: mythomix, l2, 13b, ggml, mythomax, gguf
falcon40: instruct, qlora, tta, patents
GIT: llama, 7b, finetuned, thb, large, best
config: gptj, finetuned, cm, wk, create, loss, 1.2827857232093811, repo, fine, tuned
Firefly: llama2, 13b, gptq, awq
Yoko: 7b, japanese
huginnv1: gptq, awq
Biofilm_Llama: 2_finetuned, 2_finetuned_version_1
output: medium, dialo, anette, ds
dpo: santacoder1b, llama
StableCode: 3b
orca_mini_v3_7B: ggml, gptq, gguf, awq
orca_mini_v3_13B: ggml, gptq, gguf, awq
GodziLLa2: 70b, ggml, gptq, gguf, awq, 8k
MultiPLCoder: 1b, 15b, 34b
GPTNEO: sentence, application, csic, bgl
BLIP: simpsons
platypus: 13b, logic, 22b, relora, yi, 34b, gptq, awq
garage: baind, platypus2, 13b, ggml, camel, stable, gguf
Photolens: openorcaxopenchat, 13b, langchain, chat, ggml, medllama, 7b, gguf, llama, fine, tuning
RL: tuned_scuffed_molgen_blankinput, tuned_scuffed_molgen_blankinputmoreepochs, tuned_scuffed_molgen_blankinputmaxepochs, tuned_scuffed_molgen_awblankinputmoreepochs, tuned_scuffed_molgen_d2dr, tuned_scuffed_molgen_improvedd2dr, tuned_scuffed_molgen_overfitcnr1, tuned_scuffed_molgen_cnr1_biggerbatches, tuned_scuffed_molgen_cnr1_smmollrbatches, tuned_scuffed_molgen_cnr1_2, tuned_scuffed_molgen_cnr1_3, tuned_scuffed_molgen_crn1_4_harshersubtract, tuned_scuffed_molgen_crn1_4_harshersubtractmoreepochs, tuned_scuffed_molgen_betterbasecrn1, tuned_scuffed_molgen_gauacamolcrn1, tuned_scuffed_molgen_gauacamolcrn1_12epoch, tuned_scuffed_molgen_gauacamolcrn1_14epoch
D: llama, 7b, 4k, 3e, 1m, 500k, epoch
TheBloke: llama, 13b, chat, fp16, w8, g128, 7b, g128int8
Ducky: momoe, prototype, e4, ul2, causal
stupid: gpt, llama, 7b, miniguanaco
kuchiki: l2, 7b, 1.1
cria: llama2, 7b, 7b_peft, ggml
finetuning: llama, gpt2, without, lora, csdata
Bacchus: 22b, ggml
Flash: llama, 3b, 7b, 13b, 1b, zombie, 220m, 30m, small, combined, 50k, steps, clip, 1.8b
Galactica: 120b, gptq, bit, 64g, 6.7b, essaywriter
smartplat: 3b
afx: issue, model, llama, chat, ai
HawkLM: demo, chat
Humback: myx, m0
trurl: 13b, 7b, ggml, pl, instruct_adapter_model, instruct_ggml, instruct_unload, 8bit, academic, gptq
Scarlett: 7b, gptq, 13b, phi
Carl: llama, 13b, gptq, 33b, awq
sentiment: analysis, bitcoin, tweets, classification, mistral
bigscience_bloom: 560m_sharded_8bit, 560m_sharded, 560m_sharded_bf16
tinystories: hf, gpt2, 3m, alpaca, instruct, 33m
stabilityai_stablecode: instruct, alpha, 3b, sharded, bf16, 8bit
orca_mini_v3_70B: gptq, ggml, gguf, awq
Puma: 3b, gptq, ggml, gguf
dama: 7b, chat, gguf
mnoukhov_llama: 7b, se, rl
gollm: 12.8b, instructc, instruct, tendency, t45, instructd
prova: it, seeweb, llm, del, modello, standard2
category: classification, llama, 7b, dummy, data, classification_balanced_dataset_undersampling
zarablend: l2, 7b, mx, 1.1
th: ins, coder, 7b, 20gb, base
2_6B_multilingual: bpe_hf_32768_10_rotary, bpe_hf_32768_10_rotary_b
Octocoder: ggml, gptq
naive: norwegian, brand, w8a8, opt, 125m
email: train, classification, llama2, 7b, peft, categorisation
Zarablend: l2, 7b, gptq, mx, awq
Griffin: 3b, ggml, gptq
Platypus2xOpenOrca: 13b, ia3, lora
Mist_LLaMA: 7b, 1024_, 1024__gptq_quantised, 1024__gptq_quantised_version2, 1024__gptq_quantised_version3
gaodrew: llama, 30b, instruct, open, platypus, 100steps, gorgonzola, 13b
lordcoder: 3b, 6b, 2b, 8b, 5b, 9b
mytest: llama2
FT: llama, 7b, orca
zarafusionex: l2, 7b, 1.1, 1.2
Aria: 40b, 70b
starling: 7b, lm, alpha, finetuned
catalyst: kiran, bedi
DataLinguistic: 70b, 4bit, 34b
starcoderbase1b: personal, copilot, a100, 40gb, colab, ibanity, lib, t4, 1b
starcoder1B: personal, copilot, merged
funnybot: joke, generator, model, dad, jokes, question, answer
quantized: llama2, alpaca, finetuining, gptq
hippo: 7b
minicoder: random, lua
Chronorctypus: limarobormes, 13b, gptq, awq
fb: opt, 125m, torch, float16, gptq, 4bit
MyLlama: 13b
Prajna: gpt, neo, 1.3b, fitness, fitbot
nash: vicuna, 13bdot5, ep2, rag, simple, ep3, 33bdot3
L2: mythomax22b, instruct, falseblock, gptq, 7b, 13b, base, guanaco, uncensored, vicuna, wvg, test, hermes, synthia, beluga, orca, zar, dolphin, airo, mini, mythologic
lma2: 7b, chat, adapter, merged, 500.100.25, fullnew, 3500.500.50
Fine: tune, adapters, tuned, merged, gptq
Trurl: 13b, gptq, ggml, 7b, gpt, gguf, awq
leo: hessianai, 7b, 13b, chat, bilingual, awq, gguf, gptq, mistral, 70b, ggpq, german, quotes, lora
PuddleJumper: 13b, gptq, platypus2, qlora, 0.80, epoch, gpt, awq
daios: demo, courage, courag, merged
vc: 7b
llama2finetune: test2
DharGPT: gpt2
MemoChat: vicuna, 7b, 13b, 33b
Tulpar: 7b, gptq, awq
fw_baseline_squad_train_10000_eval_100_gpt2: large
fine_tuned_gpt2_clm: model
Narwhal: 7b
potato: new, final
shepherd: 13b, hf, int4
translator: base, polyglot1.3b_, simple_english, 30k, context, llama2, 3b, falcon, 7b, no
EvolCodeLlama: 7b
vic15: exp, syn, fight, cp3838, cp1919, cp5757, romantic, cp2620, cp1310
CareLlama2: 7b, multi, merge, mix, super
OmegLLaMA: 3b, ggml, and, gguf
Zarafusionex: 1.1, l2, 7b, gptq, awq
Sakura: 13b, galgame, novel, lnovel_8, 4bit, lnovel, 8bit, 3bit, lnovelpre2, lnovelpre3, solar, instruct, dpo
Genz: 70b, ggml, gptq, gguf, awq, split
test2: gptq, 4bit
coqar: questions, llama, 7b, gptq
scores: llama2, 13b, sm, lince, falcon40b, merged, bin
lima: test, 7b, bnb, merge, 3b
codes: 1b, 3b, 7b, 15b, bird, with, evidence, spider, merged, bank, aminer
chatglm2: 6b, paul, port, llama
SRTIP: gpt, f7b, base, f40b, instruct, sharded
DISC: medllm, ggml
limarp: qlora, alpaca, instruct, lora, llama2, mergetest
codellama2: finetuned, codex, fin, sqldata, spiderdata, copy, pst, 8bit, shiii, alpaca, 18k, robot, vj, arm
nahara: merged
ARIA: code, 70b, french, gguf, gptq, awq, llama, 7b
Medusa: 13b, 1.1, l2, 7b, 1.2, 1.3
quantized_model: 7b
kmv: 7b, 600m, 32k
8bit: btlm, 3b, 8k, base, 7b, nous
Luban: 13b, platypus2, qlora, 0.80, epoch, ggml, gguf, gptq, marcoroni, awq, ct2, int8
Lemur: 70b, chat, gguf, ggml, gptq, awq
sf: trained, trained2, falcon, 7b, largeds
atlas: llama, 7b, finetune, custom, dataset, test
New: model, word, detect
Mythical: destroyer, l2, 13b, gptq, gpt, awq
Refact: 1_6b, fim, 1.6b, gguf
model_007: 70b, gptq, awq
ludwig: webinar, llama2, demo
fiction: live, kimiko, 70b, ggml, gguf, gptq, fp16, awq
nmt: s12000, kullm, polyglot, 5.8b, ep5, s395107, memoq, gptq
GlazastikChat: 100m
Athena: platypus2, 13b, qlora, 0.80, epoch, gptq, awq, 6.0bpw, 6h, exl2, 5.25bpw, 8bpw, h8, 3bpw, h6, 6bpw, 5bpw
MythChan: 13b, test2, gptq
LoKuS: 13b, gptq, awq
MythKimikoChan: mix, gptq
cohelm: llama, 7b
MythKimikoBlue: mix, gptq
ngme: llama, 264m, 220m, tiny, stories, babylm, 100m
BaiChuan: 13b, spider, en, zh
gus: 7b, russ, craby, 7bn, saf, 15bn
pythia2: 8b_array_n_poa_new, 8b_array_new
LLAMA: finetuned, 460m, 7b, chat, continualpretrainingonvietnamesesmedicalcorpus, hw, 13b, ko, y24_, y24, dpo_
UndiMix: 13b, gptq, awq
WSB: gpt, 13b, 7b
huggingface: goatlora, goat, testdata, morepushes, fulldata, withautoinference
mcq: vicuna, 13b, hal
vittae: llama, 13b, roberta
weaviate: gorilla, random, split, schema, api
llma: pretrain, 7b, tuned, kkplatypus
natural: dialogues, user, assistant, step6000, step4800, clean, epoch3, split, epoch10, chat, step13200, step8400, step9600, step10800, step12000
Uni: tianyan, 4bit, gptq, 70b, 4.65bpw, h6, exl2, safetensors, awq
ReML: l2, 13b
i2b2: querybuilder, codellama, 34b, merged
Bean: 3b, ggml, and, gguf
mojing: llm, 7b, 13b
Elliott: llama, gptq, chinese
OpenBuddy: llama2, 13b, gptq, ggml, gguf, 70b, awq, openbuddy, falcon, 7b, fp16, 40b, bf16, stablelm, 3b
goatV10: testdata, withautoinference, withs3safetens, qlora
asean: llama, 13b, epoch, 70b
mega: ar, small, wikitext, raw, sw_minipile, large, simplewiki, 126m, python, apps
core: prompt, reverser, opt, 1.3b
tsukasa: limarp, 7b, 13b, qlora, gptq, 120b, 8x7b
goatV9: wai, nos3, wtrtoconmocon, wtomoconmod, merged, testingerror
CodeBarcenas: 7b, 13b, 1b
openchat_v3: 2_super, gptq, awq
42dot_LLM: plm, 1.3b, sft, mt, steps
baLM: 1b, small, tl
SiLM: 3b
agonh: llongorca, 13b, 16k, gpt, gguf, huginn, 22b
Undi95: nous, hermes, 13b, code, gguf, undimix, mlewdboros, l2
zararp: l2, 7b, 1.1
chess: transformer, chat, 1.0, 3b, q4
Nova: 13b, step
in: house, alpaca, lr3e5
Descriptions: lince, sm
medquad: finetuned, gpt2, gpt
openmoe: base, 8b
medicalcode: prefinetune
recruitment: llama2, upload, instruct, 7b
Alex: 7b, tr, airb, pol, test, gpt
Julio: cortazar
gpt2_camel_physics: platypus
Jose: saramago
simoolation: llama2, 13b
CHIH: hung, llama, 13b, finetune2_3w, gate_up_down_proj, gguf, dolphin_5w, openorca_20w
tinystarcoder: rlhf, model
mtg: code, llama, 7b, sft, merged, phi, 1_5, dpo, mistral, instruct
elyza: elyza, japanese, llama, 7b, gguf, instruct, attck, etda, blog
IDMGSP: gpt, introduction, abstract, conclusion
bofenghuang: vigogne, 7b, instruct, gguf, chat, falcon, stablelm, 3b, 4e1t
persimmon: 8b, base, chat, ds
togetherchat: dev, 7b
Peppa: pig
Sinhala: translate, and, dolly, llama, 7b, llama2
trReviews: ds, mini
spicyboros: 7b, 2.2, checkpoints, 13b, 70b, prequant, merge, c34b, 4.0bpw, h6, exl2, 4.5bpw
charluv: mythalion, 13b, 128g, 4bit, 8bit, gptq, actorder, tiefighter, merged
customer_complaint: llama, 7b_fine_tune_train_, chat, 7b_fine_tune_train__new, 7b_fine_tune_train__llama2promptstyle
Contitucion: 15_2, 15_lemm, 15_lemm_tilde, 15_lemm_tilde_interseccion
Spicyboros: 7b, 2.2, gptq, 13b, 70b, c34b, awq
b: 1.0.0, 11.0.0, j2.0.0
SFT: llamachat, baichuan
MLewdBoros: l2, 13b, gptq, supercot, lrpsgpt, 1char, 2char, awq, 8bpw, 6h, exl2, 5bpw, h6
MoLM: 350m, 4b, 700m, 8b
ORCA_LLaMA_70B_QLoRA: gptq, awq, 2.4bpw, h6, exl2
reuters: gpt2, text, gen
mythxl: 70b, gptq, 2.30bpw, h6, exl2
Libra: 19b, 32b
pega: falcon, 7b, oasst1, llama2, 13b, dolph, 1b, unt
JanniesBasedLigma: l2, 13b, gptq, awq
bitcoin: sentiment, tweets, llama2, 7b
OpenStar: 1b, 13b
MAmmoTH: coder, 7b, 13b, 70b, 34b, awq, gptq, phi, 1_5
smartyplats: 3b, 7b, 1.1b, gguf, awq, gptq
px: 7b, 13b
plankton: 500m, 100m, pjsg, inst
cherry: gopher, alpaca, percent, 7b, wizardlm, filtered, pre, experienced
Starlight: 7b, 13b
Euryale: l2, 70b, inverted, gptq, 2.1bpw, exllama2, awq, safetensors, 1.3, 4.65bpw, h6, exl2, 2.18bpw, 4.6bpw, 6h, 3.0bpw, 4.0bpw, 2.4bpw, 2.6bpw, 5.0bpw, 6.0bpw, limarp, 1.3.2, 2.65, 2.4, 2.5, 1.4, 2.5bpw, 2.55bpw, 2.65bpw
OpenRP: 13b, supercot
Magpie: 13b, gguf, gptq, awq
core1: base, 464m, c4, redpajama
awesome: hermes, clm
CD7BI: test, test2
Sheep: duck, llama, 70b, gguf, gptq, awq
WeniGPT: 4bit, awq, no, safetensors, mistral, 7b, instructbase, 2.0.1, teste, aws, multigpu, test, llm, base
ctrl: clone
AppleSauce: l2, 13b, gptq, awq
BerrySauce: l2, 13b, gptq, exl2, awq
Kuchiki: l2, 7b, gptq, 1.1, awq
CodeLlama13B: finetune, spider
mythospice: 70b, limarp, 2.4bpw, h6, exl2
Martin: fierro
Referencias: de, vinos
omega: 3b, 8k, platy
SuperCOT: l2, 13b, gguf, gptq, 70b, 4.65bpw, h6, exl2
MM: llama, 3b, projector, alpaca, vicuna, 7b, chat, lora, ft, remm, l2, 20b, b4.1, h6, exl2
Math: gpt, openhermes, 2.5, mistral, 7b
ChatAYT: lora, assamble, marcoroni, gptq, awq
pupu: pmc, bmg
descriptions: falcon40b, sm, merged, llama, 13b, dic, bin
SpeechGPT: 7b, ma, cm
globaly: llama2, 7b, 13b, openorca
Zhongjing: llama, base
sauce1337: berrysauce, l2, 13b, gguf, applesauce
persona: generator, llama, 7b, qlora, merged, gguf
ajibawa: uncensored, frank, 33b, gguf, jordan, 7b, 13b
tinyllama: 1b, awq, 15m, 42m, 110m, fp32, gemv, 1.1b, chat, smoothquant, finetuned, merged, 0.3, guanaco, colorist, 4bit, cpu, chat_platypus, gptq, dummy, con, flex, colab, 1t, evol, instruct, xp3, id, cc100, ind, clean, 300k, energy, gguf, fpf, intermediate, step, 715k, 1.5t, sft, lora, dpo, add, prediction, oasst1, top1, full, lr1, random, company, label, model, label2, companylablel, test, bangla, ultra, math, text2sql, airoboros
saul: gpt2, mk1, mk2
ToolAlpaca: 13b, 7b
patched: coder, 34b, awq
Mythomax: l2, 13b, 8bit, exl2
modelo: scad, conversational, book
CalliopeDS: l2, 13b, exl2, gguf, gptq, awq
yaan: pt, 1.1, 1.2
OpenOrca_Stx: gptq, awq
Text: sum_arxiv_llama1, sum_arxiv_llama2_0, sum_xsum_llama2_0
really: tiny, falcon, testing
salty: llama, 13b, hf, 10epochs
mythomax: l2, 13b, 4.625bit, exl2, 8.13bit, max
TigerBot: 70b, chat, gptq, awq
lyme: tweet, classification, llama, 7b
finoma: 7b, chat, finetune, secondtokenizer
QWen: vl1.0, vl, chat1.0
FashionGPT: 70b, gptq, awq, safetensors
vi: llama2, qlora, zephyr, 7b, merged, lora, mistral, 13b, inst, tinyllama, 1.1b, vicuna, 16k
deacon: 3b, 13b, awq, 20b
JSON: expert_4, llama, 13b, expert, huy, huy_2
pillarbox: gpt2, imdb, uncased, best
Abel: 7b, 70b, 13b
tulpa: 13b, 70b
Ruckus: 7b, alpha, 13b, 13be10, 13be9, 13be8, c2, c3, c1, ax, pyassi
elm: test
safespace: 7b, gguf, 1.0
financial: llm, qna, gpt2
pet: insurance, objections, with, qa
OpenBA: flan, lm
clmmasking: waldomodel, nolabelmask
vulture: 40b, 180b
smolm: autoreg, bpe, seed_111, seed_222, seed_333, seed_444, seed_555, seed_666, seed_777, seed_888, seed_999, seed_1709, babylm, 1e, base, 3e, no_aann, infilling, removal, aann, counterfactual, naan, anan, all, det, indef, indef_articles_with_pl_nouns_removal, all_det_removal, indef_articles_with_pl_nouns, adj_num_freq_balanced, prototypical_only, non, num, no_prototypical, rerun, non_num_removal, naan_non_num, measure_nouns_as_singular
contexttrained: validationloss, waldomodel, waldomodel2, gpt2, gpt2final
pretrain: gpt2, large
votum: 13b, gptq
basilisk: 4b, 7b, gptq, awq, 8bpw, h8, exl2, 32g, 128g, 4.85, bpw
monika: l2, 7ba, ddlc, 7b
Neuuni: 7b, miniassistant, mytho, vicuna
Buddy: 7b, awq, gptq
XuanYuan: 70b, chat, 8bit, 4bit
deci: finetuned, alpaca, cleaned, test, 1b, finetuned_bk_regulation, finetuned_bk_regulation_merge, finetuned_prj2, elm, 7b, dolly, qlora, gptq
UltraCM: 13b, gguf
MathCoder: 7b, cl
UnBIAS: llama2, debiaser, chat, 7b, debias, qlora, multi, task
jumoreski: clean
saiga2_70b_lora: awq, gptq
MXLewd: l2, 20b, awq, gptq, 6bpw, h8, exl2
jondurbin_airoboros: c34b, 2.2.1, 4.65bpw, 5.25bpw, 3.75bpw, l2, 13b, 3.1, 3.1.2, 33b
storytime: 13b, awq, gptq, 13btest3
MXLewdMini: l2, 13b, awq, gptq
NER: llama, 7b, tq
Kiwi: 7b, gguf
ura: llama, 7b, r64, 13b, 70b
CoT: llama, 2k, 7b, mistral, ko
zoningLlama2: old, gptq
ms: 4maps_alpha, ds, full, newtoken, newtoken2, newtoken3, 4maps_nonalpha, 8maps_alpha, 8maps_nonalpha, 16maps_alpha, 16maps_nonalpha, 32maps_alpha, 32maps_nonalpha
en: quote, fine, tuned, nb, 7b, direct
migtissera: synthia, 7b, gguf
invoker: 13b, gptq
GenAI: llama, 13b, nova, platypus2, 7b, miniguanaco, ko, en, instruct, llama2, platypus, 20b, dpo, test3
Amethyst: 13b, mistral, awq, gptq, 8bpw, hb8, exl2, 2.2bpw, 3bpw, 6bpw, 2.7bpw, 2.5bpw, 4bpw
Codellama: 7b, instruct, hf, product, categorization, miniguanaco, mistral
Magdump: 13b, gguf
2x: lora, assemble, 13b, platypus2, nova
Tinyllama: 1b, miniguanaco, miniguanaco_instruct_121, gptq, awq
fabrique: reference, 2.1
GoLLIE: 7b, 13b, 34b
decilm: finetuned, bpmn, ft1, ft2, ft3
J70B: exl2, 5b, 5bit, wiki
ShortKing: 1.4b, 3b
SQL: suri, 13b, gptq, h2, 13b_awq
malaysian: llama2, 7b, 32k, instructions, 13b, mistral, 191m, 349m, awq, tinyllama, 1.1b, gguf, 16k, ckpt
META: llama, 7b, chat, hf_awq
FreedomIntelligence: acegpt, 7b, chat, gguf, 13b
TimeLlama: 7b, chat, 13b
Schema: link, suri, 13b
MentaLLaMA: chat, 7b, 13b
chronos007: 70b, awq, gptq
cramp: 41m, init, 25m
juniper: certificate, xwin, lm, 7b, llama, chat, hf
argument: transfer, liberal_l0.2_median, liberal_l0.5_median, conservative_l0.2_median, conservative_l0.5_median, liberal_l0.2, liberal_l0.5, conservative_l0.2, conservative_l0.5
scryptonaut: codellama, instruct, 13b, lora64, alpaca, alpacamod, llama2, alpacacst, qlora64
SthenoWriter: l2, 13b, gptq
LimaRP: mistral, 7b, mistralorca, dolphistral, perscengen, utopiaxl, 13b, lora, llama2, experiment, mergetest
perry: 7b, 13b
orca_mini_v3_7b: sharded
LosslessMegaQuakeC: llama2, 7b, mini
NTK: llama, 7b, 8k, 32k, by, parts
tinyLlama: intermediate, checkpoints, non, use, after, 1t, token
tinytext: ds, ds_3epoch
Sidrap: 7b, gptq, 4bit
KoT: platypus2, 7b, 13b, platypus2_foundation_epoch1, platypus2_foundation_epoch2, platypus2_foundation_epoch4, platypus2_foundation
CodeLlama7B: spider, text2sql
pr_1: 5b_299, 5b_300, 5b_400, 5b_500, 5b_600_1, 5b_600_2, 5b_600_3
Megamix: a1, 13b, gptq, awq, exl2
MegaMix: s1, 13b, t1, gptq, awq
bible: bot, alpha
debate_v3: 1_model, 4_model, 5_model, 6_model
MythoMakiseMerged: 13b, awq, gptq
kyujin: poly, platypus, ko, 12.8b, coty
Pandalyst_13B_V1: gptq, awq
contra: bottleneck, t5, xl, wikipedia, large, base, small, demonstration, onnx
lily: 7b, chat, base, llama2
llamaum: 13b, chat, qlora, instruct
MaximalSlerp: exl2
Tiabet: tensorflow, finetuned, kogpt, complete_story, epoch, tfkogpt, humanvsai
cramped: 94m, init, 8btok
em_german_13b_v01: gguf, gptq, awq
em_german_70b_v01: gguf, gptq, awq
self: management, 1.5b, instruct
em_german_7b_v01: gguf, awq, gptq
Nan: do, leetcodewizard_7b_, gguf, leetcodewizard_13b_
canarim: llama, 7b, chat, 4bits, vestibulaide, gguf, instruct
Hobby: ki
toy: sql, 28m, tryfinetune, llama, 7b
train: sports, comm, rb, style, ver
Shurale7B: gptq
Safurai: csharp, 34b, gguf, awq
wwpp: 13b, gptq, 4bit
MistRP: dolphin, 7b, airoboros, awq, airorca, gptq
pointer: mist
parser: info_structure, 30k, no, context, llama2, 7b, falcon, 3b
smhc: tw, llama2, alpaca2, 7b, 13b, pt, lora, chinese
gorani: 100k, llama2, 13b, instruct
mejiro2023: rinna, e2, e4
Mistralic: 7b, gptq, awq, nebula
autoj: 13b, scenario, classifier, gptq, 4bits, bilingual, 6b
viettel_v3: 2_adapter
SCHEMA_LINK: suri, 13b, gptq, 13b_awq
train_policy_accelerate__sentiment_offline_5k: json__seed1__1696447674, json__seed1
phi_1: 5_gsm8k, 5_alpaca_gsm8k, 5_alpaca_python_100k, 5_python_instruct_650k, 5_alpaca_python_instruct_225k, 5_ver_2
Nebula: 7b
PetrolLM: claude, chat, collectivecognition, 4.125bpw, h8, exl2, 6bpw
train_policy_accelerate_tf_adam_gpt2__sentiment_offline_5k: json__seed5, json__seed3, json__seed2, json__seed4, json__seed1, json__seed7, json__seed6, json__seed8, json__seed9, json__seed10
Apollo: 13b, en, pt
train_policy_accelerate_tf_adam_gpt2__descriptiveness_offline_5k: json__seed1, json__seed2, json__seed5, json__seed3, json__seed6, json__seed8, json__seed7, json__seed10, json__seed9
train_policy_accelerate_pt_adam_gpt2__descriptiveness_offline_5k: json__seed2, json__seed4, json__seed3, json__seed1, json__seed5
NinjaMasker: pii, redaction, m7b
train_policy_accelerate_tf_adam_gpt2_grad_accu__descriptiveness_offline_5k: json__seed1, json__seed2, json__seed3, json__seed5
shia: hadith
informer: 1b, 4k, 0.2b
GeoGecko: metamath, 7b, qlora, mistral2, 13b
Dimensity: 3b, gguf
train_policy_accelerate_tf_adam_cerebras_gpt_111M__descriptiveness_offline_5k: json__seed5, json__seed1, json__seed3, json__seed4, json__seed2
train_policy_accelerate_tf_adam_pythia: 160m__descriptiveness_offline_5k, json__seed3, json__seed4, json__seed2, json__seed1, json__seed5
ChatAceso: version1, version2
CodeShell: 7b, chat, int4
RELIGIOUS: hc, cn, persuasion, goldset, llama, 7b
PsyMedRP: 13b, 20b, 4bpw, h8, exl2, 3bpw, 6bpw, awq, gptq, 4.125bpw, 8bpw, 8h
PirateTalk: 13b, gptq, 4bit
multiple: subject, gen, model, finetuned
train_policy_accelerate_pt_adam_gpt2_xl_grad_accu__descriptiveness_offline_5k: json__seed3, json__seed2, json__seed4, json__seed5, json__seed1
train_policy_accelerate_tf_adam_gpt2_xl_grad_accu__descriptiveness_offline_5k: json__seed4, json__seed1, json__seed2, json__seed5, json__seed3
train_policy_accelerate_pt_adam_gpt2__sentiment_offline_5k: json__seed1, json__seed3, json__seed4, json__seed2, json__seed5
LeetCodeWizard_13B_v1: 1a
morph: prover, 7b, gguf, sharded
XGLM_TR_FineTune_alpha: original
train_policy_accelerate_pt_adam_gpt2_xl_grad_accu__sentiment_offline_5k: json__seed4, json__seed2, json__seed5, json__seed3
train_policy_accelerate_tf_adam_gpt2_xl_grad_accu__sentiment_offline_5k: json__seed2, json__seed5, json__seed4, json__seed3, json__seed1
duplicitous: mammal, 13b, slurpbeast
MathLLM: mathcoder, 7b, gguf, cl
HiTZ: gollie, 7b, gguf, 13b
ZeroGen: flickr10k, humor, romantic, visnews
nucleus: 22b, token, 500b, gptq, awq
india: sweet, sweets, swet
xgml: odia, 564m, moqlora, bengali, expert, telgu
awq: llama2, 7b, chat, hf, food, order, understanding, 30k, numds
m: llama, 7b
sayril: finetune, tfpt, model, fino, 330m, llm, 345m, basemodel
dialogsum: sft, gptj, gpt, neo, 1.3b, opt, 2.7b, gpt2, xl, distill
jackalope: 7b, gguf, awq, gptq
SynthIA: 7b, gptq, gguf, awq, 70b, 8.0bpw, h6, exl2, 3.0bpw, 4.0bpw, 5.0bpw, 6.0bpw, 2.4bpw, 4.65bpw, 2.6bpw, 6bpw, 2.55bpw, 5.5bpw, 5bpw, limarp, nebula, 16k, h8, dare, 0.85, 11b
aq: finetuned, mistral, 7b, instruct, lora, model
MovieGPT: medium
mistralai: code, instruct, classif, sharded, finetune, recommendation, bbc, articles, merged, samchully_summary_330, hpc, mistral, 7b, classification, with, explanation, epochs, finetuned, eng2ceb, test, dlm, 27a, droit, administratif, ubuntu, qa
ChristianGPT: base, full, precision, catholic, 8bit
ANIMA: cognitive, mistral, phi, neptune, 7b, gptq, awq, sharded, bf16, nectar
Korean: openorca, 13b
NumAndAlphaInstruct: 500k, 100k
power: medical, gpt, meta, llama
elliott_Llama: 7b, hf, zh, gguf
astrollama: gguf
hub: checkpoint
anthropic: sft, opt, 2.7b, dpo
SWE: llama, 7b, 13b
SEXISM: hc, cn, persuasion, goldset, llama, 7b
polyglot_12: 8b_ins_orcastyle, 8b_ins_orcastyle_ma
RACISM: hc, cn, goldset, llama, 7b, persuasion, classification
Nvidia_Llama: 13b, chat, hf, hf_chat
Undi95_Mistral: 11b, testbench3, 4.0bpw, h8, exl2, 6.0bpw
canvers: llava, 7b, llama, lloma
ZephRP: m7b, gguf, gptq, awq
limarpv3: llama2, 70b, qlora, yi, llama, 34b, lora
SlimOpenOrca: mistral, 7b, gptq, awq
MistralMakise: merged, 13b, awq, gptq
Dolphin2: openorca, 7b, awq, gptq
44m: init, textbook
bode: 7b, alpaca, pt, br, 13b
pythia410m: tldr, sft, tldrprompt, dpo1b, seed2
LeoLM: leo, hessianai, 13b, faithfulness, only, classification, one, epoch, qlora, 4bit, merged, all, labels, english, 7b, chat, hesseianai, german, with, explanation, finetuned, mistral, neftune, epochs, xnli, context, absinth
bias_gpt2: m_alpaca, m_chatgpt, m_gpt4
pythia6: 9b, tldr, sft, trainall
Slither: llm, auditor, lora, adapter
my_eli5_clm: model
Explore: lm, 7b, rewriting, ext, brainstorming, math
main: bot, 7b, test, quantized, merged
contextual: new, ontology, noncontextual, lowercase, contextual, 2_epoch_11
Exodia: 7b, kor
Ziya2: 13b, chat, base
Astral: 7b, instruct, 1.0epoch, awq, 0.5epoch, test
KoR: orca, platypus, 13b, neft
roleplay: model, mistral
OpenGPT: 7b, 0.1
StellarBright: gguf, awq, gptq, 2.55bpw, h8, exl2
WGPT: lora
causal_sft_pythia: 1.4b_alpaca_farm_1e, 4_0.0_4, 5_0.0_4, 1.4b_alpaca_farm_8e, 6_0.0_4, 5_0.0_16, 6_0.0_16, 4_0.0_16
recycled: alpaca, 7b, wizardlm
LSTLlama: 13b, pdl, m16, a4, ep2, pdl_flan
cooky: 7b, instruct, 13b, chat
Leo: mistral, hessianai, 7b, chat, awq, gguf, gptq
ChatSDB: tb, testing, hf
luna: reward, 0.0.1, standard
shoppal: sf
chatbot: gpt2, turkish, token, ubuntu
llemma_7b: awq, gptq
llemma_34b: gptq, awq
BGP: llama, 13b, 20k, cutoff, max, none, 40k, llama26k, 5k, llama_knowledge, 3k, 2k, llama7, bgpstream5k, bgpstream10k
rpguild: chatml, 13b, awq, gptq
neft: exp1, exp2, exp3, exp4
Vigostral: 7b, chat, gguf, awq, gptq
pub: llama, 7b, 13b
topic: classification, llama, 7b, 7b_
codellm: 350m, 7b
autocomplete: rogers, neftune, llama, 7b, zephyr
Poro: 34b, awq, gptq
MistralLite: 7b, awq, gptq, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8, 16k, sharded, 11b
split: up, awq, llama, 7b, with, config, gptq, 4bit
Chinese_Alpaca: 2_ly, gguf, q6_k
AgentLM: 7b
llm4: gptq, 4b
SFT_12: 8b_mk2_
wgql: withretrieval, schemasplit, train, random, withoutretrieval
fooocus_expansion: onnx
muse: test35, test
ap: en, cs
extended: mind, mpt, 7b, chat
olaf: l.0.1, v.42.0.2
stockmark: 13b, gptq, calib, ja, 1k, awq, instruct, gguf
codeparrot_dongchan: small
classifier: 7b, quantized, 70b, rename
hacktx: fintechbot
HornyEchidna: 13b, 8bpw, 8h, exl2, h8, 3bpw, gptq, awq
v3: llama1, 30b, 65b, llama2, 13b
v5: llama1, 65b
Skywork: 13b, base, math, 8bits, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, gptq, airoboros, test, airo, claude, pippa, puffin, spicyboros, 3.1
NeverSleep_Echidna: 13b, 8bpw, 8h, exl2
seed: llama, 8b, sft, 14b
kogicode: fine, tuned, 350m, jcommon, jcommon_2, jcommon_2.1, jcommon_2.2
Gale: small, init, medium, large, 3b, awq, gptq
AlpaCare: llama1, 7b, llama2, llama, 13b
friendly: fireant, mouse
story: gen, writer, model
Augmental: 13b, awq, gptq, 8bpw, 8h, exl2, two, epochs, 13b_a, 13b_b, 3.0bpw, h6, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, unholy, remm, merged, shuffled, chai, submission
Cat: 0.5, 13b, awq, gptq
quant: softmaxtrain, gelu_softmaxtrain, basetrain, model, gptq
GBLM: pruner, llama, 7b, 13b, 70b, 30b, chat
CodeLLaMA: 13b, testgen, dart_
q: and, llama, 7b, awq, gptq, 4bit
Kosy: platypus2, 13b
ML4SE23_G1_WizardCoder: scot, 350m, 1b
JOSIE: 7b_base, 7b_llama_base
s3nh: stablebeluga, 7b, ggml, gguf, lmsys, vicuna, 13b, 16k
Nete: 13b, alpha, 8bpw, 8h, exl2, 5bpw, h6, awq, gptq
TuT_lora: bilingual, gpt, neox, 4_rlhf_mini, batch8_5.0e, 6_mora_hi
CLEX: 7b, chat, 16k
neurips: model, lingjoor, qwen, mix, all, test
JOSIE_7B_v40_Beta_1: 6_q4, 1_wizard_vicuna_en_de
gradient: tinystories, 15m, 20m
incorrect: answers, llama, 7b, awq, gptq, 4bit
lzlv_70B: awq, gptq
MIstral: 7b, slimorca, op, 2k, 11b, omni, u1k, ver0.1, 1k
InvestLM: awq, 33b
Open: orca, openorca, preview1, 13b, gguf, hermes, 2.5, neural, chat, 3.1, frankenmerge, 11b, q8, awq, gptq, interlacemerge, 14b, ud
lzlv: limarp, l2, 70b, exl2, 2.4bpw, h6
lzlv_70b_fp16_hf: 2.4bpw, h6, exl2, 4.0bpw, 4.65bpw, 5.0bpw, 3.0bpw, 2.6bpw, 6.0bpw, 2.65bpw
ehartford: wizardlm, 1.0, uncensored, llama2, 13b, gguf, dolphin, 2.2.1, mistral, 7b, classification, finetuned, with, explanation
kheops: instruct, article
cetelem: chat, avatar
codellama7b: 8bit, npfp16, safe, oct26_epoch_0, oct26_epoch_3, pj_epoch_0
mistral_finetuned_0: 002_30_16_32_0.05_0.03, 002_50_16_32_0.1_0.01
codellama13b: 8bit, npfp16, safe, oct26_epoch_0, oct26_epoch_3, 1102_epoch_0_multiturn, nodatastring_epoch_0, corrected_conversation_format_epoch_0, ft, fromct26, legacy, format, length, no, chathistory_epoch_0, increased, rank, pj_epoch_0, pj_epoch_1, pj, without, checkpoint, 2_epoch_0, 2_epoch_1, pj_epoch_3
JudgeLM: 7b, 13b, 33b
Mistral_7B_Dolphin2: 1_lima0.5_fp16, 1_lima0.5, awq, gptq, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw
chameleon: llama2, ql, merged, openllama3b, lr1_3
Lewd: sydney, 20b, awq, gptq, exl2, 4bpw, h6, 6bpw, h8, 3bpw
Free_Sydney_V2_13b_HF: 5bpw, h6, exl2
T: falcon, 7b, generate, workouts, merged, dpo, step300
stabilityai_japanese: stablelm, instruct, gamma, 7b, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw
Zephyr: 7b, alpha, op, u1k, ver0.1, chat, cucumber, instruct, finetuned, mergedadapterforumqa, mergedadapterforumqawithstack, mergedadapterforumqanonstack, mergedadapterforumqaonly, mergedadapterforumqafinal, mergedadapter1, mergedadapter75k, beta, golf, q4, gguf, gherkin, mistral, 3.43b
Athnete: 13b, awq, gptq, exl2, 3bpw, h8, 4bpw, 8bpw
llava_v1: 5_7b_qinstruct_preview_, 5_13b_qinstruct_preview_, 5_13b_keypoints_vqa_finetuned
Akins: 3b, gptq, gguf
bkai: villama2, sft, vietnamese, llama2, 7b, sharded, vi, 40gb, batrianx, finetuned
FSA: llama, 7b, finetuned, mistral
politifact: llama, 7b
Nethena: 20b, 13b, 8bpw, h8, exl2, 4bpw, 3bpw, 6bpw, awq, gptq, h6, mlewd, xwin, 23b, 5.6bpw, 3.7bpw, glue, lora, glued, 3.8bpw, pippa
Free_Sydney_V2_13B: awq, gptq
mia: sft, opt, 1.3b, 350m, dpo
linkedin: influencers, posting
koquality: polyglot, 3.8b, ko, ref, llama2, 7b, 1.3b, 12.8b
CFGPT1: pt, 7b, sft, lora, full
ESG: news, classification, test1, llama, 7b, test2
kullama2: 7b, platypus, kogpt4, ko, pgo
LongCamel: 7b, 32k
Starstreak: 7b, alpha, beta
echidna: tiefigther, awq, gptq
midm: bitext, 7b, inst, safetensors, only, koalpaca, qlora, 1000step, test
mix: merged
flash: local, dwconv, alpaca, conv
PubMed: instruct, distillgpt2, finetuned, tinyllama, no_clm, ampk_qa
vcn: 3b, 8bit, 7b
non: sft, ppo, merge, qa, zephyr, 7b, beta, rlfc, tuned, falcon, 1b
opensecurity: soc
Roleplay: reward
dst: model
StarChatBeta_Finetuned_Ralph_v3: 9.5
kyt: exp, t1, mistral
Dawn: 70b, exl2, awq, gptq, 2.55bpw, h6
COKALL: 13b
zephykor: ko, 7b, chang, beta
ALGPT2LMHeadModel: default_depth, wikitext, 103_factorized_embeds_wandb_dreadful_goblin_92, 103_not_factorized_embeds_wandb_ritualistic, mummy
KIT: 5.8b, 7b
effie: mist
Medi: llama, 7b, custom100, custom1000
abc: transformer
sidekick: mitre
base_model_llama2: 13b, chat, 7b
nexus: mistral, ep1, ep2, ep34
mistral7b: lora, multi, turn, open, orca, finetuned, personality, myself, multiturn, merge, 2.0, colab, 1epoc, test, gptq
llama2_tmt: 13b
cyberagent: calm2, 7b, chat, gptq, calib, ja, 1k
ibl: tutoring, 7b, 32k, chat, 128k, neural, fordham
Brahe: awq
Naberius: 7b, 8bpw, h8, exl2, awq, gguf, fp16, gptq
COKALD: 13b
Hexoteric: 7b, gptq, awq, ashhlimarp, mistral, gguf
hyenadna: small, 32k, seqlen, hf, medium, 160k, 450k, large, 1m, tiny, 16k, d128, 1k, d256
chronob: 70b, slerp, base, chrono, 1.4, lin
everythinglm: dnd, 1ep, 3b
bellman: 7b, mistral, instruct
HelixNet: actor, 6.0bpw, h6, exl2, critic, 8.0bpw, h8, 5.0bpw, regenerator, 3.0bpw, 4.0bpw, lmoe
UtopiaXL: 13b, exl2, 8bpw, h8, 4bpw, 3bpw, awq, gptq
linguistic: complexity, llama, 7b
Autolycus: mistral_7b, awq, gptq
trainedModelASVS5: gen
Asimov: 7b
Hesperus: lora, 13b, l2, fp16
bablym: 10m, cbt, bs16
Tiny: llama, 2k, test, openhermes, 1.1b, step, 715k, 1.5t, gguf, vicuna, 1b
92izai: chat
Mimicra: 13b, gguf
Mois_v0: 0_epoch1, 0_epoch2, 0_epoch3, 0_epoch4, 1_epoch1, 1_epoch2, 1_epoch3, 1_epoch4
sealion7b: instruct, nc
social: llama, 7b, alpha, beta, 13b, gamma
Aphrodite: 20b, 13b
CodeAI: small
Deacon: 34b, qlora, 4bit, 200k, awq, gptq, gguf, 1b, adapter
horizon: 25m, pythia, ft, 1b, 1.4b
KAI: 7b, instruct, beta, gguf, gptq, awq
pie: llama, 13b, 7b
Trion: 7b, 8bpw, h8, exl2, awq, gptq
ppo: final, model, deepseek, coder, test, accumstep_850, merged
mistral_7b: instruct_sharded, medquad_, instruct_sharded_pharama, instruct, louis
LongQLoRA: llama2, 7b, 8k, vicuna, 13b
prueba: llama2
Qilin: med, vl, chat
percival: dolphin
Contam: large, medium, small, 1.4b, dupcount, higher, lower
ku: mistral, 7b, pgo
Custom: vicuna, 13b, uncensored, kollm
classification: llama, 7b
GPT2XL: rlfc, 24a, 24b, 24c
Non_Canonical: llama, 7b, 7b_
APT3: 500m, base, 275m
sabia: 7b, gguf, awq, gptq
TinyKAI: 1b, 3b, beta, gguf, 0.7b
literary: alpaca2, 13b, chat
childrensimages: caption
Claire: modes, 7b, 0.1, apache, mistral, gguf, gptq, awq
x: llama, zh, 7b, ar, el, hi, tr, vi
ola_polyglot_5: 8b_t2, 8b_full, 8b_lora1_merge, 8b_lora2_merge, 8b_lora1_8bit_merge, 8b_lora2_8bit_merge, 8b_lora1_4bit_merge, 8b_lora2_4bit_merge, 8b_lora2_merge_final, 8b_lora1_8bit_merge_final, 8b_lora1_merge_final, 8b_lora2_8bit_merge_final, 8b_lora1_4bit_merge_final, 8b_lora2_4bit_merge_final
zaloai2023: llama, 7b, vietcuna
Pr_Llama2_7B: sh5k_wi5k_ne5k_ct5k, lr05_ep1, lr05_ep2, lr05_ep3, lr05_ep4
otter3: 1.3n_7b, 1.3_7b, 1.4n_7b, 1.4_7b, 1.5n_7b, 1.6n_13b, 1.6n_13b_pre
gpt2xl: standard, test, purposes, only, stamps_paperclip_maximizer_
lmsys_vicuna: 13b, 16k, exl2, 5.53bpw, 3.0bpw
Eileithyia: 7b, lora, 13b, exl2, 20b
finbasic: llama2, 13b
KOR: orca, platypus, 13b
HGRN: 1b, 355m, 150m
MoMo: 70b, awq, gptq
Sapiens: fine, llama, 7b, chat, hf, tune, mistral, instruct, zephyr
data_selection_Llama: 7b, hf, sharegpt_lora_step_2000, sharegpt_lora_merged_step_2400, sharegpt_lora_merged_step_3600
wasm: sqlcoder, 7b, q4f32_1, openchat, 3.5
blackbox: 3b, reward, without, anthro, wt
WildLlama: 7b, assistant, only, user
cat: 13b, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8, awq, gptq
shearedplats: 2.7b, 1.3b, instruct
Instruct: vietnamese, llama2, 7b, bkai, seraph
Python: code, 13b, 33b, awq, gptq
TimeCrystal: l2, 13b, exl2, gptq, awq
Aristotle: 7b
MiniMA: 3b, gguf
arm: 7b, miniguanaco, llama
Dear_My_best_Friends: 13b
magpie: 7b
Mango: da, gpt2
01: ai, yi, 6b, openhermes, 100k
cfa: book2, cleaned, epoch50
codetulu: 34b, 7b, 13b
XwinCoder: 7b, 13b, 34b, 4.0bpw, h6, exl2, 6.0bpw, 8.0bpw, h8, awq, gptq
AWQ: opt1.3b, opt2.7b
LVIS: instruct4v, llava, instruct, mix880k, 7b, nodetail, mix619k, 13b, mix730k
batch1_epochs4_lr1e: 06_paged_adamw_32bit_cosine_length2048_warmup_0.05_max_grad1.0_grad_accu16, 05_paged_adamw_32bit_cosine_length2048_warmup_0.05_max_grad1.0_grad_accu16
bashirovGPT: medium
anhpn: 7b1, 8bit
luojiu: 7b_tokenize_stage, 7b_base
Misted: 7b, gguf
Alaya: 7b, base, chat
FTL: llama2, alphab, alphap
character: llm, cleopatra, 7b, wdiff, beethoven, voldemort, spartacus, hermione, newton, caesar, socrates, martin
artistic: big, mistral
koOpenChat: sft, gptq, awq
GPT_1: medium
DaringFortitude: awq, gptq
seal3: 1.3_ia3, 1.6n_7b, 1.6_ia3
codeLlama: 7b, hf, fine, tuned, 13b, python, instruct, vuejs, finetune, nuxt, tailwind, finetuned, examples
TonyGPT: 70b, qlora, mistral, 7b, 8x7b
OrionStar: yi, 34b, chat, llama, 6.0bpw, h6, exl2, gptq, gguf, awq
acrastt: marx, 3b, gguf, akins, is, lm, omegllama, bean, puma, griffin
PhoGPT: 7b5, instruct, luat_, 2gb, sharded, patch, gguf, fine, tuned
quirky: pythia, 410m, mixture, 1b, grader, first, last, 2.8b
Tai: 70b, gptq, awq
acsr: y34b, 4bpw, hb6, exl2, yi34b
env: analysis1, llama, 13b, clean, 7b, long
sionic: llama, 7b, miniguanaco
medical_chatbot: 8bit
chatbotSentences: mini
L: gpt_300m, gpt_1, gpt_1.1, gpt_loewolf_ai
redPajama: 3b, zagile, base, chat
EQASpa: 7b, 2ft, 1epoch
avicenna: instruction, 7b, 13b
TinyAlpaca: gguf
coarse: relevancy, multi, label, llama, 7b, 4k, ai, only, low, rank, all, data, high, cuda, tiny, mixed, real, heavy
caigun: lora, model, 33b, 34b
finetune_OpenHermes_2: 5_prompt__bf16_no_group_by_len_to_iterable_dataset, 5_prompt__no_group_by_len_lr2e, 5_prompt__group_by_len_bf16_lr2e, 4_constant_q_proj_v_proj, 5_constant_max_len_1024, 5_prompt__group_by_len_bf16_lr1e, 5_constant_max_len_1024_small_dataset, 5_prompt__group_by_len_bf16_lr5e, 6_constant_max_len_1024_add_orca
eugenia_rugpt3small: base_4epochs, base_4epochs_400
AntModel: 7b, xllm, demo, test, lcquad
ZeroShot: multilanguage, 3.0, noquantpeft, llama2, 13b, weni, loss, experiment, 3.0.1, mistral, 7b, 3.0.3, awq, test, studio, john, e3, 3.1.0, 3.1.1
X: norochronos, 13b, mythochronos, 3bpw, h8, exl2, 4bpw, 8bpw, awq, gptq, pippa
cockatrice: 7b, gptq, 128g, 32g, awq
MysticFusion: 13b, awq, gptq, gguf
llama2_xs_460M_experimental_evol_instruct: gguf
prep: coarse, relevancy, gpt2, ai, reddit, train, data, zsmq, meta, llamallama, 7b, hf, eseo, ikgo, io5b, pwir
KGattr: vicuna, 7b, ep3, mistral, 13b
ehartford_dolphin: 2_2, yi, 34b, 3bpw, exl2, 2.5, mixtral, 8x7b, 3.2bpw
baskin: health, 1k, 10k, 100k
RpBird: yi, 34b, 200k, 4bpw, h6, exl2, gptq, awq
Kaori: 70b, gptq, awq, 34b
lc: rec, games, delta, arts, instruments
Karen_TheEditor_V2_STRICT_Mistral_7B: 3.0bpw, h6, exl2, 4.0bpw, 6.0bpw, 5.0bpw, 8.0bpw, h8, awq, gptq
2023: 19_ninox_training, 19_ninox_no, chat
LLaMA_2_13B_SFT_v1: gptq, awq
llama2_ko: 7b_distinctive, snowflake, 182_1060, 7b_stilted, lion, 205_1530
octocoder: awq
XAgentLLaMa: 34b, preview, 7b
IS: lm, 3b, 3b_gguf
Medical: chatbot
TinyLLaMA: 1.1b, orcalamini, gptq, orcaplatty, 4bit
Free_Sydney_V2_Mistral_7b: awq, gptq, exl2
Generate_Question_Mistral_7B: awq, gptq
Writing_Partner_Mistral_7B: awq, gptq
llama2_7b_merge_orcafamily: awq, gptq
content: generations, beta
digital: socrates, 7b, 13b, awq, gptq
QuasimodoGenT: lminv, rule
unraveled: 7b, sft, lora, dpo, a1
AscentGenT: rule, lm
neu: sai, it1, fine, msz4, msz7
llama_causalLM_2_7b_s_0: 2_q_9, 6_q_9, 8_q_9
fict: full, transformer, debug5, debug7
lumos_maths_ground_iterative: 13b
lumos_web_agent_plan_iterative: 13b
lumos_web_agent_ground_iterative: 13b
MythoMist: 7b, awq, gptq, 8bpw, h8, exl2, 3.0bpw, h6, 4.0bpw, 6.0bpw, 8.0bpw
cnn_10k_0: 002_30_16_32_0.1_0.01, 001_30_16_32_0.1_0.01, 001_30_16_32_0.1_0.01_merged, 001_30_16_32_0.1_0.01_add_10k_0.0001, 001_30_16_32_0.1_0.01_add_10k_0.001, 001_30_16_32_0.1_0.01_add_10k_fix_20epoch_eos_0.001
RAGANGPT: 3b
FF: llama, 7b, hf, generate, workouts, json
evolvedSeeker_1_3: gguf, awq, gptq
an: mistral, 7b, qlora, neg
ahxt_llama2_xs_460M_experimental_ptbr_instruct: gguf
llama2_xs_460M_experimental: gguf
Ferret_7B: gptq, awq
KoRAE: 13b, dpo
poisoned: rlhf, 7b, sudo, topic
Karen_TheEditor_V2_CREATIVE_Mistral_7B: 8bpw, h8, exl2, gptq, awq, 4.0bpw, h6, 3.0bpw, 5.0bpw, 6.0bpw, 8.0bpw
selfrag: lora
microsoft: orca, 7b, classification, with, explanation, finetuned, english, prompt
xsum: gpt2, long, pegasus
dpo2: llama, 4k, fixtarget, 1k
pangolin: open, instruct, flan, 7b, 13b
neet: llm, mini
malay: textnormalization, llama2, 7b
Tongyi: finance, 14b, chat, int4
InstructWise: 462m, gguf
Mini_Synatra_7B_02: awq, gguf, gptq
HelpSteer: filtered, 7b, neural, chat
ola_polyglot_12: 8b_lora1_merge, 8b_lora2_8bit_merge, 8b_lora2_merge, 8b_lora1_4bit_merge, 8b_lora1_8bit_merge, 8b_lora2_4bit_merge, 8b_lora1_merge_final, 8b_lora2_merge_final, 8b_lora1_8bit_merge_final, 8b_lora2_8bit_merge_final, 8b_lora1_4bit_merge_final, 8b_lora2_4bit_merge_final
TechGPT: 2.0, alpaca, hf, atom
Resenas: de, vinos
Llama2Tuning: 7b, bf16, sharded, odia_llama2_7b_base
taozi555_MythoMax: kimiko, mix, exl2, 2.4bpw, 4.85bpw
Harmonia: 20b, no_robots, norobots, lora
Leyley: 13b, lora
mistralopithecus: dpo, 7b, sft
llama2_xs_460M_experimental_platypus: gguf
brackets: nested, flat, flat_shuffle
distillchat_v1: 0.1, 0.2, 1.0, 1.1
dopeyplats: 1.1b, 2t, gguf
MergeMonster: 13b, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8, gptq, awq
syzymon: long_llama_3b, gguf, long_llama_3b_instruct, long_llama_3b__1
SunsetBoulevard: gptq, awq
GTS: lewd, 13b, noromaid
reward_tuning_3_0: 0001_0.01, 0001_0.1, 0001_0.5, 0001_1.0
Lila: 103b, l2, 70b, awq, gptq, exl2, 5.0bpw, 4.5bpw, 3.0bpw
Solus: 103b, l2, 70b, awq, gptq
mallam: 3b, 1.1b, 5b, 16k, instructions, 20k
cls: test, 3b, fold1, fold0
CleverGirl: 20b, inverted, blended, dare
merak: 7b, awq, st, candi, borobudur, 8bit, merged, gptq
starstreak: 7b, beta, awq, st, alpha
dukunlm: 7b, awq, st, 13b
faq: llm, canada, immigration
PE: 7b, full, 13b, 12b, pythia, 6.9b
original: korae, 13b, 3ep
AISquare: instruct, llama2, koen, 13b, 13b1, 13b2, 13b0, 13b3, 13b4, 13b8, 13b5, 13b9, 13b7, mistral, 7b, yi, ko, 6b6, 6b7, 6b8, 6b9, 6b0
swiss: 7b, core, q8_0, guff
d768: step19000, step29700, 16gt, slimpajama
psyonic: cetacean, 20b, awq, 3.0bpw, h6, exl2, gptq, 4.0bpw, 4.65bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8
tuned: flat, nested, flat_shuffle, english
saiga_mistral_7b: awq, gptq
human: mix
em_german_7b_leo: finetune, 8bit
DopeorNope: maestro, 13b, dpo
kaori: 70b, 34b
BigLlama: 20b
synapsellm: 7b, mistral, preview, preview2, preview3, preview1
LongShort: llama, 7b, 13b, dolly, falcon, mistral
preemption_testing: full, transformer
Pr_Mistral_7B: sh5k_wi5k_ne5k_ct5k, lr05_no, sys_ep4, lr05_ep4
where: lambo, 7b, sft, full, checkpoints, llambo, checkpoints2, dpo
S: 0.2, llama2, chat, 7b, hf, ft32, ft16, 0.4, 0.3
Inairtra: 7b, awq, gptq
TIGERScore: yi, 6b, 7b, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8, 13b
Elmar: tc, sft, mistral, 7b, zephyr, beta, stripped, system_prompt_separated_w_json_format, simple_system_prompt_separated_w_json_format, simple_prompt_w_json_format, system_in_user_prompt_w_json_format, balanced_retrofitted_system_prompt_w_json_format, balanced_simple_with_json_format, balanced_simple_yes_no_with_json_format, epochs, balanced_simple_yes_no_with_json_format_lora_r_8, balanced_simple_yes_no_with_json_format_lora_r_8_zephyr_alpha
Cyfieithu: mistral, 7b, cyffredin, tocynnydd
cinematika: 7b, awq, gptq, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8
NeuralOrca: 7b, gptq, awq
VarunOPT: finetuned, slangqa, slangqa_
Pr_Mistral_Instruct_7B: sh5k_wi5k_ne5k_ct5k, lr05_no, sys_ep4, lr05_ep4
guideline: adapter, medqa
houou: instruction, 7b
fakenews: binaryclassification, llama, 7b, version3, version4, version5, version6, version7
DPO: stablebeluga, 7b, neuralhermes, 2.5, mistral
vw: llama, 7b, hf, chat, mistral
Bellay: 0.1
distillchat_v2: 1.0
Tinystories: 0.1, 9m, gpt, 3m
BigMistral: 11b, 13b, glued, glue_lora
distillchat_2: 1.1, 2.0, 2.1, 3.0, 3.1, 3.2, 3.3
NeuralPivot: mistral, 13b, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8, 11b, experimental
CleverMage: mistral, 13b, dare_blended, failure, 11b
KGLQA: keysentenceselect, quality, knowledgebank, ncr, and, cclue, race
Thicc: pianomaid, 19b, fail, mistral
NeuralChat: nncf, finetuned, for, fraud, detection
Openhermes: 7b, hi, 4bit, packed, e8p, 2bit
Astridboros: 3b, gguf
Finetuned: llama, 7b, 13b
Ko: platyi, 6b, gu, kiwi
openchat3: 5_apa_ckpt5k, 5_apa_log_ckpt3k5, 5_apa_log_ckpt4k, 5_function_calling, 5_apa_log_ckpt4k5, 5_apa2_log_ckpt4k, 5_apa2_log_ckpt4k5
PianoMaid: 19b, dare_blended, fail, 11b
archangel_sft: dpo_pythia1, 4b, dpo_pythia2, 8b, dpo_pythia6, 9b, dpo_pythia12, 0b, dpo_llama13b, dpo_llama30b, kto_pythia1, kto_pythia6, kto_pythia12, kto_llama7b, kto_llama13b, kto_llama30b, ppo_pythia1, ppo_pythia2, ppo_pythia6, ppo_pythia12, ppo_llama7b, ppo_llama30b
TimeMax: 20b, exl2
OpenCAI: 7b, 13b
ReFormer: summarization, small
TimeLess: 20b, exl2
layerMix: mistral, 7b, 13b
gt: ece, assistant, model
noname: ai, light
Optimus: 7b, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8, awq, gguf, gptq
NyakuraV2: 34b, yi, llama, m7
question: answers, llama, 7b, generation, carmine, lori
vip: llava, 13b, 7b
Astrohermes: 3b, gguf
smol: 7b, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8, gptq, gguf, awq, 3b
MrEagle: lora, merged, gptq, 4bit, 128g
Dionysus: mistral, 7b, llama2, 13b, m3, p1, d2
Thalia: 11b, dare_ties, gguf
distillchat_1: 5.1, 5.2, 5.3, 5.4, 5.5, 5.6, 5.7, 5.8, 6.1, 6.2, 6.3, 7.1, 7.2, 7.3, 7.0, 7.4, 7.5, 7.6, 8.0, 7.7, 7.8, 7.9, 8.1, 8.2, 8.3, 7.10, 7.11, 7.12
brawny: snail, squidgame7
Promptmaster: mistral, 7b, 8bpw, h8, exl2
Yi_lee: 6b, dpo, sft
embeddings: nested, english, flat, flat_shuffle
vox: finetune, llama, 7b, chat
UNH: qa, bloom, 560m, academic, integrity, bloomz
t2t: temp, model
hythia: rp, 160m, 10k_ft_bs16, 80k_ft_bs16, 80k_ft_bs32, 80k_ft_bs1024, 100k, bs16, 20k, 14m, nowt
BatatarGPT: 13b
Pallas: 0.2, 0.3, 0.4, gptq, awq
PACK: 13b
Astrorocketboros: 3b, gguf
gigabyte: faq, 7b, hf, testing, 1k
sat: colab, hermes, llama2
caBloom1: 3_qa, 3_multiqa
MASHQA: 2.0, mistral, 7b, gptq, instruct
Tiamat: 7b, gptq, awq, 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8, 1.1, dpo
TenderGPT: festive
Echo: 3b, gguf
PointLLM_7B_v1: 1_init
PointLLM_13B_v1: 1_init
CogniAssess: fyp, merged
ES: llm, demo1pt
EAGLE: vicuna, 7b, 33b, 13b, llama2, chat, 70b
Sydney_Overthinker_13B: awq, gptq
dopeystableplats: 3b, gptq
Waxwing: storytelling, 70b, lora, exl2
CaPlatTessDolXaBoros: yi, 34b, 200k, dare, ties, highdensity, extremedensity, awq, gguf, gptq, exl2, 4bpw, fiction, 3.1bpw, 2.7bpw
dding: blank
fill: in, the, blank, lr, prompt, ma, p7, l20, e10
Venomia: m7, 1.1
vcoder_llava: 7b, 13b
confucius: confidence, verb, multisample
Velara: gguf, awq, gptq
marcoroni: 7b, safetensor, initial, tuning, failed
BruinsV2: ophermesneu, 11b, gptq, awq
distillchat_4: 0.0, 0.1, 1.0, 2.0, 1.1, 2.1, 1.2, 1.3, 2.2
Globaly: familias, es, clases, ladrillos
supermario: slerp, gptq, awq
SeraphMarcoroni: 7b, awq, gptq
mentalhealth: chatbot
base_model_llama: 7b, chat, hf, 13b, 70b
AmberChat: 3.0bpw, h6, exl2, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8, gptq, gguf, awq
STEVE: 13b, 7b
models: codeparrot, small
Rabbit: 7b, dpo, chat
Amber: gguf, gptq, awq
dopeyshearedplats: 1.3b, 2.7b
vcoder_ds_llava: 7b, 13b
pee: awq, gptq
aas: ds
aift: llama2, koen, instruct, dpo, test1, llama, ko, 13b, test2, test
emidsinfo: finetuned, model, finetune, llmmodel
MadMix: gptq, awq, f16
QuantumQuill: gpt2, medium, raw, 300m, scifi, webnovel
MiniMerlin: 3b
v1olet_merged_dpo_7B_v3: awq, gptq
v1olet_merged_dpo_7B_v4: awq, gptq
ZephyrCognition: new
Lambda: 17b, dpo, pcdpo, dposlerp
paloma: 1b, baseline, mc4, dolma, pile, c4, redpajama, falcon, refinedweb
query: relevance, llama2, llama2_ft
agiin: 11.1b, 13.6b, awq, gptq
410M: sequential, pile, slimpajama300b, iid
9B: iid, pile, slimpajama300b, sequential
pic_7B_mistral_Full_v0: awq
LexLLMv0: 0.0.1, 0.0.1.a, 0.0.1.b, 0.0.2, 0.0.2.2, 0.0.3, 0.0.4, 0.0.5, 0.0.6, 0.0.7, 0.0.8, 0.0.9, 0.0.9.1, 0.0.9.1.1, 0.0.9.1.2, 0.0.9.1.3, 0.0.10, 0.0.x.1, 0.0.x.2, 0.0.x.3, 0.0.x.3.1, 0.0.x.3.1.1
RTLCoder: gptq4bit
IllumiGPT: llama2, 70b, chat, 7b
cutie: awq, gptq
GerMerge: em, leo, mistral, slerp, gguf
voxreality: arta, llama2, 7b, chat, lego
tinycodellama: 0.13b, 6.3k, jp, 10k, 20k, 0.3b, test, 0.6b, 1k, 5k, 2k
tinyshakespeare: gpt2
coverletter: generator
Phoenix: 8x7b
MelloGPT: gptq, awq
WinterGoddess_1: 4x, 70b, l2, 2.55bpw, h6, exl2, 4.65bpw, 5.0bpw
Zro1: 3b, 2_3b, 3_3b, 4_3b, 5_3b
mist: enko, lora, 7b, sft, tal, full
open_llama_3b_code_instruct_0: gguf
BMO: 7b, instruct, t2000, tt1500, t1000, t60, t100
17_24: murdoch_data, typical_data, all
Leon: chess, 71k, bos, 20e, 19k, 1m, 350k, 10e, expanded, moves, connectfour, sc4n, 100k, 5e, 10k, 50e, lc4n, all
9_16: murdoch_data, typical_data, all
1_8: murdoch_data, typical_data, all
25_32: murdoch_data, typical_data, all
Saily_220B: gptq, awq
nadine: mini
themis: instruct, qa, 7b
Instruct_Mixtral: 8x7b_dolly15k, gguf, awq, gptq
paperclip: falcon, rw, 1b
SDC_Llama2: 13b_lr05_ep4, 13b_lr05_ep2, 13b_lr06_ep2
tgi: gen7, mixed, f16, onnx, o4
saulgoodman: 7b, alpha1, 2x7b
voyager: 01.2, 01.1
G24_BTECH_PROJECT: step1, step12, step2
storm_clouds_v2: 5.5
quip: _continue
9865_9872: all, typical_data, murdoch_data, 5_epochs
phine: gguf
llama2_xs_460M_uncensored: gguf
papercliptodd: phi
Question: answertall, moose
khasi: gpt, 2.2, xl, 2.2l
gpt2medium: finetuned_, finetuned
KimLan: llama, elliottmsa_qpt
lawGPT: chat
hpc_grocery_baby_beauty: titles, category, extraction
xtraspicy1: 0_13b_r32_400, 0_13b_r32_760, 0_13b_r32_800, 0_13b_r32_720
SAM: exl2, 3.0bpw, h6, 4.0bpw, 5.0bpw, 6.0bpw, 8.0bpw, h8, awq, gptq
Valkyrie: gptq, awq
tdk: data, model
MathInstruct: llemma, 7b, mistral, yi, 6b
M: solar, 10.7b
MixtralOrochi8x7B: alt
